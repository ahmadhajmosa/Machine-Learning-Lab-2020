{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-Learning-Lab-2020/blob/master/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2aqi3Fu-AJ",
        "colab_type": "code",
        "outputId": "eabf27bc-af78-4605-b30d-27f8077196c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "sess = tf.Session() \n",
        "sess.run(init)\n",
        "    \n",
        "for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "print('predicted ', y_p_p)\n",
        "print('real ', y_gr)\n",
        "\n",
        "#sess.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  0.5914289\n",
            "iter:  1 cost:  0.590312\n",
            "iter:  2 cost:  0.5891979\n",
            "iter:  3 cost:  0.5880863\n",
            "iter:  4 cost:  0.5869776\n",
            "iter:  5 cost:  0.58587164\n",
            "iter:  6 cost:  0.58476853\n",
            "iter:  7 cost:  0.5836683\n",
            "iter:  8 cost:  0.5825709\n",
            "iter:  9 cost:  0.5814764\n",
            "predicted  [[-0.07052338 -1.4182088  -0.20726368  0.6065073 ]\n",
            " [-0.41022846  0.7640786   1.0915788   0.5919728 ]\n",
            " [-0.07066503 -0.73823565  0.10052644  0.31935224]\n",
            " [-0.2946304  -0.3036949  -0.28762203  1.1227872 ]\n",
            " [-0.57840925  0.8397043   0.8986697   1.1970941 ]\n",
            " [-0.32035732 -0.3043637  -0.03262401  1.0779576 ]\n",
            " [-0.25558242 -0.41777548  0.2828037   0.74326414]\n",
            " [-0.6702078   0.774465    0.18325244  1.8486446 ]\n",
            " [-0.3652569   0.5447639   0.8380182   0.6200657 ]\n",
            " [-0.48676854  1.0157775   1.195501    0.7295743 ]]\n",
            "real  [[0.28427788 0.58920269 0.76702838 0.54495026]\n",
            " [0.10077242 0.44421162 0.08474814 0.63117015]\n",
            " [0.01157148 0.14526119 0.81988115 0.22006669]\n",
            " [0.09844534 0.24078169 0.60496677 0.87281952]\n",
            " [0.21494239 0.5097351  0.2950044  0.82441926]\n",
            " [0.41953509 0.88851562 0.17726813 0.39356474]\n",
            " [0.10843995 0.98875919 0.14346219 0.47484879]\n",
            " [0.40487958 0.82369397 0.90377852 0.66008465]\n",
            " [0.94767349 0.42496532 0.47597419 0.79391048]\n",
            " [0.89124593 0.46764663 0.18470877 0.82758744]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6qknOSWeKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7403b92-bfd5-4729-90dd-abf101839420"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPSS03avw5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "2759de58-f542-437c-cb8f-e8da5208463c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#      O   O   O\n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "\n",
        "\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "#F(WX+b)\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "#placeholders/inputs:outpus --> Variables/Weights --> Model --> cost --> optimizer --> initilize all variables --> start the session\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "iter:  0 cost:  3.7154167\n",
            "iter:  1 cost:  3.683918\n",
            "iter:  2 cost:  3.6526089\n",
            "iter:  3 cost:  3.6214912\n",
            "iter:  4 cost:  3.5905685\n",
            "iter:  5 cost:  3.5598426\n",
            "iter:  6 cost:  3.5293164\n",
            "iter:  7 cost:  3.4989905\n",
            "iter:  8 cost:  3.468869\n",
            "iter:  9 cost:  3.438953\n",
            "predicted  [[-1.6994138   3.139545   -0.4892214   1.2918464 ]\n",
            " [-1.4141936   3.3962379  -0.56025493  1.5004289 ]\n",
            " [-1.4271342   3.3675833  -0.5393368   1.5128952 ]\n",
            " [-1.6095712   3.2013707  -0.47851884  1.3919413 ]\n",
            " [-1.3633152   3.4307165  -0.57185465  1.5379093 ]\n",
            " [-1.4424294   3.3801365  -0.5667      1.4672735 ]\n",
            " [-1.7370307   3.1341696  -0.56446815  1.1919265 ]\n",
            " [-1.6812286   3.1714983  -0.5625444   1.2488736 ]\n",
            " [-1.69051     3.1477184  -0.4841122   1.3061322 ]\n",
            " [-1.7484171   3.0911536  -0.47290224  1.2563245 ]]\n",
            "real  [[0.24953134 0.73089036 0.9260432  0.02266804]\n",
            " [0.26182008 0.19663483 0.14013938 0.71980935]\n",
            " [0.97197882 0.45010515 0.21673956 0.62109635]\n",
            " [0.38568086 0.2567362  0.679027   0.98177855]\n",
            " [0.70510403 0.63541986 0.09825508 0.63960136]\n",
            " [0.58304965 0.41118293 0.01241144 0.59573531]\n",
            " [0.12318628 0.98534367 0.48893402 0.94055269]\n",
            " [0.10609206 0.15890923 0.60863427 0.22580786]\n",
            " [0.96061428 0.7335734  0.85362857 0.42405943]\n",
            " [0.15095686 0.13433172 0.071118   0.23971957]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhCMk9VyVj-",
        "colab_type": "code",
        "outputId": "97473f72-6025-499f-890e-be96d308bcd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "display(mnist.train.images.shape) # 28*28 = 784\n",
        "\n",
        "image =mnist.train.images[0].reshape((28,28))\n",
        "#MNIST data input (img shape: 28*28)\n",
        "imshow(image)\n",
        "\n",
        "print(mnist.train.labels[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-b22520be68e0>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOL0lEQVR4nO3df4wc9XnH8c8H4x8BDMahcS1+xISStqRKTXKYFlBrSkOJFRXStBS3IFeiuZRAFZQIlRJFIfmjoqghSktANQXFJAGKFH5W0IY4iVAqApyRY8yPACEG7Jx9YFNhaGOf7ad/3IAOuJk9dmZ31n7eL+m0e/PszDwa3edmd2Znvo4IAdj37dd2AwD6g7ADSRB2IAnCDiRB2IEk9u/nymZ5dszRgf1cJZDKL/WadsYOT1WrFXbbZ0j6mqQZkv4tIq6oev0cHagTfVqdVQKo8GCsLq11/Tbe9gxJX5f0UUnHSVpu+7hulwegt+p8Zl8i6ZmIeDYidkq6RdKZzbQFoGl1wn64pBcm/b6xmPYmtodtj9geGdeOGqsDUEfPj8ZHxMqIGIqIoZma3evVAShRJ+ybJB056fcjimkABlCdsD8s6VjbR9ueJekcSXc10xaApnV96i0idtm+SNJ/aeLU2w0R8VhjnQFoVK3z7BFxj6R7GuoFQA/xdVkgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAErWGbLa9QdJ2Sbsl7YqIoSaaAtC8WmEvnBoRLzWwHAA9xNt4IIm6YQ9J37W9xvbwVC+wPWx7xPbIuHbUXB2AbtV9G39KRGyy/R5J99l+MiLun/yCiFgpaaUkHez5UXN9ALpUa88eEZuKxzFJt0ta0kRTAJrXddhtH2h77uvPJZ0uaX1TjQFoVp238Qsk3W779eXcFBH/2UhXABrXddgj4llJv91gLwB6iFNvQBKEHUiCsANJEHYgCcIOJNHEhTBo2ehnTyqtucN3FudsrX7By79RPf/CB3ZXL//uh6oXgL5hzw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSewz59nHLiw/1yxJ//PB8cr67adf3WQ7ffWbsx7uet5fxq7K+iH7vauyPnbea5X1X/xz+Z/YVZs/Ujnv1rMPrqzvemFjZR1vxp4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JwRP8GaTnY8+NEn9b1/E9dd0Jp7cll11TOO9szu14v2nHuhqWV9Zf/osN5+A3PN9jN3uHBWK1XYpunqrFnB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk9qrr2a899cbSWqfz6P+49djK+tjOuV311ITb1ny4sn7U3VOeNh0IG0+r3l9cueym0tonDnqlct5vLfphZf3cm5ZW1l/+8yNKaxmvhe+4Z7d9g+0x2+snTZtv+z7bTxePh/a2TQB1Tedt/DcknfGWaZdKWh0Rx0paXfwOYIB1DHtE3C9p21smnylpVfF8laSzGu4LQMO6/cy+ICJGi+ebJS0oe6HtYUnDkjRHB3S5OgB11T4aHxNX0pReTRMRKyNiKCKGZmp23dUB6FK3Yd9ie6EkFY9jzbUEoBe6DftdklYUz1dIurOZdgD0Ssfr2W3fLGmppMMkbZH0RUl3SLpV0lGSnpN0dkS89SDe29S9nt0f/kBp7aXF1dc2v+eOn1bWd2/t2D66sN8Hywd4/9gt/10574XzXqi17l+//oLS2qIvPFBr2YOq6nr2jgfoImJ5San71ALoO74uCyRB2IEkCDuQBGEHkiDsQBJ71a2ksW/Z+snfrayPfOnaWstfs2Nnae2yo5fUWvag4lbSAAg7kAVhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgib1qyGbsfTZedlJpbc/x23u67gUzyq9n3/UH1cNk7//9NU230zr27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBPeN3wfs/75FpbVnzl9YOe8156xsuJs3WzpnvLQ2w+3ta342/mpl/dPvPaVPnTSr1n3jbd9ge8z2+knTLre9yfba4mdZkw0DaN50/rV+Q9IZU0z/akQsLn7uabYtAE3rGPaIuF/Stj70AqCH6nxousj2uuJt/qFlL7I9bHvE9si4dtRYHYA6ug37tZKOkbRY0qikr5S9MCJWRsRQRAzN1OwuVwegrq7CHhFbImJ3ROyRdJ2kfXNITGAf0lXYbU8+n/NxSevLXgtgMHS8nt32zZKWSjrM9kZJX5S01PZiSSFpg6RP9bDHfd6rf3ZiZf3FD1X/T/7yn9xSWjtn7std9dScwfze1h9+7+LK+vs10qdO+qdj2CNi+RSTr+9BLwB6aDD/7QJoHGEHkiDsQBKEHUiCsANJcCvpBvj4D1TW5109Wlm/Z9G1lfVeXgp6x2sHVdbX/98RtZb/H1cuLa3N2FF9efWKL99dWR8+5BfdtCRJmrV5Ztfz7q3YswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEpxnn6bnvlQ+9PAXzvn3ynn/cu7Wyvrzu/63sv7kztK7fkmS/vbmvy6tHTA65V2F37Dwhy9V1nc//lRlvZND9OOu53367xd0WHj1efafV9wuetGd1beS3hexZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDjPPk3zThgrrXU6j37a439cWR//l1+trL/rzocq64v0QGW9yu6u56xvz+8fX1k/a16nmxhX76u27ZlVXnzo0Q7L3vewZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDjPPk3vPr/8+udf++wFlfMec0n1efD99XxXPe3tXn7/nMr6yXPq7YuG159bWjtM9a7T3xt13Jq2j7T9A9uP237M9meK6fNt32f76eKx+g4LAFo1nX+duyR9LiKOk/Q7ki60fZykSyWtjohjJa0ufgcwoDqGPSJGI+KR4vl2SU9IOlzSmZJWFS9bJemsXjUJoL539Jnd9iJJx0t6UNKCiHh9ELPNkqa8YZjtYUnDkjRHB3TbJ4Capn0ExPZBkr4j6eKIeGVyLSJC0pSj9EXEyogYioihmZpdq1kA3ZtW2G3P1ETQvx0RtxWTt9heWNQXSiq/LAxA6zq+jbdtSddLeiIirppUukvSCklXFI939qTDAbFrdHNp7ZhLymsot/WEXbXmf2Jn9S24515zSK3l72um85n9ZEnnSXrU9tpi2mWaCPmtts+X9Jyks3vTIoAmdAx7RPxIUtlIA6c12w6AXuHrskAShB1IgrADSRB2IAnCDiTBJa7oqT9a/0pp7fZ5X+8wd8WtoCWteGxFZf3Qex/usPxc2LMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKcZ0dP/enB60prB+x3UOW8T42/Vlk/4Op5XfWUFXt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC8+yoZezTJ1XWF8wov6b85+Plw2BL0vJ/uKSyfti91UNh483YswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEtMZn/1ISTdKWiApJK2MiK/ZvlzSJyW9WLz0soi4p1eNoh2ePbuy/om/+X5lffuenaW1ZQ9dUDnvUf/KefQmTedLNbskfS4iHrE9V9Ia2/cVta9GxD/1rj0ATZnO+OyjkkaL59ttPyHp8F43BqBZ7+gzu+1Fko6X9GAx6SLb62zfYPvQknmGbY/YHhnXjlrNAujetMNu+yBJ35F0cUS8IulaScdIWqyJPf9XppovIlZGxFBEDM1U9ec/AL0zrbDbnqmJoH87Im6TpIjYEhG7I2KPpOskLeldmwDq6hh225Z0vaQnIuKqSdMXTnrZxyWtb749AE2ZztH4kyWdJ+lR22uLaZdJWm57sSZOx22Q9KmedIh27YnK8jfvPrWyfu9PlpbWjrr1x910hC5N52j8jyR5ihLn1IG9CN+gA5Ig7EAShB1IgrADSRB2IAnCDiTBraRRKcbLL1GVpEWf5zLUvQV7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IwhHV1ys3ujL7RUnPTZp0mKSX+tbAOzOovQ1qXxK9davJ3t4bEb8yVaGvYX/byu2RiBhqrYEKg9rboPYl0Vu3+tUbb+OBJAg7kETbYV/Z8vqrDGpvg9qXRG/d6ktvrX5mB9A/be/ZAfQJYQeSaCXsts+w/VPbz9i+tI0eytjeYPtR22ttj7Tcyw22x2yvnzRtvu37bD9dPE45xl5LvV1ue1Ox7dbaXtZSb0fa/oHtx20/ZvszxfRWt11FX33Zbn3/zG57hqSnJH1E0kZJD0taHhGP97WRErY3SBqKiNa/gGH79yS9KunGiPitYtqVkrZFxBXFP8pDI+LvBqS3yyW92vYw3sVoRQsnDzMu6SxJf6UWt11FX2erD9utjT37EknPRMSzEbFT0i2Szmyhj4EXEfdL2vaWyWdKWlU8X6WJP5a+K+ltIETEaEQ8UjzfLun1YcZb3XYVffVFG2E/XNILk37fqMEa7z0kfdf2GtvDbTczhQURMVo83yxpQZvNTKHjMN799JZhxgdm23Uz/HldHKB7u1Mi4kOSPirpwuLt6kCKic9gg3TudFrDePfLFMOMv6HNbdft8Od1tRH2TZKOnPT7EcW0gRARm4rHMUm3a/CGot7y+gi6xeNYy/28YZCG8Z5qmHENwLZrc/jzNsL+sKRjbR9te5akcyTd1UIfb2P7wOLAiWwfKOl0Dd5Q1HdJWlE8XyHpzhZ7eZNBGca7bJhxtbztWh/+PCL6/iNpmSaOyP9M0ufb6KGkr/dJ+knx81jbvUm6WRNv68Y1cWzjfEnvlrRa0tOSvidp/gD19k1Jj0pap4lgLWypt1M08RZ9naS1xc+ytrddRV992W58XRZIggN0QBKEHUiCsANJEHYgCcIOJEHYgSQIO5DE/wN7/T2QKq1v5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXapqksBX95W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "tic = time.clock()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeRp0T10834",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c21981c3-facc-4765-ac13-7c0e1f34b49c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# training data\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "# testing data\n",
        "X_test = mnist.test.images\n",
        "Y_test = mnist.test.labels\n",
        "\n",
        "# training data\n",
        "X_val = mnist.validation.images\n",
        "Y_val = mnist.validation.labels\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 784\n",
        "num_h1_n = 100\n",
        "num_h2_n = 100\n",
        "num_outputs = 10\n",
        "\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "\n",
        "## 3 images, y_p=[[0.1,0.0,0,0.9],[0.9,0.1,0,0.],[0,0.9,0,0.1]] \n",
        "\n",
        "# tf.argmax(y_p, 1) [3,0,1] \n",
        "\n",
        "# 3 images, y=[[0,0.0,0,1],[0,1,0,0],[0,1,0,0]] \n",
        "\n",
        "# tf.argmax(y, 1) [3,1,1]\n",
        "\n",
        "# tf.equal [True,False,True]--[1,0,1]--- 2/3 \n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "tic = time.clock()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    #print('predicted ', y_p_p)\n",
        "    #print('real ', y_gr)\n",
        "\n",
        "\n",
        "\n",
        "toc = time.clock()\n",
        "toc-tic"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-9d619b82b806>:58: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "iter:  0 train_cost:  6.949297 train_acc:  0.0859375 test_cost:  5.9144135 test_acc:  0.1328125\n",
            "iter:  1 train_cost:  5.962449 train_acc:  0.1171875 test_cost:  5.7017236 test_acc:  0.1328125\n",
            "iter:  2 train_cost:  6.0448 train_acc:  0.09375 test_cost:  6.155078 test_acc:  0.0703125\n",
            "iter:  3 train_cost:  5.8841457 train_acc:  0.0703125 test_cost:  5.954007 test_acc:  0.109375\n",
            "iter:  4 train_cost:  5.1000423 train_acc:  0.1484375 test_cost:  5.6549373 test_acc:  0.1015625\n",
            "iter:  5 train_cost:  5.563229 train_acc:  0.125 test_cost:  5.6251545 test_acc:  0.09375\n",
            "iter:  6 train_cost:  5.5034456 train_acc:  0.109375 test_cost:  5.891651 test_acc:  0.09375\n",
            "iter:  7 train_cost:  4.723694 train_acc:  0.09375 test_cost:  4.7521877 test_acc:  0.1484375\n",
            "iter:  8 train_cost:  4.1869183 train_acc:  0.1953125 test_cost:  4.840802 test_acc:  0.140625\n",
            "iter:  9 train_cost:  4.6370106 train_acc:  0.1484375 test_cost:  4.9572296 test_acc:  0.1171875\n",
            "iter:  10 train_cost:  5.061462 train_acc:  0.1015625 test_cost:  4.2219143 test_acc:  0.1640625\n",
            "iter:  11 train_cost:  4.649314 train_acc:  0.1640625 test_cost:  4.7277374 test_acc:  0.125\n",
            "iter:  12 train_cost:  4.5119967 train_acc:  0.1328125 test_cost:  4.3507776 test_acc:  0.1328125\n",
            "iter:  13 train_cost:  4.812082 train_acc:  0.09375 test_cost:  4.820387 test_acc:  0.125\n",
            "iter:  14 train_cost:  4.9014745 train_acc:  0.078125 test_cost:  4.7143497 test_acc:  0.078125\n",
            "iter:  15 train_cost:  4.412494 train_acc:  0.125 test_cost:  4.4401217 test_acc:  0.1484375\n",
            "iter:  16 train_cost:  4.421981 train_acc:  0.09375 test_cost:  4.2467003 test_acc:  0.140625\n",
            "iter:  17 train_cost:  3.9870427 train_acc:  0.15625 test_cost:  3.7922125 test_acc:  0.1875\n",
            "iter:  18 train_cost:  4.5143604 train_acc:  0.140625 test_cost:  4.04718 test_acc:  0.1875\n",
            "iter:  19 train_cost:  3.8688526 train_acc:  0.15625 test_cost:  3.7638884 test_acc:  0.15625\n",
            "iter:  20 train_cost:  3.8464699 train_acc:  0.15625 test_cost:  4.3964853 test_acc:  0.0859375\n",
            "iter:  21 train_cost:  3.9428122 train_acc:  0.1796875 test_cost:  3.572546 test_acc:  0.1953125\n",
            "iter:  22 train_cost:  3.802968 train_acc:  0.1328125 test_cost:  4.151085 test_acc:  0.1640625\n",
            "iter:  23 train_cost:  3.8579004 train_acc:  0.1015625 test_cost:  4.025568 test_acc:  0.09375\n",
            "iter:  24 train_cost:  4.037197 train_acc:  0.0625 test_cost:  3.8338094 test_acc:  0.140625\n",
            "iter:  25 train_cost:  3.7018142 train_acc:  0.15625 test_cost:  3.6279273 test_acc:  0.1953125\n",
            "iter:  26 train_cost:  3.924866 train_acc:  0.1328125 test_cost:  3.6724977 test_acc:  0.1640625\n",
            "iter:  27 train_cost:  3.9477239 train_acc:  0.1484375 test_cost:  3.6728425 test_acc:  0.1796875\n",
            "iter:  28 train_cost:  3.842412 train_acc:  0.2109375 test_cost:  3.4466023 test_acc:  0.1484375\n",
            "iter:  29 train_cost:  3.632103 train_acc:  0.1796875 test_cost:  3.7187557 test_acc:  0.125\n",
            "iter:  30 train_cost:  3.6403136 train_acc:  0.1796875 test_cost:  3.4441266 test_acc:  0.2109375\n",
            "iter:  31 train_cost:  3.271758 train_acc:  0.203125 test_cost:  3.3257751 test_acc:  0.140625\n",
            "iter:  32 train_cost:  3.3956041 train_acc:  0.1875 test_cost:  3.662182 test_acc:  0.1796875\n",
            "iter:  33 train_cost:  3.5629573 train_acc:  0.1953125 test_cost:  2.8913999 test_acc:  0.265625\n",
            "iter:  34 train_cost:  3.3030818 train_acc:  0.2265625 test_cost:  3.1610937 test_acc:  0.2109375\n",
            "iter:  35 train_cost:  3.3755448 train_acc:  0.2109375 test_cost:  2.9908993 test_acc:  0.2578125\n",
            "iter:  36 train_cost:  3.3948479 train_acc:  0.1953125 test_cost:  3.165068 test_acc:  0.25\n",
            "iter:  37 train_cost:  3.6648731 train_acc:  0.1640625 test_cost:  3.0942369 test_acc:  0.25\n",
            "iter:  38 train_cost:  2.794201 train_acc:  0.2890625 test_cost:  3.182332 test_acc:  0.1875\n",
            "iter:  39 train_cost:  3.1781871 train_acc:  0.2109375 test_cost:  3.5070653 test_acc:  0.1953125\n",
            "iter:  40 train_cost:  2.8886337 train_acc:  0.21875 test_cost:  3.0368109 test_acc:  0.203125\n",
            "iter:  41 train_cost:  3.0623517 train_acc:  0.21875 test_cost:  2.9813728 test_acc:  0.2109375\n",
            "iter:  42 train_cost:  3.2069569 train_acc:  0.2421875 test_cost:  3.054108 test_acc:  0.2734375\n",
            "iter:  43 train_cost:  2.9698653 train_acc:  0.3125 test_cost:  3.327082 test_acc:  0.1953125\n",
            "iter:  44 train_cost:  3.1936936 train_acc:  0.2265625 test_cost:  3.2648997 test_acc:  0.1953125\n",
            "iter:  45 train_cost:  3.467605 train_acc:  0.21875 test_cost:  3.149592 test_acc:  0.2578125\n",
            "iter:  46 train_cost:  3.0646398 train_acc:  0.265625 test_cost:  3.230726 test_acc:  0.2734375\n",
            "iter:  47 train_cost:  2.9110503 train_acc:  0.2265625 test_cost:  2.8074396 test_acc:  0.2421875\n",
            "iter:  48 train_cost:  3.0681515 train_acc:  0.2265625 test_cost:  2.8682468 test_acc:  0.2734375\n",
            "iter:  49 train_cost:  2.9600143 train_acc:  0.2578125 test_cost:  2.7800016 test_acc:  0.234375\n",
            "iter:  50 train_cost:  2.665148 train_acc:  0.296875 test_cost:  2.6604428 test_acc:  0.3046875\n",
            "iter:  51 train_cost:  3.1325355 train_acc:  0.21875 test_cost:  2.8161511 test_acc:  0.21875\n",
            "iter:  52 train_cost:  2.5741796 train_acc:  0.3359375 test_cost:  2.6984458 test_acc:  0.296875\n",
            "iter:  53 train_cost:  3.1213832 train_acc:  0.3046875 test_cost:  2.6386127 test_acc:  0.3359375\n",
            "iter:  54 train_cost:  2.5114005 train_acc:  0.296875 test_cost:  2.870171 test_acc:  0.2734375\n",
            "iter:  55 train_cost:  2.7989883 train_acc:  0.3203125 test_cost:  2.776164 test_acc:  0.28125\n",
            "iter:  56 train_cost:  2.7386415 train_acc:  0.3125 test_cost:  2.9907393 test_acc:  0.2734375\n",
            "iter:  57 train_cost:  2.7849884 train_acc:  0.3046875 test_cost:  2.8497536 test_acc:  0.2421875\n",
            "iter:  58 train_cost:  3.1615014 train_acc:  0.2578125 test_cost:  2.4427204 test_acc:  0.34375\n",
            "iter:  59 train_cost:  2.6821012 train_acc:  0.2578125 test_cost:  2.710942 test_acc:  0.296875\n",
            "iter:  60 train_cost:  2.606052 train_acc:  0.2734375 test_cost:  2.1959684 test_acc:  0.390625\n",
            "iter:  61 train_cost:  2.730963 train_acc:  0.28125 test_cost:  2.6478257 test_acc:  0.3125\n",
            "iter:  62 train_cost:  2.7551527 train_acc:  0.2734375 test_cost:  2.3382907 test_acc:  0.3125\n",
            "iter:  63 train_cost:  2.878428 train_acc:  0.25 test_cost:  2.2863398 test_acc:  0.34375\n",
            "iter:  64 train_cost:  2.6347408 train_acc:  0.3046875 test_cost:  2.2516913 test_acc:  0.3515625\n",
            "iter:  65 train_cost:  2.6598778 train_acc:  0.296875 test_cost:  2.6009731 test_acc:  0.3125\n",
            "iter:  66 train_cost:  2.5811274 train_acc:  0.3359375 test_cost:  2.602764 test_acc:  0.296875\n",
            "iter:  67 train_cost:  2.3946729 train_acc:  0.3359375 test_cost:  2.4467149 test_acc:  0.3515625\n",
            "iter:  68 train_cost:  2.5743792 train_acc:  0.296875 test_cost:  2.425904 test_acc:  0.421875\n",
            "iter:  69 train_cost:  2.4448333 train_acc:  0.359375 test_cost:  2.5572047 test_acc:  0.2734375\n",
            "iter:  70 train_cost:  2.6185102 train_acc:  0.3359375 test_cost:  2.4561477 test_acc:  0.3359375\n",
            "iter:  71 train_cost:  2.5831807 train_acc:  0.328125 test_cost:  2.3892539 test_acc:  0.3203125\n",
            "iter:  72 train_cost:  2.0603135 train_acc:  0.3984375 test_cost:  2.0771267 test_acc:  0.40625\n",
            "iter:  73 train_cost:  2.4762447 train_acc:  0.34375 test_cost:  2.1731112 test_acc:  0.4375\n",
            "iter:  74 train_cost:  2.3625956 train_acc:  0.359375 test_cost:  2.4021196 test_acc:  0.28125\n",
            "iter:  75 train_cost:  2.4221673 train_acc:  0.3203125 test_cost:  2.2300944 test_acc:  0.34375\n",
            "iter:  76 train_cost:  2.4155517 train_acc:  0.3359375 test_cost:  2.4065135 test_acc:  0.34375\n",
            "iter:  77 train_cost:  2.468024 train_acc:  0.3125 test_cost:  2.129962 test_acc:  0.359375\n",
            "iter:  78 train_cost:  2.2263966 train_acc:  0.40625 test_cost:  2.5807846 test_acc:  0.3203125\n",
            "iter:  79 train_cost:  2.494316 train_acc:  0.359375 test_cost:  2.1539986 test_acc:  0.359375\n",
            "iter:  80 train_cost:  1.9702971 train_acc:  0.421875 test_cost:  2.542725 test_acc:  0.359375\n",
            "iter:  81 train_cost:  2.2984214 train_acc:  0.4140625 test_cost:  1.9075241 test_acc:  0.4609375\n",
            "iter:  82 train_cost:  2.251069 train_acc:  0.359375 test_cost:  2.1409569 test_acc:  0.40625\n",
            "iter:  83 train_cost:  2.361051 train_acc:  0.3515625 test_cost:  2.1679444 test_acc:  0.375\n",
            "iter:  84 train_cost:  2.184219 train_acc:  0.375 test_cost:  2.2696276 test_acc:  0.3828125\n",
            "iter:  85 train_cost:  2.1185822 train_acc:  0.40625 test_cost:  2.3161318 test_acc:  0.375\n",
            "iter:  86 train_cost:  1.9366602 train_acc:  0.421875 test_cost:  2.4931304 test_acc:  0.3515625\n",
            "iter:  87 train_cost:  2.378644 train_acc:  0.3828125 test_cost:  2.2287843 test_acc:  0.3984375\n",
            "iter:  88 train_cost:  2.1828291 train_acc:  0.3984375 test_cost:  2.1326172 test_acc:  0.3984375\n",
            "iter:  89 train_cost:  2.4069188 train_acc:  0.359375 test_cost:  2.0885787 test_acc:  0.421875\n",
            "iter:  90 train_cost:  2.0899034 train_acc:  0.421875 test_cost:  1.5883482 test_acc:  0.421875\n",
            "iter:  91 train_cost:  1.732224 train_acc:  0.4921875 test_cost:  1.9990122 test_acc:  0.453125\n",
            "iter:  92 train_cost:  2.2156093 train_acc:  0.4375 test_cost:  1.8784573 test_acc:  0.4375\n",
            "iter:  93 train_cost:  2.1275277 train_acc:  0.4140625 test_cost:  1.9935453 test_acc:  0.3984375\n",
            "iter:  94 train_cost:  1.6954842 train_acc:  0.4765625 test_cost:  2.187683 test_acc:  0.3359375\n",
            "iter:  95 train_cost:  2.0581622 train_acc:  0.4296875 test_cost:  1.9292792 test_acc:  0.4609375\n",
            "iter:  96 train_cost:  1.8711947 train_acc:  0.4609375 test_cost:  1.8518773 test_acc:  0.4140625\n",
            "iter:  97 train_cost:  1.7274914 train_acc:  0.453125 test_cost:  1.9821831 test_acc:  0.3828125\n",
            "iter:  98 train_cost:  1.9263101 train_acc:  0.4765625 test_cost:  2.0809684 test_acc:  0.4140625\n",
            "iter:  99 train_cost:  2.0054874 train_acc:  0.390625 test_cost:  1.6793406 test_acc:  0.453125\n",
            "iter:  100 train_cost:  2.064614 train_acc:  0.3984375 test_cost:  1.9440688 test_acc:  0.4140625\n",
            "iter:  101 train_cost:  1.896117 train_acc:  0.40625 test_cost:  1.745201 test_acc:  0.5234375\n",
            "iter:  102 train_cost:  1.8800901 train_acc:  0.4140625 test_cost:  2.0399907 test_acc:  0.4453125\n",
            "iter:  103 train_cost:  1.7493806 train_acc:  0.5078125 test_cost:  1.9538869 test_acc:  0.4609375\n",
            "iter:  104 train_cost:  1.8415781 train_acc:  0.4453125 test_cost:  1.6406127 test_acc:  0.53125\n",
            "iter:  105 train_cost:  1.9515343 train_acc:  0.453125 test_cost:  1.9428407 test_acc:  0.4453125\n",
            "iter:  106 train_cost:  1.8328083 train_acc:  0.4765625 test_cost:  1.7235975 test_acc:  0.4453125\n",
            "iter:  107 train_cost:  1.8649976 train_acc:  0.4453125 test_cost:  2.158368 test_acc:  0.390625\n",
            "iter:  108 train_cost:  1.8648639 train_acc:  0.4296875 test_cost:  1.7733045 test_acc:  0.453125\n",
            "iter:  109 train_cost:  1.9366897 train_acc:  0.453125 test_cost:  1.9207795 test_acc:  0.4453125\n",
            "iter:  110 train_cost:  1.85198 train_acc:  0.4609375 test_cost:  1.8252674 test_acc:  0.46875\n",
            "iter:  111 train_cost:  1.8353045 train_acc:  0.4765625 test_cost:  1.7346038 test_acc:  0.515625\n",
            "iter:  112 train_cost:  1.59124 train_acc:  0.4921875 test_cost:  1.76599 test_acc:  0.5\n",
            "iter:  113 train_cost:  2.07057 train_acc:  0.34375 test_cost:  2.137482 test_acc:  0.3828125\n",
            "iter:  114 train_cost:  1.7573752 train_acc:  0.4765625 test_cost:  1.9730666 test_acc:  0.4375\n",
            "iter:  115 train_cost:  1.7553172 train_acc:  0.453125 test_cost:  1.750377 test_acc:  0.5078125\n",
            "iter:  116 train_cost:  1.7202148 train_acc:  0.4921875 test_cost:  1.801444 test_acc:  0.5\n",
            "iter:  117 train_cost:  1.82677 train_acc:  0.484375 test_cost:  1.6970606 test_acc:  0.4296875\n",
            "iter:  118 train_cost:  1.5433369 train_acc:  0.5 test_cost:  1.9847484 test_acc:  0.4140625\n",
            "iter:  119 train_cost:  2.0196915 train_acc:  0.46875 test_cost:  1.4958831 test_acc:  0.484375\n",
            "iter:  120 train_cost:  1.636889 train_acc:  0.5 test_cost:  1.4370036 test_acc:  0.515625\n",
            "iter:  121 train_cost:  1.6830748 train_acc:  0.484375 test_cost:  1.7747502 test_acc:  0.46875\n",
            "iter:  122 train_cost:  1.6224174 train_acc:  0.5 test_cost:  1.6286169 test_acc:  0.4609375\n",
            "iter:  123 train_cost:  1.8094664 train_acc:  0.4609375 test_cost:  1.3432097 test_acc:  0.609375\n",
            "iter:  124 train_cost:  1.9834555 train_acc:  0.4609375 test_cost:  1.4032638 test_acc:  0.5546875\n",
            "iter:  125 train_cost:  1.4839689 train_acc:  0.5546875 test_cost:  1.5752604 test_acc:  0.5859375\n",
            "iter:  126 train_cost:  1.8215032 train_acc:  0.5234375 test_cost:  1.8417273 test_acc:  0.484375\n",
            "iter:  127 train_cost:  1.5822117 train_acc:  0.5234375 test_cost:  1.5492351 test_acc:  0.5390625\n",
            "iter:  128 train_cost:  1.8140745 train_acc:  0.453125 test_cost:  1.6025937 test_acc:  0.53125\n",
            "iter:  129 train_cost:  1.6518445 train_acc:  0.5078125 test_cost:  1.6093118 test_acc:  0.5390625\n",
            "iter:  130 train_cost:  1.7104256 train_acc:  0.5 test_cost:  1.5630882 test_acc:  0.53125\n",
            "iter:  131 train_cost:  1.5612458 train_acc:  0.5546875 test_cost:  1.5565512 test_acc:  0.515625\n",
            "iter:  132 train_cost:  1.5171859 train_acc:  0.5546875 test_cost:  1.7045505 test_acc:  0.4921875\n",
            "iter:  133 train_cost:  1.8638554 train_acc:  0.453125 test_cost:  1.5521632 test_acc:  0.515625\n",
            "iter:  134 train_cost:  1.4485197 train_acc:  0.546875 test_cost:  1.5917184 test_acc:  0.5390625\n",
            "iter:  135 train_cost:  1.7743138 train_acc:  0.4609375 test_cost:  1.6823466 test_acc:  0.4765625\n",
            "iter:  136 train_cost:  1.693184 train_acc:  0.5546875 test_cost:  1.4560442 test_acc:  0.5390625\n",
            "iter:  137 train_cost:  1.6344206 train_acc:  0.5078125 test_cost:  1.5390742 test_acc:  0.53125\n",
            "iter:  138 train_cost:  1.5165551 train_acc:  0.515625 test_cost:  1.5177572 test_acc:  0.4921875\n",
            "iter:  139 train_cost:  1.90198 train_acc:  0.40625 test_cost:  1.4959643 test_acc:  0.5234375\n",
            "iter:  140 train_cost:  1.401787 train_acc:  0.53125 test_cost:  1.4838679 test_acc:  0.5078125\n",
            "iter:  141 train_cost:  1.6550723 train_acc:  0.5625 test_cost:  1.5110433 test_acc:  0.5\n",
            "iter:  142 train_cost:  1.6119962 train_acc:  0.4765625 test_cost:  1.644793 test_acc:  0.4453125\n",
            "iter:  143 train_cost:  1.6896782 train_acc:  0.484375 test_cost:  1.686101 test_acc:  0.3984375\n",
            "iter:  144 train_cost:  1.518561 train_acc:  0.5390625 test_cost:  1.3297151 test_acc:  0.5625\n",
            "iter:  145 train_cost:  1.4359212 train_acc:  0.5859375 test_cost:  1.5043656 test_acc:  0.5390625\n",
            "iter:  146 train_cost:  1.6830907 train_acc:  0.4453125 test_cost:  1.4813719 test_acc:  0.5859375\n",
            "iter:  147 train_cost:  1.3919106 train_acc:  0.5703125 test_cost:  1.6177447 test_acc:  0.53125\n",
            "iter:  148 train_cost:  1.4303875 train_acc:  0.5859375 test_cost:  1.6078849 test_acc:  0.4765625\n",
            "iter:  149 train_cost:  1.4953666 train_acc:  0.5390625 test_cost:  1.5485296 test_acc:  0.515625\n",
            "iter:  150 train_cost:  1.557765 train_acc:  0.546875 test_cost:  1.391897 test_acc:  0.5625\n",
            "iter:  151 train_cost:  1.6293018 train_acc:  0.5078125 test_cost:  1.4453802 test_acc:  0.5390625\n",
            "iter:  152 train_cost:  1.5837979 train_acc:  0.5546875 test_cost:  1.5812789 test_acc:  0.546875\n",
            "iter:  153 train_cost:  1.4260163 train_acc:  0.578125 test_cost:  1.4481897 test_acc:  0.4921875\n",
            "iter:  154 train_cost:  1.4868551 train_acc:  0.578125 test_cost:  1.5160363 test_acc:  0.5546875\n",
            "iter:  155 train_cost:  1.2909905 train_acc:  0.5703125 test_cost:  1.6354407 test_acc:  0.53125\n",
            "iter:  156 train_cost:  1.5032802 train_acc:  0.5703125 test_cost:  1.5230865 test_acc:  0.578125\n",
            "iter:  157 train_cost:  1.3397189 train_acc:  0.5859375 test_cost:  1.2337675 test_acc:  0.6171875\n",
            "iter:  158 train_cost:  1.5933008 train_acc:  0.5625 test_cost:  1.2471458 test_acc:  0.609375\n",
            "iter:  159 train_cost:  1.4367583 train_acc:  0.5390625 test_cost:  1.5281038 test_acc:  0.5234375\n",
            "iter:  160 train_cost:  1.4301825 train_acc:  0.5859375 test_cost:  1.4389267 test_acc:  0.5546875\n",
            "iter:  161 train_cost:  1.4171704 train_acc:  0.578125 test_cost:  1.3362889 test_acc:  0.609375\n",
            "iter:  162 train_cost:  1.4678189 train_acc:  0.5703125 test_cost:  1.3792045 test_acc:  0.5703125\n",
            "iter:  163 train_cost:  1.7022312 train_acc:  0.5234375 test_cost:  1.1888016 test_acc:  0.609375\n",
            "iter:  164 train_cost:  1.3163731 train_acc:  0.640625 test_cost:  1.2578647 test_acc:  0.6171875\n",
            "iter:  165 train_cost:  1.4752607 train_acc:  0.5703125 test_cost:  1.1136842 test_acc:  0.609375\n",
            "iter:  166 train_cost:  1.4002149 train_acc:  0.6171875 test_cost:  1.4452423 test_acc:  0.546875\n",
            "iter:  167 train_cost:  1.5566766 train_acc:  0.5625 test_cost:  1.3513299 test_acc:  0.59375\n",
            "iter:  168 train_cost:  1.3982303 train_acc:  0.5859375 test_cost:  1.3981013 test_acc:  0.5234375\n",
            "iter:  169 train_cost:  1.3858976 train_acc:  0.6640625 test_cost:  1.4782457 test_acc:  0.5703125\n",
            "iter:  170 train_cost:  1.3297529 train_acc:  0.5546875 test_cost:  1.3657992 test_acc:  0.609375\n",
            "iter:  171 train_cost:  1.3481529 train_acc:  0.5703125 test_cost:  1.3771625 test_acc:  0.625\n",
            "iter:  172 train_cost:  1.3912065 train_acc:  0.5703125 test_cost:  1.2830553 test_acc:  0.6015625\n",
            "iter:  173 train_cost:  1.3340786 train_acc:  0.5546875 test_cost:  1.3152287 test_acc:  0.5859375\n",
            "iter:  174 train_cost:  1.4687524 train_acc:  0.5546875 test_cost:  1.5173857 test_acc:  0.546875\n",
            "iter:  175 train_cost:  1.1975963 train_acc:  0.6171875 test_cost:  1.5284883 test_acc:  0.578125\n",
            "iter:  176 train_cost:  1.2746468 train_acc:  0.578125 test_cost:  1.3218979 test_acc:  0.5546875\n",
            "iter:  177 train_cost:  1.5192494 train_acc:  0.546875 test_cost:  1.2580231 test_acc:  0.625\n",
            "iter:  178 train_cost:  1.2543427 train_acc:  0.6328125 test_cost:  1.3334465 test_acc:  0.609375\n",
            "iter:  179 train_cost:  1.5759068 train_acc:  0.5625 test_cost:  1.4828513 test_acc:  0.5703125\n",
            "iter:  180 train_cost:  1.594553 train_acc:  0.5078125 test_cost:  1.2411749 test_acc:  0.6015625\n",
            "iter:  181 train_cost:  1.0529842 train_acc:  0.7109375 test_cost:  1.2535517 test_acc:  0.6171875\n",
            "iter:  182 train_cost:  1.6890296 train_acc:  0.53125 test_cost:  1.0506225 test_acc:  0.6015625\n",
            "iter:  183 train_cost:  1.2425067 train_acc:  0.609375 test_cost:  1.3538065 test_acc:  0.5390625\n",
            "iter:  184 train_cost:  0.9289434 train_acc:  0.6796875 test_cost:  1.137069 test_acc:  0.65625\n",
            "iter:  185 train_cost:  1.1863949 train_acc:  0.59375 test_cost:  1.2355397 test_acc:  0.625\n",
            "iter:  186 train_cost:  1.2797277 train_acc:  0.5859375 test_cost:  1.5504496 test_acc:  0.546875\n",
            "iter:  187 train_cost:  1.2788541 train_acc:  0.59375 test_cost:  1.4129863 test_acc:  0.5703125\n",
            "iter:  188 train_cost:  1.180019 train_acc:  0.65625 test_cost:  1.009411 test_acc:  0.6875\n",
            "iter:  189 train_cost:  1.4394981 train_acc:  0.59375 test_cost:  1.1231825 test_acc:  0.65625\n",
            "iter:  190 train_cost:  1.4666846 train_acc:  0.640625 test_cost:  1.2304208 test_acc:  0.6015625\n",
            "iter:  191 train_cost:  1.4520634 train_acc:  0.546875 test_cost:  1.1124864 test_acc:  0.640625\n",
            "iter:  192 train_cost:  1.5223687 train_acc:  0.578125 test_cost:  1.1880064 test_acc:  0.5546875\n",
            "iter:  193 train_cost:  1.3422016 train_acc:  0.625 test_cost:  1.2588694 test_acc:  0.625\n",
            "iter:  194 train_cost:  1.2051417 train_acc:  0.640625 test_cost:  1.23787 test_acc:  0.6484375\n",
            "iter:  195 train_cost:  1.2112986 train_acc:  0.6328125 test_cost:  1.3998091 test_acc:  0.5078125\n",
            "iter:  196 train_cost:  1.0786159 train_acc:  0.6171875 test_cost:  1.2458763 test_acc:  0.6328125\n",
            "iter:  197 train_cost:  1.4452963 train_acc:  0.5390625 test_cost:  1.1803772 test_acc:  0.6328125\n",
            "iter:  198 train_cost:  1.2785828 train_acc:  0.59375 test_cost:  1.3298298 test_acc:  0.59375\n",
            "iter:  199 train_cost:  1.255886 train_acc:  0.5859375 test_cost:  1.3422949 test_acc:  0.5859375\n",
            "iter:  200 train_cost:  1.2903376 train_acc:  0.5703125 test_cost:  1.4180272 test_acc:  0.5625\n",
            "iter:  201 train_cost:  1.1639322 train_acc:  0.6640625 test_cost:  1.1884663 test_acc:  0.6796875\n",
            "iter:  202 train_cost:  1.2584918 train_acc:  0.640625 test_cost:  1.1530714 test_acc:  0.6484375\n",
            "iter:  203 train_cost:  1.1041298 train_acc:  0.6328125 test_cost:  1.2295148 test_acc:  0.6171875\n",
            "iter:  204 train_cost:  1.1998491 train_acc:  0.625 test_cost:  1.0676064 test_acc:  0.703125\n",
            "iter:  205 train_cost:  1.2214867 train_acc:  0.625 test_cost:  1.2307581 test_acc:  0.5859375\n",
            "iter:  206 train_cost:  1.4203197 train_acc:  0.6015625 test_cost:  1.372181 test_acc:  0.625\n",
            "iter:  207 train_cost:  1.229461 train_acc:  0.609375 test_cost:  1.3899238 test_acc:  0.625\n",
            "iter:  208 train_cost:  1.3413069 train_acc:  0.5703125 test_cost:  1.1417136 test_acc:  0.640625\n",
            "iter:  209 train_cost:  1.3273602 train_acc:  0.59375 test_cost:  1.265458 test_acc:  0.6484375\n",
            "iter:  210 train_cost:  1.3957381 train_acc:  0.5546875 test_cost:  1.2574131 test_acc:  0.6015625\n",
            "iter:  211 train_cost:  1.3920696 train_acc:  0.5078125 test_cost:  1.2181969 test_acc:  0.5546875\n",
            "iter:  212 train_cost:  1.2238237 train_acc:  0.6875 test_cost:  1.1330993 test_acc:  0.6484375\n",
            "iter:  213 train_cost:  1.1002505 train_acc:  0.640625 test_cost:  1.1447358 test_acc:  0.640625\n",
            "iter:  214 train_cost:  1.065458 train_acc:  0.625 test_cost:  1.25842 test_acc:  0.59375\n",
            "iter:  215 train_cost:  1.3999326 train_acc:  0.5234375 test_cost:  1.1531298 test_acc:  0.6328125\n",
            "iter:  216 train_cost:  1.3389845 train_acc:  0.6328125 test_cost:  1.1799355 test_acc:  0.6171875\n",
            "iter:  217 train_cost:  1.1710088 train_acc:  0.640625 test_cost:  1.2549087 test_acc:  0.6171875\n",
            "iter:  218 train_cost:  0.994391 train_acc:  0.7421875 test_cost:  1.0182085 test_acc:  0.6875\n",
            "iter:  219 train_cost:  1.1073468 train_acc:  0.65625 test_cost:  1.1893266 test_acc:  0.6171875\n",
            "iter:  220 train_cost:  0.93330026 train_acc:  0.7421875 test_cost:  1.1226261 test_acc:  0.671875\n",
            "iter:  221 train_cost:  1.3707461 train_acc:  0.625 test_cost:  1.1051686 test_acc:  0.609375\n",
            "iter:  222 train_cost:  1.0987272 train_acc:  0.65625 test_cost:  1.0783956 test_acc:  0.671875\n",
            "iter:  223 train_cost:  1.1764824 train_acc:  0.59375 test_cost:  1.1323173 test_acc:  0.671875\n",
            "iter:  224 train_cost:  1.3601353 train_acc:  0.5625 test_cost:  1.0541134 test_acc:  0.65625\n",
            "iter:  225 train_cost:  1.1459117 train_acc:  0.6484375 test_cost:  1.2250128 test_acc:  0.609375\n",
            "iter:  226 train_cost:  1.3466196 train_acc:  0.5859375 test_cost:  0.9492815 test_acc:  0.6640625\n",
            "iter:  227 train_cost:  1.4153366 train_acc:  0.546875 test_cost:  1.1557332 test_acc:  0.640625\n",
            "iter:  228 train_cost:  1.3053222 train_acc:  0.6171875 test_cost:  0.8520211 test_acc:  0.71875\n",
            "iter:  229 train_cost:  1.6055944 train_acc:  0.5703125 test_cost:  1.212935 test_acc:  0.6171875\n",
            "iter:  230 train_cost:  1.0575039 train_acc:  0.6875 test_cost:  1.1850762 test_acc:  0.640625\n",
            "iter:  231 train_cost:  1.2016846 train_acc:  0.6328125 test_cost:  1.1984485 test_acc:  0.625\n",
            "iter:  232 train_cost:  1.1126721 train_acc:  0.6171875 test_cost:  1.0660203 test_acc:  0.625\n",
            "iter:  233 train_cost:  1.0096893 train_acc:  0.6484375 test_cost:  1.0383774 test_acc:  0.609375\n",
            "iter:  234 train_cost:  1.1353528 train_acc:  0.609375 test_cost:  1.0917377 test_acc:  0.6171875\n",
            "iter:  235 train_cost:  1.1825106 train_acc:  0.625 test_cost:  1.0936888 test_acc:  0.6328125\n",
            "iter:  236 train_cost:  0.91358566 train_acc:  0.734375 test_cost:  1.18145 test_acc:  0.625\n",
            "iter:  237 train_cost:  1.2536994 train_acc:  0.6484375 test_cost:  1.2707167 test_acc:  0.6015625\n",
            "iter:  238 train_cost:  1.0580169 train_acc:  0.609375 test_cost:  1.0537616 test_acc:  0.6796875\n",
            "iter:  239 train_cost:  1.2585447 train_acc:  0.640625 test_cost:  0.96545327 test_acc:  0.6875\n",
            "iter:  240 train_cost:  1.1250639 train_acc:  0.671875 test_cost:  1.2145904 test_acc:  0.609375\n",
            "iter:  241 train_cost:  0.9774287 train_acc:  0.6953125 test_cost:  1.0396862 test_acc:  0.6796875\n",
            "iter:  242 train_cost:  1.2395575 train_acc:  0.609375 test_cost:  1.0945522 test_acc:  0.6640625\n",
            "iter:  243 train_cost:  1.2994664 train_acc:  0.6171875 test_cost:  1.1222 test_acc:  0.6171875\n",
            "iter:  244 train_cost:  1.0866446 train_acc:  0.6640625 test_cost:  1.0579402 test_acc:  0.625\n",
            "iter:  245 train_cost:  1.2704501 train_acc:  0.6171875 test_cost:  1.041526 test_acc:  0.6640625\n",
            "iter:  246 train_cost:  0.93085855 train_acc:  0.6796875 test_cost:  1.1619785 test_acc:  0.671875\n",
            "iter:  247 train_cost:  1.1183553 train_acc:  0.6171875 test_cost:  0.9350927 test_acc:  0.7265625\n",
            "iter:  248 train_cost:  0.8359717 train_acc:  0.75 test_cost:  0.9460832 test_acc:  0.671875\n",
            "iter:  249 train_cost:  1.3287337 train_acc:  0.578125 test_cost:  0.93814194 test_acc:  0.7109375\n",
            "iter:  250 train_cost:  0.93084145 train_acc:  0.6953125 test_cost:  1.1480721 test_acc:  0.640625\n",
            "iter:  251 train_cost:  1.1987011 train_acc:  0.65625 test_cost:  1.2268896 test_acc:  0.6796875\n",
            "iter:  252 train_cost:  1.1552396 train_acc:  0.609375 test_cost:  1.0207775 test_acc:  0.6796875\n",
            "iter:  253 train_cost:  1.1280206 train_acc:  0.6171875 test_cost:  0.9631877 test_acc:  0.671875\n",
            "iter:  254 train_cost:  1.0874476 train_acc:  0.6640625 test_cost:  0.7938989 test_acc:  0.734375\n",
            "iter:  255 train_cost:  1.2190818 train_acc:  0.6640625 test_cost:  1.212951 test_acc:  0.5859375\n",
            "iter:  256 train_cost:  1.1565028 train_acc:  0.6328125 test_cost:  1.1503254 test_acc:  0.625\n",
            "iter:  257 train_cost:  1.0733956 train_acc:  0.6875 test_cost:  0.7953919 test_acc:  0.765625\n",
            "iter:  258 train_cost:  1.1018941 train_acc:  0.6796875 test_cost:  1.0198374 test_acc:  0.6953125\n",
            "iter:  259 train_cost:  1.1273196 train_acc:  0.671875 test_cost:  0.92217195 test_acc:  0.6953125\n",
            "iter:  260 train_cost:  1.091175 train_acc:  0.625 test_cost:  1.0278399 test_acc:  0.640625\n",
            "iter:  261 train_cost:  1.2133733 train_acc:  0.625 test_cost:  1.0424073 test_acc:  0.671875\n",
            "iter:  262 train_cost:  0.99116385 train_acc:  0.7265625 test_cost:  1.018293 test_acc:  0.6640625\n",
            "iter:  263 train_cost:  1.0040197 train_acc:  0.703125 test_cost:  1.0324686 test_acc:  0.640625\n",
            "iter:  264 train_cost:  1.1188945 train_acc:  0.703125 test_cost:  1.0324025 test_acc:  0.65625\n",
            "iter:  265 train_cost:  0.87212557 train_acc:  0.75 test_cost:  0.9167728 test_acc:  0.75\n",
            "iter:  266 train_cost:  0.83577883 train_acc:  0.71875 test_cost:  0.87723494 test_acc:  0.71875\n",
            "iter:  267 train_cost:  0.9941238 train_acc:  0.71875 test_cost:  1.1443626 test_acc:  0.71875\n",
            "iter:  268 train_cost:  1.0903335 train_acc:  0.6796875 test_cost:  1.0367994 test_acc:  0.6796875\n",
            "iter:  269 train_cost:  1.0741383 train_acc:  0.6953125 test_cost:  0.8215935 test_acc:  0.734375\n",
            "iter:  270 train_cost:  1.0783241 train_acc:  0.6875 test_cost:  0.9716503 test_acc:  0.671875\n",
            "iter:  271 train_cost:  1.1491995 train_acc:  0.65625 test_cost:  1.0466638 test_acc:  0.671875\n",
            "iter:  272 train_cost:  1.1298034 train_acc:  0.671875 test_cost:  1.0924106 test_acc:  0.6875\n",
            "iter:  273 train_cost:  1.0053933 train_acc:  0.65625 test_cost:  1.1621143 test_acc:  0.6328125\n",
            "iter:  274 train_cost:  1.0282576 train_acc:  0.6484375 test_cost:  0.94010746 test_acc:  0.71875\n",
            "iter:  275 train_cost:  0.9465523 train_acc:  0.7265625 test_cost:  0.8130261 test_acc:  0.7578125\n",
            "iter:  276 train_cost:  1.1349521 train_acc:  0.6484375 test_cost:  0.8666506 test_acc:  0.71875\n",
            "iter:  277 train_cost:  1.0271914 train_acc:  0.671875 test_cost:  0.8619843 test_acc:  0.671875\n",
            "iter:  278 train_cost:  0.97241485 train_acc:  0.6796875 test_cost:  0.7747416 test_acc:  0.7578125\n",
            "iter:  279 train_cost:  1.1025118 train_acc:  0.640625 test_cost:  0.86871433 test_acc:  0.7109375\n",
            "iter:  280 train_cost:  0.9695846 train_acc:  0.65625 test_cost:  1.0585217 test_acc:  0.6796875\n",
            "iter:  281 train_cost:  1.0038285 train_acc:  0.6875 test_cost:  1.0152295 test_acc:  0.71875\n",
            "iter:  282 train_cost:  0.8760546 train_acc:  0.734375 test_cost:  1.0223383 test_acc:  0.6796875\n",
            "iter:  283 train_cost:  1.1486601 train_acc:  0.6796875 test_cost:  1.0238152 test_acc:  0.6953125\n",
            "iter:  284 train_cost:  0.86443454 train_acc:  0.7265625 test_cost:  0.7951352 test_acc:  0.71875\n",
            "iter:  285 train_cost:  0.97799724 train_acc:  0.703125 test_cost:  1.2129855 test_acc:  0.6328125\n",
            "iter:  286 train_cost:  0.89628375 train_acc:  0.734375 test_cost:  0.8930371 test_acc:  0.6796875\n",
            "iter:  287 train_cost:  0.99211 train_acc:  0.7109375 test_cost:  0.9097971 test_acc:  0.6953125\n",
            "iter:  288 train_cost:  1.0400227 train_acc:  0.640625 test_cost:  0.82088256 test_acc:  0.703125\n",
            "iter:  289 train_cost:  0.8773218 train_acc:  0.7421875 test_cost:  0.8842184 test_acc:  0.703125\n",
            "iter:  290 train_cost:  1.0993762 train_acc:  0.640625 test_cost:  0.9074051 test_acc:  0.734375\n",
            "iter:  291 train_cost:  0.86346054 train_acc:  0.7265625 test_cost:  0.7769653 test_acc:  0.734375\n",
            "iter:  292 train_cost:  1.1104677 train_acc:  0.6484375 test_cost:  0.83850527 test_acc:  0.71875\n",
            "iter:  293 train_cost:  1.0415798 train_acc:  0.6796875 test_cost:  0.84799767 test_acc:  0.71875\n",
            "iter:  294 train_cost:  1.0252092 train_acc:  0.6953125 test_cost:  1.0574664 test_acc:  0.6953125\n",
            "iter:  295 train_cost:  0.83551884 train_acc:  0.734375 test_cost:  0.9215051 test_acc:  0.6875\n",
            "iter:  296 train_cost:  0.92867696 train_acc:  0.671875 test_cost:  0.7943844 test_acc:  0.6796875\n",
            "iter:  297 train_cost:  0.9087925 train_acc:  0.7265625 test_cost:  0.99549496 test_acc:  0.7265625\n",
            "iter:  298 train_cost:  1.2421415 train_acc:  0.5859375 test_cost:  1.0325377 test_acc:  0.6953125\n",
            "iter:  299 train_cost:  1.0446391 train_acc:  0.703125 test_cost:  1.0064005 test_acc:  0.6796875\n",
            "iter:  300 train_cost:  1.0379591 train_acc:  0.6796875 test_cost:  1.0726624 test_acc:  0.7109375\n",
            "iter:  301 train_cost:  0.88110894 train_acc:  0.6796875 test_cost:  0.83705103 test_acc:  0.7265625\n",
            "iter:  302 train_cost:  0.92035365 train_acc:  0.6640625 test_cost:  1.0006276 test_acc:  0.6953125\n",
            "iter:  303 train_cost:  0.92567265 train_acc:  0.703125 test_cost:  0.9346269 test_acc:  0.7265625\n",
            "iter:  304 train_cost:  0.98848313 train_acc:  0.6953125 test_cost:  0.90773076 test_acc:  0.7109375\n",
            "iter:  305 train_cost:  0.9268496 train_acc:  0.7109375 test_cost:  0.82364297 test_acc:  0.7265625\n",
            "iter:  306 train_cost:  0.9397178 train_acc:  0.6640625 test_cost:  1.1704626 test_acc:  0.671875\n",
            "iter:  307 train_cost:  0.80930865 train_acc:  0.6796875 test_cost:  0.947715 test_acc:  0.6953125\n",
            "iter:  308 train_cost:  1.0641886 train_acc:  0.7109375 test_cost:  0.8715789 test_acc:  0.7734375\n",
            "iter:  309 train_cost:  0.8985646 train_acc:  0.7421875 test_cost:  0.9206659 test_acc:  0.640625\n",
            "iter:  310 train_cost:  1.0627066 train_acc:  0.7109375 test_cost:  0.947718 test_acc:  0.703125\n",
            "iter:  311 train_cost:  0.89032745 train_acc:  0.734375 test_cost:  0.92027503 test_acc:  0.7109375\n",
            "iter:  312 train_cost:  1.1586468 train_acc:  0.65625 test_cost:  1.0125676 test_acc:  0.7421875\n",
            "iter:  313 train_cost:  0.92285943 train_acc:  0.6953125 test_cost:  0.7834804 test_acc:  0.7421875\n",
            "iter:  314 train_cost:  0.8914171 train_acc:  0.75 test_cost:  0.6472053 test_acc:  0.7890625\n",
            "iter:  315 train_cost:  0.96342885 train_acc:  0.6953125 test_cost:  0.89343226 test_acc:  0.7265625\n",
            "iter:  316 train_cost:  1.1636702 train_acc:  0.671875 test_cost:  1.0319166 test_acc:  0.6875\n",
            "iter:  317 train_cost:  0.87475973 train_acc:  0.6953125 test_cost:  0.9369219 test_acc:  0.71875\n",
            "iter:  318 train_cost:  0.81385946 train_acc:  0.7578125 test_cost:  0.84973186 test_acc:  0.78125\n",
            "iter:  319 train_cost:  0.87359774 train_acc:  0.671875 test_cost:  0.75555784 test_acc:  0.734375\n",
            "iter:  320 train_cost:  0.8755946 train_acc:  0.7265625 test_cost:  0.8771852 test_acc:  0.7265625\n",
            "iter:  321 train_cost:  1.0648327 train_acc:  0.65625 test_cost:  0.9674555 test_acc:  0.71875\n",
            "iter:  322 train_cost:  1.0658226 train_acc:  0.6875 test_cost:  0.9592546 test_acc:  0.6953125\n",
            "iter:  323 train_cost:  0.9654802 train_acc:  0.7109375 test_cost:  0.684839 test_acc:  0.78125\n",
            "iter:  324 train_cost:  0.89023197 train_acc:  0.7421875 test_cost:  0.8364886 test_acc:  0.7109375\n",
            "iter:  325 train_cost:  1.0793784 train_acc:  0.65625 test_cost:  0.94174653 test_acc:  0.7265625\n",
            "iter:  326 train_cost:  0.95524687 train_acc:  0.703125 test_cost:  0.73266137 test_acc:  0.765625\n",
            "iter:  327 train_cost:  0.99573904 train_acc:  0.703125 test_cost:  0.97380126 test_acc:  0.75\n",
            "iter:  328 train_cost:  0.973694 train_acc:  0.671875 test_cost:  0.81741154 test_acc:  0.7265625\n",
            "iter:  329 train_cost:  1.0523052 train_acc:  0.640625 test_cost:  0.9405885 test_acc:  0.6875\n",
            "iter:  330 train_cost:  0.7317982 train_acc:  0.75 test_cost:  1.119164 test_acc:  0.7109375\n",
            "iter:  331 train_cost:  0.9601917 train_acc:  0.65625 test_cost:  0.89919156 test_acc:  0.6953125\n",
            "iter:  332 train_cost:  1.0392212 train_acc:  0.7109375 test_cost:  1.3489969 test_acc:  0.59375\n",
            "iter:  333 train_cost:  0.7931314 train_acc:  0.7734375 test_cost:  0.8671827 test_acc:  0.703125\n",
            "iter:  334 train_cost:  0.8968408 train_acc:  0.7578125 test_cost:  0.8653852 test_acc:  0.734375\n",
            "iter:  335 train_cost:  0.92181146 train_acc:  0.71875 test_cost:  0.78581935 test_acc:  0.71875\n",
            "iter:  336 train_cost:  0.8443145 train_acc:  0.75 test_cost:  0.8924433 test_acc:  0.71875\n",
            "iter:  337 train_cost:  0.89893115 train_acc:  0.7265625 test_cost:  0.76820517 test_acc:  0.7421875\n",
            "iter:  338 train_cost:  1.0154002 train_acc:  0.6953125 test_cost:  0.9291761 test_acc:  0.6796875\n",
            "iter:  339 train_cost:  0.9344696 train_acc:  0.6953125 test_cost:  0.95679677 test_acc:  0.6875\n",
            "iter:  340 train_cost:  0.97892284 train_acc:  0.6328125 test_cost:  0.87368923 test_acc:  0.7265625\n",
            "iter:  341 train_cost:  0.8656584 train_acc:  0.6953125 test_cost:  1.0523928 test_acc:  0.671875\n",
            "iter:  342 train_cost:  0.6198292 train_acc:  0.8046875 test_cost:  0.5437402 test_acc:  0.8203125\n",
            "iter:  343 train_cost:  0.7150686 train_acc:  0.7734375 test_cost:  0.8597745 test_acc:  0.7578125\n",
            "iter:  344 train_cost:  1.0448265 train_acc:  0.65625 test_cost:  0.7485712 test_acc:  0.75\n",
            "iter:  345 train_cost:  1.0278835 train_acc:  0.71875 test_cost:  0.77704006 test_acc:  0.7578125\n",
            "iter:  346 train_cost:  0.88720065 train_acc:  0.7421875 test_cost:  0.9452729 test_acc:  0.7109375\n",
            "iter:  347 train_cost:  0.7619979 train_acc:  0.7265625 test_cost:  0.6238748 test_acc:  0.7578125\n",
            "iter:  348 train_cost:  1.2735715 train_acc:  0.640625 test_cost:  0.9170829 test_acc:  0.6640625\n",
            "iter:  349 train_cost:  0.8460684 train_acc:  0.75 test_cost:  0.7646363 test_acc:  0.7578125\n",
            "iter:  350 train_cost:  0.86831784 train_acc:  0.7734375 test_cost:  0.8742779 test_acc:  0.734375\n",
            "iter:  351 train_cost:  0.86160356 train_acc:  0.7265625 test_cost:  0.69636095 test_acc:  0.765625\n",
            "iter:  352 train_cost:  0.8736507 train_acc:  0.6640625 test_cost:  0.92830443 test_acc:  0.6953125\n",
            "iter:  353 train_cost:  0.7986717 train_acc:  0.7421875 test_cost:  0.89055645 test_acc:  0.71875\n",
            "iter:  354 train_cost:  0.8206223 train_acc:  0.7734375 test_cost:  0.68321055 test_acc:  0.7421875\n",
            "iter:  355 train_cost:  0.72017646 train_acc:  0.7578125 test_cost:  0.90665126 test_acc:  0.75\n",
            "iter:  356 train_cost:  1.0800881 train_acc:  0.671875 test_cost:  0.71474576 test_acc:  0.7734375\n",
            "iter:  357 train_cost:  0.96194714 train_acc:  0.7421875 test_cost:  0.90612227 test_acc:  0.7578125\n",
            "iter:  358 train_cost:  1.0108776 train_acc:  0.671875 test_cost:  0.8083657 test_acc:  0.6875\n",
            "iter:  359 train_cost:  0.7718748 train_acc:  0.75 test_cost:  0.91051173 test_acc:  0.734375\n",
            "iter:  360 train_cost:  1.0945809 train_acc:  0.65625 test_cost:  0.7301794 test_acc:  0.8203125\n",
            "iter:  361 train_cost:  0.870141 train_acc:  0.734375 test_cost:  0.76891154 test_acc:  0.7734375\n",
            "iter:  362 train_cost:  0.8179937 train_acc:  0.703125 test_cost:  0.8471334 test_acc:  0.7109375\n",
            "iter:  363 train_cost:  0.9068014 train_acc:  0.703125 test_cost:  0.8249141 test_acc:  0.703125\n",
            "iter:  364 train_cost:  0.90524936 train_acc:  0.6796875 test_cost:  0.6217946 test_acc:  0.7890625\n",
            "iter:  365 train_cost:  0.6420421 train_acc:  0.78125 test_cost:  0.8126819 test_acc:  0.71875\n",
            "iter:  366 train_cost:  0.9545337 train_acc:  0.734375 test_cost:  0.70698106 test_acc:  0.71875\n",
            "iter:  367 train_cost:  0.97539896 train_acc:  0.6640625 test_cost:  0.9241662 test_acc:  0.6796875\n",
            "iter:  368 train_cost:  0.9410201 train_acc:  0.7265625 test_cost:  0.7339636 test_acc:  0.765625\n",
            "iter:  369 train_cost:  0.95220184 train_acc:  0.6875 test_cost:  0.8702522 test_acc:  0.765625\n",
            "iter:  370 train_cost:  1.0646268 train_acc:  0.7109375 test_cost:  0.8098533 test_acc:  0.734375\n",
            "iter:  371 train_cost:  0.9584307 train_acc:  0.7421875 test_cost:  0.667974 test_acc:  0.8125\n",
            "iter:  372 train_cost:  0.8204363 train_acc:  0.7265625 test_cost:  0.80621237 test_acc:  0.7578125\n",
            "iter:  373 train_cost:  0.95840555 train_acc:  0.75 test_cost:  0.81326926 test_acc:  0.765625\n",
            "iter:  374 train_cost:  0.92364293 train_acc:  0.703125 test_cost:  0.88071877 test_acc:  0.6796875\n",
            "iter:  375 train_cost:  0.9603871 train_acc:  0.7421875 test_cost:  0.9603416 test_acc:  0.75\n",
            "iter:  376 train_cost:  0.9965247 train_acc:  0.6796875 test_cost:  0.92455935 test_acc:  0.6875\n",
            "iter:  377 train_cost:  0.8231834 train_acc:  0.7265625 test_cost:  0.8212959 test_acc:  0.7109375\n",
            "iter:  378 train_cost:  0.87221134 train_acc:  0.7421875 test_cost:  0.82371855 test_acc:  0.7421875\n",
            "iter:  379 train_cost:  0.99540764 train_acc:  0.734375 test_cost:  0.79854804 test_acc:  0.71875\n",
            "iter:  380 train_cost:  0.8841459 train_acc:  0.703125 test_cost:  0.7853651 test_acc:  0.7421875\n",
            "iter:  381 train_cost:  0.868556 train_acc:  0.75 test_cost:  0.66687024 test_acc:  0.7578125\n",
            "iter:  382 train_cost:  0.9115681 train_acc:  0.7265625 test_cost:  0.99442935 test_acc:  0.71875\n",
            "iter:  383 train_cost:  0.6834687 train_acc:  0.7734375 test_cost:  0.76426256 test_acc:  0.78125\n",
            "iter:  384 train_cost:  0.8930881 train_acc:  0.6953125 test_cost:  0.7276885 test_acc:  0.78125\n",
            "iter:  385 train_cost:  0.75236595 train_acc:  0.78125 test_cost:  0.9459427 test_acc:  0.6875\n",
            "iter:  386 train_cost:  0.8125493 train_acc:  0.7265625 test_cost:  0.9064707 test_acc:  0.7421875\n",
            "iter:  387 train_cost:  0.74141717 train_acc:  0.7578125 test_cost:  0.771629 test_acc:  0.78125\n",
            "iter:  388 train_cost:  0.71681297 train_acc:  0.78125 test_cost:  0.7113953 test_acc:  0.8125\n",
            "iter:  389 train_cost:  0.9127325 train_acc:  0.7109375 test_cost:  0.9263046 test_acc:  0.7265625\n",
            "iter:  390 train_cost:  0.98242176 train_acc:  0.6953125 test_cost:  0.7203402 test_acc:  0.8125\n",
            "iter:  391 train_cost:  1.0845528 train_acc:  0.703125 test_cost:  0.89153105 test_acc:  0.7421875\n",
            "iter:  392 train_cost:  0.8364106 train_acc:  0.703125 test_cost:  0.7895549 test_acc:  0.75\n",
            "iter:  393 train_cost:  1.0424336 train_acc:  0.6953125 test_cost:  0.6569077 test_acc:  0.7734375\n",
            "iter:  394 train_cost:  1.04937 train_acc:  0.6484375 test_cost:  0.8169681 test_acc:  0.765625\n",
            "iter:  395 train_cost:  0.90307283 train_acc:  0.734375 test_cost:  0.74956846 test_acc:  0.7578125\n",
            "iter:  396 train_cost:  0.543737 train_acc:  0.84375 test_cost:  0.8136685 test_acc:  0.7578125\n",
            "iter:  397 train_cost:  0.9213425 train_acc:  0.6953125 test_cost:  0.8053138 test_acc:  0.734375\n",
            "iter:  398 train_cost:  0.756632 train_acc:  0.71875 test_cost:  0.84801435 test_acc:  0.71875\n",
            "iter:  399 train_cost:  0.6846446 train_acc:  0.8125 test_cost:  0.9053793 test_acc:  0.703125\n",
            "iter:  400 train_cost:  0.8747538 train_acc:  0.75 test_cost:  0.74986506 test_acc:  0.765625\n",
            "iter:  401 train_cost:  0.7672277 train_acc:  0.765625 test_cost:  0.6473875 test_acc:  0.7578125\n",
            "iter:  402 train_cost:  0.6432228 train_acc:  0.78125 test_cost:  0.70775867 test_acc:  0.75\n",
            "iter:  403 train_cost:  0.56375146 train_acc:  0.8046875 test_cost:  0.7542362 test_acc:  0.7265625\n",
            "iter:  404 train_cost:  0.76812077 train_acc:  0.765625 test_cost:  0.6852537 test_acc:  0.78125\n",
            "iter:  405 train_cost:  0.72022235 train_acc:  0.7890625 test_cost:  0.6869645 test_acc:  0.796875\n",
            "iter:  406 train_cost:  0.8518644 train_acc:  0.78125 test_cost:  0.7612097 test_acc:  0.71875\n",
            "iter:  407 train_cost:  0.91784763 train_acc:  0.75 test_cost:  0.73198104 test_acc:  0.7734375\n",
            "iter:  408 train_cost:  0.8515883 train_acc:  0.7265625 test_cost:  0.68814707 test_acc:  0.75\n",
            "iter:  409 train_cost:  0.856213 train_acc:  0.6796875 test_cost:  0.6757957 test_acc:  0.8046875\n",
            "iter:  410 train_cost:  0.7326418 train_acc:  0.7578125 test_cost:  0.6703391 test_acc:  0.7734375\n",
            "iter:  411 train_cost:  0.86390865 train_acc:  0.7109375 test_cost:  0.72295594 test_acc:  0.796875\n",
            "iter:  412 train_cost:  0.8627774 train_acc:  0.7265625 test_cost:  0.7629565 test_acc:  0.7265625\n",
            "iter:  413 train_cost:  0.82353234 train_acc:  0.734375 test_cost:  0.7380997 test_acc:  0.7421875\n",
            "iter:  414 train_cost:  0.48387837 train_acc:  0.8359375 test_cost:  0.69812363 test_acc:  0.765625\n",
            "iter:  415 train_cost:  0.7289528 train_acc:  0.75 test_cost:  0.71316653 test_acc:  0.796875\n",
            "iter:  416 train_cost:  0.9180949 train_acc:  0.6640625 test_cost:  0.6949783 test_acc:  0.78125\n",
            "iter:  417 train_cost:  0.56514966 train_acc:  0.796875 test_cost:  0.94766915 test_acc:  0.7265625\n",
            "iter:  418 train_cost:  0.63467824 train_acc:  0.7734375 test_cost:  0.79174507 test_acc:  0.7734375\n",
            "iter:  419 train_cost:  0.8179805 train_acc:  0.7109375 test_cost:  0.7679911 test_acc:  0.7109375\n",
            "iter:  420 train_cost:  0.739883 train_acc:  0.765625 test_cost:  0.8560258 test_acc:  0.734375\n",
            "iter:  421 train_cost:  0.6775044 train_acc:  0.734375 test_cost:  0.87233734 test_acc:  0.71875\n",
            "iter:  422 train_cost:  1.0057044 train_acc:  0.7421875 test_cost:  0.7644646 test_acc:  0.78125\n",
            "iter:  423 train_cost:  0.80693376 train_acc:  0.71875 test_cost:  0.67674947 test_acc:  0.7421875\n",
            "iter:  424 train_cost:  0.8984955 train_acc:  0.7421875 test_cost:  0.66317564 test_acc:  0.7734375\n",
            "iter:  425 train_cost:  1.0177448 train_acc:  0.7265625 test_cost:  0.67763203 test_acc:  0.7890625\n",
            "iter:  426 train_cost:  0.9693247 train_acc:  0.703125 test_cost:  0.61327827 test_acc:  0.7890625\n",
            "iter:  427 train_cost:  0.8479012 train_acc:  0.7578125 test_cost:  0.84059787 test_acc:  0.7265625\n",
            "iter:  428 train_cost:  0.872716 train_acc:  0.734375 test_cost:  1.004349 test_acc:  0.75\n",
            "iter:  429 train_cost:  0.8585189 train_acc:  0.7265625 test_cost:  0.90250367 test_acc:  0.7578125\n",
            "iter:  430 train_cost:  0.85342187 train_acc:  0.734375 test_cost:  0.5332078 test_acc:  0.8359375\n",
            "iter:  431 train_cost:  0.6784717 train_acc:  0.7734375 test_cost:  0.850111 test_acc:  0.7421875\n",
            "iter:  432 train_cost:  0.6780103 train_acc:  0.7890625 test_cost:  0.8282322 test_acc:  0.7109375\n",
            "iter:  433 train_cost:  0.7609315 train_acc:  0.7734375 test_cost:  0.67846537 test_acc:  0.7734375\n",
            "iter:  434 train_cost:  0.7152899 train_acc:  0.734375 test_cost:  0.8689068 test_acc:  0.75\n",
            "iter:  435 train_cost:  0.8242339 train_acc:  0.7421875 test_cost:  0.8160899 test_acc:  0.7734375\n",
            "iter:  436 train_cost:  0.7109228 train_acc:  0.796875 test_cost:  0.828845 test_acc:  0.7578125\n",
            "iter:  437 train_cost:  0.50109243 train_acc:  0.8515625 test_cost:  0.6434486 test_acc:  0.7890625\n",
            "iter:  438 train_cost:  0.8356942 train_acc:  0.7265625 test_cost:  0.5715114 test_acc:  0.8203125\n",
            "iter:  439 train_cost:  0.7122443 train_acc:  0.7578125 test_cost:  0.8018682 test_acc:  0.7421875\n",
            "iter:  440 train_cost:  0.74425095 train_acc:  0.765625 test_cost:  0.6038346 test_acc:  0.78125\n",
            "iter:  441 train_cost:  0.73942643 train_acc:  0.796875 test_cost:  0.6780157 test_acc:  0.7734375\n",
            "iter:  442 train_cost:  0.8586513 train_acc:  0.7578125 test_cost:  0.7758071 test_acc:  0.765625\n",
            "iter:  443 train_cost:  0.77510387 train_acc:  0.7578125 test_cost:  0.6849975 test_acc:  0.8046875\n",
            "iter:  444 train_cost:  0.56022215 train_acc:  0.8359375 test_cost:  0.692322 test_acc:  0.75\n",
            "iter:  445 train_cost:  0.7842508 train_acc:  0.7265625 test_cost:  0.71543443 test_acc:  0.7734375\n",
            "iter:  446 train_cost:  0.8744962 train_acc:  0.6953125 test_cost:  0.7998113 test_acc:  0.734375\n",
            "iter:  447 train_cost:  0.7638021 train_acc:  0.7734375 test_cost:  0.690526 test_acc:  0.75\n",
            "iter:  448 train_cost:  0.6951722 train_acc:  0.78125 test_cost:  0.7992891 test_acc:  0.75\n",
            "iter:  449 train_cost:  0.65068996 train_acc:  0.765625 test_cost:  0.6798495 test_acc:  0.78125\n",
            "iter:  450 train_cost:  0.84812367 train_acc:  0.6953125 test_cost:  0.8161213 test_acc:  0.7421875\n",
            "iter:  451 train_cost:  0.7877599 train_acc:  0.7265625 test_cost:  0.6593061 test_acc:  0.8203125\n",
            "iter:  452 train_cost:  0.58790016 train_acc:  0.8046875 test_cost:  0.6836879 test_acc:  0.7578125\n",
            "iter:  453 train_cost:  1.0020374 train_acc:  0.765625 test_cost:  0.6387251 test_acc:  0.8359375\n",
            "iter:  454 train_cost:  0.7767592 train_acc:  0.7109375 test_cost:  0.75972056 test_acc:  0.7578125\n",
            "iter:  455 train_cost:  0.75885886 train_acc:  0.7578125 test_cost:  0.66371715 test_acc:  0.8046875\n",
            "iter:  456 train_cost:  0.51752496 train_acc:  0.828125 test_cost:  0.8591638 test_acc:  0.7265625\n",
            "iter:  457 train_cost:  0.73527473 train_acc:  0.75 test_cost:  0.65819526 test_acc:  0.7890625\n",
            "iter:  458 train_cost:  0.74599624 train_acc:  0.7421875 test_cost:  0.75588584 test_acc:  0.7421875\n",
            "iter:  459 train_cost:  0.66076297 train_acc:  0.78125 test_cost:  0.5387267 test_acc:  0.8046875\n",
            "iter:  460 train_cost:  0.7975952 train_acc:  0.7421875 test_cost:  0.83342636 test_acc:  0.765625\n",
            "iter:  461 train_cost:  0.78242236 train_acc:  0.7734375 test_cost:  0.66134155 test_acc:  0.8046875\n",
            "iter:  462 train_cost:  0.7409518 train_acc:  0.734375 test_cost:  0.713538 test_acc:  0.7890625\n",
            "iter:  463 train_cost:  0.84647524 train_acc:  0.7109375 test_cost:  0.65330803 test_acc:  0.8046875\n",
            "iter:  464 train_cost:  0.6158595 train_acc:  0.8046875 test_cost:  0.5933998 test_acc:  0.8125\n",
            "iter:  465 train_cost:  0.7469878 train_acc:  0.7734375 test_cost:  0.89181185 test_acc:  0.734375\n",
            "iter:  466 train_cost:  0.7759703 train_acc:  0.7265625 test_cost:  0.83542293 test_acc:  0.734375\n",
            "iter:  467 train_cost:  0.85229135 train_acc:  0.7578125 test_cost:  0.6025803 test_acc:  0.8046875\n",
            "iter:  468 train_cost:  0.70694554 train_acc:  0.78125 test_cost:  0.69855213 test_acc:  0.7734375\n",
            "iter:  469 train_cost:  0.73946357 train_acc:  0.7890625 test_cost:  0.61431324 test_acc:  0.796875\n",
            "iter:  470 train_cost:  0.669206 train_acc:  0.8515625 test_cost:  0.7559437 test_acc:  0.78125\n",
            "iter:  471 train_cost:  0.8859328 train_acc:  0.7265625 test_cost:  0.76674855 test_acc:  0.75\n",
            "iter:  472 train_cost:  0.6061123 train_acc:  0.78125 test_cost:  0.7901349 test_acc:  0.7421875\n",
            "iter:  473 train_cost:  0.66880625 train_acc:  0.8046875 test_cost:  0.80802727 test_acc:  0.7890625\n",
            "iter:  474 train_cost:  0.6379386 train_acc:  0.8515625 test_cost:  0.7585578 test_acc:  0.7890625\n",
            "iter:  475 train_cost:  0.69993955 train_acc:  0.734375 test_cost:  0.69817257 test_acc:  0.8125\n",
            "iter:  476 train_cost:  0.5571932 train_acc:  0.84375 test_cost:  0.5819768 test_acc:  0.84375\n",
            "iter:  477 train_cost:  0.68125093 train_acc:  0.7734375 test_cost:  0.92327166 test_acc:  0.71875\n",
            "iter:  478 train_cost:  0.71937656 train_acc:  0.7890625 test_cost:  0.62808883 test_acc:  0.8046875\n",
            "iter:  479 train_cost:  0.7670636 train_acc:  0.7890625 test_cost:  0.5783226 test_acc:  0.828125\n",
            "iter:  480 train_cost:  0.5659224 train_acc:  0.8203125 test_cost:  0.97347784 test_acc:  0.6953125\n",
            "iter:  481 train_cost:  0.862334 train_acc:  0.7265625 test_cost:  0.8751534 test_acc:  0.75\n",
            "iter:  482 train_cost:  0.8123087 train_acc:  0.734375 test_cost:  0.7017823 test_acc:  0.7734375\n",
            "iter:  483 train_cost:  0.8540276 train_acc:  0.6953125 test_cost:  0.6142721 test_acc:  0.7890625\n",
            "iter:  484 train_cost:  0.7752568 train_acc:  0.7421875 test_cost:  0.73923284 test_acc:  0.765625\n",
            "iter:  485 train_cost:  0.8811361 train_acc:  0.734375 test_cost:  0.48493123 test_acc:  0.875\n",
            "iter:  486 train_cost:  0.736468 train_acc:  0.7578125 test_cost:  0.6923021 test_acc:  0.7890625\n",
            "iter:  487 train_cost:  0.64017725 train_acc:  0.796875 test_cost:  0.52910316 test_acc:  0.84375\n",
            "iter:  488 train_cost:  0.8022566 train_acc:  0.765625 test_cost:  0.7467501 test_acc:  0.78125\n",
            "iter:  489 train_cost:  0.8043197 train_acc:  0.7578125 test_cost:  0.65961194 test_acc:  0.8125\n",
            "iter:  490 train_cost:  0.59272075 train_acc:  0.7890625 test_cost:  0.5540657 test_acc:  0.796875\n",
            "iter:  491 train_cost:  0.79314125 train_acc:  0.75 test_cost:  0.56863075 test_acc:  0.8359375\n",
            "iter:  492 train_cost:  0.8432106 train_acc:  0.765625 test_cost:  0.649825 test_acc:  0.7890625\n",
            "iter:  493 train_cost:  0.69023967 train_acc:  0.78125 test_cost:  0.7856758 test_acc:  0.765625\n",
            "iter:  494 train_cost:  0.6640905 train_acc:  0.8203125 test_cost:  0.60323745 test_acc:  0.8046875\n",
            "iter:  495 train_cost:  0.67232823 train_acc:  0.796875 test_cost:  0.8890437 test_acc:  0.7265625\n",
            "iter:  496 train_cost:  0.5727829 train_acc:  0.8203125 test_cost:  0.782315 test_acc:  0.7734375\n",
            "iter:  497 train_cost:  0.58769435 train_acc:  0.8046875 test_cost:  0.59395194 test_acc:  0.8359375\n",
            "iter:  498 train_cost:  0.6049017 train_acc:  0.8125 test_cost:  0.56117123 test_acc:  0.8359375\n",
            "iter:  499 train_cost:  0.678444 train_acc:  0.796875 test_cost:  0.7122215 test_acc:  0.7890625\n",
            "iter:  500 train_cost:  0.5685971 train_acc:  0.8359375 test_cost:  0.8399161 test_acc:  0.75\n",
            "iter:  501 train_cost:  0.6334735 train_acc:  0.7578125 test_cost:  0.7154423 test_acc:  0.7734375\n",
            "iter:  502 train_cost:  0.72336864 train_acc:  0.7890625 test_cost:  0.6143571 test_acc:  0.75\n",
            "iter:  503 train_cost:  0.5039592 train_acc:  0.875 test_cost:  0.81977 test_acc:  0.765625\n",
            "iter:  504 train_cost:  0.7908366 train_acc:  0.7109375 test_cost:  0.6937704 test_acc:  0.796875\n",
            "iter:  505 train_cost:  0.6684879 train_acc:  0.78125 test_cost:  0.61529785 test_acc:  0.78125\n",
            "iter:  506 train_cost:  0.6574136 train_acc:  0.8125 test_cost:  0.5500763 test_acc:  0.828125\n",
            "iter:  507 train_cost:  0.6479737 train_acc:  0.7734375 test_cost:  0.5879247 test_acc:  0.828125\n",
            "iter:  508 train_cost:  0.6012616 train_acc:  0.8046875 test_cost:  0.8116644 test_acc:  0.71875\n",
            "iter:  509 train_cost:  0.6793008 train_acc:  0.7421875 test_cost:  0.6719034 test_acc:  0.7890625\n",
            "iter:  510 train_cost:  0.5441535 train_acc:  0.8125 test_cost:  0.46638614 test_acc:  0.8515625\n",
            "iter:  511 train_cost:  0.79923594 train_acc:  0.7578125 test_cost:  0.6721209 test_acc:  0.7890625\n",
            "iter:  512 train_cost:  0.6346894 train_acc:  0.796875 test_cost:  0.6693882 test_acc:  0.84375\n",
            "iter:  513 train_cost:  0.6139388 train_acc:  0.8125 test_cost:  0.68806833 test_acc:  0.796875\n",
            "iter:  514 train_cost:  0.6814254 train_acc:  0.75 test_cost:  0.6491405 test_acc:  0.8046875\n",
            "iter:  515 train_cost:  0.5987636 train_acc:  0.8203125 test_cost:  0.55700564 test_acc:  0.8671875\n",
            "iter:  516 train_cost:  0.5478405 train_acc:  0.796875 test_cost:  0.6702574 test_acc:  0.7890625\n",
            "iter:  517 train_cost:  0.83723724 train_acc:  0.7265625 test_cost:  0.68848395 test_acc:  0.7578125\n",
            "iter:  518 train_cost:  0.72140014 train_acc:  0.796875 test_cost:  0.67056847 test_acc:  0.7890625\n",
            "iter:  519 train_cost:  0.82112724 train_acc:  0.7578125 test_cost:  0.70290434 test_acc:  0.7734375\n",
            "iter:  520 train_cost:  0.56292534 train_acc:  0.765625 test_cost:  0.70815647 test_acc:  0.78125\n",
            "iter:  521 train_cost:  0.8213539 train_acc:  0.75 test_cost:  0.51745486 test_acc:  0.8203125\n",
            "iter:  522 train_cost:  0.8333489 train_acc:  0.703125 test_cost:  0.6682417 test_acc:  0.8046875\n",
            "iter:  523 train_cost:  0.5195561 train_acc:  0.8203125 test_cost:  0.59929496 test_acc:  0.7734375\n",
            "iter:  524 train_cost:  0.9428482 train_acc:  0.7109375 test_cost:  0.70213604 test_acc:  0.7890625\n",
            "iter:  525 train_cost:  0.7763218 train_acc:  0.7578125 test_cost:  0.6855671 test_acc:  0.765625\n",
            "iter:  526 train_cost:  0.66348505 train_acc:  0.7578125 test_cost:  0.8468299 test_acc:  0.7265625\n",
            "iter:  527 train_cost:  0.6740054 train_acc:  0.765625 test_cost:  0.5442383 test_acc:  0.84375\n",
            "iter:  528 train_cost:  0.68715596 train_acc:  0.78125 test_cost:  0.6720914 test_acc:  0.765625\n",
            "iter:  529 train_cost:  0.5682536 train_acc:  0.796875 test_cost:  0.7635606 test_acc:  0.734375\n",
            "iter:  530 train_cost:  0.7959064 train_acc:  0.78125 test_cost:  0.5713428 test_acc:  0.8046875\n",
            "iter:  531 train_cost:  0.9724181 train_acc:  0.703125 test_cost:  0.66780007 test_acc:  0.75\n",
            "iter:  532 train_cost:  0.56798494 train_acc:  0.8203125 test_cost:  0.6372303 test_acc:  0.8046875\n",
            "iter:  533 train_cost:  0.63058585 train_acc:  0.8046875 test_cost:  0.51548934 test_acc:  0.84375\n",
            "iter:  534 train_cost:  0.64384425 train_acc:  0.7890625 test_cost:  0.6792245 test_acc:  0.7890625\n",
            "iter:  535 train_cost:  0.647138 train_acc:  0.8125 test_cost:  0.63126683 test_acc:  0.8046875\n",
            "iter:  536 train_cost:  0.724094 train_acc:  0.8046875 test_cost:  0.6266404 test_acc:  0.796875\n",
            "iter:  537 train_cost:  0.8368323 train_acc:  0.7421875 test_cost:  0.5709232 test_acc:  0.796875\n",
            "iter:  538 train_cost:  0.676707 train_acc:  0.8046875 test_cost:  0.6872859 test_acc:  0.765625\n",
            "iter:  539 train_cost:  0.6105586 train_acc:  0.7734375 test_cost:  0.6553678 test_acc:  0.8046875\n",
            "iter:  540 train_cost:  0.6862081 train_acc:  0.78125 test_cost:  0.56023026 test_acc:  0.828125\n",
            "iter:  541 train_cost:  0.8432287 train_acc:  0.7109375 test_cost:  0.58082455 test_acc:  0.8203125\n",
            "iter:  542 train_cost:  0.50969654 train_acc:  0.828125 test_cost:  0.6738759 test_acc:  0.796875\n",
            "iter:  543 train_cost:  0.58361673 train_acc:  0.8046875 test_cost:  0.6503811 test_acc:  0.7890625\n",
            "iter:  544 train_cost:  0.6521091 train_acc:  0.8125 test_cost:  0.5378159 test_acc:  0.8046875\n",
            "iter:  545 train_cost:  0.782794 train_acc:  0.7421875 test_cost:  0.6705892 test_acc:  0.8125\n",
            "iter:  546 train_cost:  0.45843035 train_acc:  0.84375 test_cost:  0.7910074 test_acc:  0.7421875\n",
            "iter:  547 train_cost:  0.83696103 train_acc:  0.7265625 test_cost:  0.608338 test_acc:  0.8203125\n",
            "iter:  548 train_cost:  0.58583635 train_acc:  0.8203125 test_cost:  0.57775843 test_acc:  0.828125\n",
            "iter:  549 train_cost:  0.61323106 train_acc:  0.828125 test_cost:  0.59245884 test_acc:  0.84375\n",
            "iter:  550 train_cost:  0.72817016 train_acc:  0.7421875 test_cost:  0.6220301 test_acc:  0.8046875\n",
            "iter:  551 train_cost:  0.66007525 train_acc:  0.7890625 test_cost:  0.6764591 test_acc:  0.8125\n",
            "iter:  552 train_cost:  0.72134906 train_acc:  0.7734375 test_cost:  0.6754352 test_acc:  0.7578125\n",
            "iter:  553 train_cost:  0.76725566 train_acc:  0.765625 test_cost:  0.5135765 test_acc:  0.84375\n",
            "iter:  554 train_cost:  0.7420442 train_acc:  0.7890625 test_cost:  0.7431803 test_acc:  0.7421875\n",
            "iter:  555 train_cost:  0.5212746 train_acc:  0.828125 test_cost:  0.68701744 test_acc:  0.7890625\n",
            "iter:  556 train_cost:  0.5637429 train_acc:  0.7890625 test_cost:  0.6078069 test_acc:  0.7734375\n",
            "iter:  557 train_cost:  0.5650163 train_acc:  0.84375 test_cost:  0.6901753 test_acc:  0.7890625\n",
            "iter:  558 train_cost:  0.6713431 train_acc:  0.8046875 test_cost:  0.5654992 test_acc:  0.8359375\n",
            "iter:  559 train_cost:  0.76677364 train_acc:  0.75 test_cost:  0.5845791 test_acc:  0.84375\n",
            "iter:  560 train_cost:  0.5764326 train_acc:  0.8125 test_cost:  0.67144525 test_acc:  0.8046875\n",
            "iter:  561 train_cost:  0.7635422 train_acc:  0.71875 test_cost:  0.7106394 test_acc:  0.7734375\n",
            "iter:  562 train_cost:  0.5574683 train_acc:  0.84375 test_cost:  0.63531667 test_acc:  0.8046875\n",
            "iter:  563 train_cost:  1.0120325 train_acc:  0.734375 test_cost:  0.7031514 test_acc:  0.7734375\n",
            "iter:  564 train_cost:  0.6557044 train_acc:  0.8046875 test_cost:  0.6672566 test_acc:  0.7890625\n",
            "iter:  565 train_cost:  0.76120234 train_acc:  0.8046875 test_cost:  0.49198103 test_acc:  0.8515625\n",
            "iter:  566 train_cost:  0.8346163 train_acc:  0.734375 test_cost:  0.57336307 test_acc:  0.828125\n",
            "iter:  567 train_cost:  0.5652797 train_acc:  0.8203125 test_cost:  0.5022611 test_acc:  0.828125\n",
            "iter:  568 train_cost:  0.64512163 train_acc:  0.8046875 test_cost:  1.0371705 test_acc:  0.703125\n",
            "iter:  569 train_cost:  0.5632895 train_acc:  0.8203125 test_cost:  0.52811885 test_acc:  0.828125\n",
            "iter:  570 train_cost:  0.73477364 train_acc:  0.7890625 test_cost:  0.66537 test_acc:  0.796875\n",
            "iter:  571 train_cost:  0.8570458 train_acc:  0.75 test_cost:  0.7965638 test_acc:  0.75\n",
            "iter:  572 train_cost:  0.65929717 train_acc:  0.8046875 test_cost:  0.63178396 test_acc:  0.7734375\n",
            "iter:  573 train_cost:  0.6828272 train_acc:  0.8125 test_cost:  0.52385855 test_acc:  0.875\n",
            "iter:  574 train_cost:  0.89526635 train_acc:  0.6953125 test_cost:  0.5849725 test_acc:  0.828125\n",
            "iter:  575 train_cost:  0.63322175 train_acc:  0.828125 test_cost:  0.5748203 test_acc:  0.8359375\n",
            "iter:  576 train_cost:  0.56969815 train_acc:  0.828125 test_cost:  0.49697423 test_acc:  0.84375\n",
            "iter:  577 train_cost:  0.6123586 train_acc:  0.7734375 test_cost:  0.52139884 test_acc:  0.828125\n",
            "iter:  578 train_cost:  0.6218939 train_acc:  0.765625 test_cost:  0.41485268 test_acc:  0.8515625\n",
            "iter:  579 train_cost:  0.6714643 train_acc:  0.8125 test_cost:  0.6653503 test_acc:  0.78125\n",
            "iter:  580 train_cost:  0.7398465 train_acc:  0.7734375 test_cost:  0.66100425 test_acc:  0.8046875\n",
            "iter:  581 train_cost:  0.66737187 train_acc:  0.8203125 test_cost:  0.6756933 test_acc:  0.796875\n",
            "iter:  582 train_cost:  0.6646858 train_acc:  0.8203125 test_cost:  0.65648425 test_acc:  0.8203125\n",
            "iter:  583 train_cost:  0.7452893 train_acc:  0.765625 test_cost:  0.5505624 test_acc:  0.8125\n",
            "iter:  584 train_cost:  0.56925124 train_acc:  0.828125 test_cost:  0.63391966 test_acc:  0.828125\n",
            "iter:  585 train_cost:  0.6003635 train_acc:  0.8203125 test_cost:  0.6631253 test_acc:  0.78125\n",
            "iter:  586 train_cost:  0.6163986 train_acc:  0.75 test_cost:  0.5921303 test_acc:  0.78125\n",
            "iter:  587 train_cost:  0.62222797 train_acc:  0.7890625 test_cost:  0.66198236 test_acc:  0.828125\n",
            "iter:  588 train_cost:  0.57558465 train_acc:  0.765625 test_cost:  0.8939152 test_acc:  0.7265625\n",
            "iter:  589 train_cost:  0.6827221 train_acc:  0.8125 test_cost:  0.6218938 test_acc:  0.796875\n",
            "iter:  590 train_cost:  0.7786711 train_acc:  0.8046875 test_cost:  0.4938815 test_acc:  0.8671875\n",
            "iter:  591 train_cost:  0.560341 train_acc:  0.8515625 test_cost:  0.72109675 test_acc:  0.7578125\n",
            "iter:  592 train_cost:  0.70125014 train_acc:  0.71875 test_cost:  0.59564006 test_acc:  0.8125\n",
            "iter:  593 train_cost:  0.64995587 train_acc:  0.765625 test_cost:  0.65480304 test_acc:  0.8203125\n",
            "iter:  594 train_cost:  0.62557817 train_acc:  0.8203125 test_cost:  0.5920596 test_acc:  0.8203125\n",
            "iter:  595 train_cost:  0.7147097 train_acc:  0.7734375 test_cost:  0.4579016 test_acc:  0.875\n",
            "iter:  596 train_cost:  0.54651713 train_acc:  0.8125 test_cost:  0.5183805 test_acc:  0.8515625\n",
            "iter:  597 train_cost:  0.52495086 train_acc:  0.828125 test_cost:  0.4588824 test_acc:  0.8828125\n",
            "iter:  598 train_cost:  0.7042226 train_acc:  0.8125 test_cost:  0.37157828 test_acc:  0.8515625\n",
            "iter:  599 train_cost:  0.5331912 train_acc:  0.796875 test_cost:  0.84008 test_acc:  0.7578125\n",
            "iter:  600 train_cost:  0.6291139 train_acc:  0.796875 test_cost:  0.6695775 test_acc:  0.7734375\n",
            "iter:  601 train_cost:  0.57781684 train_acc:  0.8203125 test_cost:  0.61445355 test_acc:  0.828125\n",
            "iter:  602 train_cost:  0.59769255 train_acc:  0.7734375 test_cost:  0.6006464 test_acc:  0.828125\n",
            "iter:  603 train_cost:  0.78831464 train_acc:  0.7890625 test_cost:  0.7285118 test_acc:  0.78125\n",
            "iter:  604 train_cost:  0.591871 train_acc:  0.8125 test_cost:  0.7667572 test_acc:  0.7265625\n",
            "iter:  605 train_cost:  0.5480778 train_acc:  0.84375 test_cost:  0.48212862 test_acc:  0.8515625\n",
            "iter:  606 train_cost:  0.6470009 train_acc:  0.7890625 test_cost:  0.56951004 test_acc:  0.828125\n",
            "iter:  607 train_cost:  0.78950536 train_acc:  0.7578125 test_cost:  0.5500493 test_acc:  0.8203125\n",
            "iter:  608 train_cost:  0.39297527 train_acc:  0.875 test_cost:  0.5568968 test_acc:  0.8203125\n",
            "iter:  609 train_cost:  0.73831093 train_acc:  0.765625 test_cost:  0.7229425 test_acc:  0.8046875\n",
            "iter:  610 train_cost:  0.6556286 train_acc:  0.8046875 test_cost:  0.7422958 test_acc:  0.75\n",
            "iter:  611 train_cost:  0.6770655 train_acc:  0.8203125 test_cost:  0.6335633 test_acc:  0.8125\n",
            "iter:  612 train_cost:  0.5682434 train_acc:  0.828125 test_cost:  0.6570288 test_acc:  0.8046875\n",
            "iter:  613 train_cost:  0.57157683 train_acc:  0.859375 test_cost:  0.6417421 test_acc:  0.8046875\n",
            "iter:  614 train_cost:  0.58358586 train_acc:  0.8046875 test_cost:  0.62321246 test_acc:  0.8046875\n",
            "iter:  615 train_cost:  0.74420464 train_acc:  0.7734375 test_cost:  0.6919974 test_acc:  0.78125\n",
            "iter:  616 train_cost:  0.9459231 train_acc:  0.765625 test_cost:  0.6334657 test_acc:  0.796875\n",
            "iter:  617 train_cost:  0.490091 train_acc:  0.8671875 test_cost:  0.627615 test_acc:  0.8125\n",
            "iter:  618 train_cost:  0.6153684 train_acc:  0.8125 test_cost:  0.57546985 test_acc:  0.796875\n",
            "iter:  619 train_cost:  0.6580815 train_acc:  0.828125 test_cost:  0.5791498 test_acc:  0.8203125\n",
            "iter:  620 train_cost:  0.7078258 train_acc:  0.8046875 test_cost:  0.5901741 test_acc:  0.796875\n",
            "iter:  621 train_cost:  0.74111027 train_acc:  0.75 test_cost:  0.6810949 test_acc:  0.7890625\n",
            "iter:  622 train_cost:  0.562312 train_acc:  0.8515625 test_cost:  0.5798458 test_acc:  0.84375\n",
            "iter:  623 train_cost:  0.56728846 train_acc:  0.8125 test_cost:  0.6951044 test_acc:  0.796875\n",
            "iter:  624 train_cost:  0.693856 train_acc:  0.7890625 test_cost:  0.41467598 test_acc:  0.8359375\n",
            "iter:  625 train_cost:  0.6660828 train_acc:  0.78125 test_cost:  0.65674317 test_acc:  0.796875\n",
            "iter:  626 train_cost:  0.6132311 train_acc:  0.7890625 test_cost:  0.40993494 test_acc:  0.8671875\n",
            "iter:  627 train_cost:  0.8665372 train_acc:  0.7265625 test_cost:  0.48135933 test_acc:  0.8203125\n",
            "iter:  628 train_cost:  0.6076273 train_acc:  0.7890625 test_cost:  0.60231805 test_acc:  0.7890625\n",
            "iter:  629 train_cost:  0.6849762 train_acc:  0.7890625 test_cost:  0.534196 test_acc:  0.859375\n",
            "iter:  630 train_cost:  0.5676762 train_acc:  0.8515625 test_cost:  0.5871155 test_acc:  0.8359375\n",
            "iter:  631 train_cost:  0.57067895 train_acc:  0.78125 test_cost:  0.66470605 test_acc:  0.8046875\n",
            "iter:  632 train_cost:  0.70296884 train_acc:  0.7734375 test_cost:  0.47298816 test_acc:  0.8515625\n",
            "iter:  633 train_cost:  0.51313233 train_acc:  0.84375 test_cost:  0.5900835 test_acc:  0.8359375\n",
            "iter:  634 train_cost:  0.52414834 train_acc:  0.8671875 test_cost:  0.57950294 test_acc:  0.796875\n",
            "iter:  635 train_cost:  0.54092544 train_acc:  0.8359375 test_cost:  0.55869925 test_acc:  0.8359375\n",
            "iter:  636 train_cost:  0.7236111 train_acc:  0.7734375 test_cost:  0.5166035 test_acc:  0.828125\n",
            "iter:  637 train_cost:  0.7167889 train_acc:  0.796875 test_cost:  0.7129446 test_acc:  0.765625\n",
            "iter:  638 train_cost:  0.6176118 train_acc:  0.78125 test_cost:  0.65537155 test_acc:  0.8046875\n",
            "iter:  639 train_cost:  0.6441591 train_acc:  0.7890625 test_cost:  0.8331388 test_acc:  0.734375\n",
            "iter:  640 train_cost:  0.7355042 train_acc:  0.765625 test_cost:  0.5656184 test_acc:  0.8203125\n",
            "iter:  641 train_cost:  0.8117165 train_acc:  0.765625 test_cost:  0.688596 test_acc:  0.796875\n",
            "iter:  642 train_cost:  0.5864278 train_acc:  0.796875 test_cost:  0.5031717 test_acc:  0.859375\n",
            "iter:  643 train_cost:  0.5027954 train_acc:  0.8359375 test_cost:  0.39903164 test_acc:  0.875\n",
            "iter:  644 train_cost:  0.4498937 train_acc:  0.84375 test_cost:  0.5377795 test_acc:  0.828125\n",
            "iter:  645 train_cost:  0.58472645 train_acc:  0.8203125 test_cost:  0.54301685 test_acc:  0.8125\n",
            "iter:  646 train_cost:  0.63861066 train_acc:  0.78125 test_cost:  0.53290045 test_acc:  0.8515625\n",
            "iter:  647 train_cost:  0.5188557 train_acc:  0.84375 test_cost:  0.5958798 test_acc:  0.7890625\n",
            "iter:  648 train_cost:  0.50528777 train_acc:  0.8125 test_cost:  0.64500517 test_acc:  0.8203125\n",
            "iter:  649 train_cost:  0.6067447 train_acc:  0.8046875 test_cost:  0.60913324 test_acc:  0.7734375\n",
            "iter:  650 train_cost:  0.47752428 train_acc:  0.828125 test_cost:  0.61667097 test_acc:  0.8203125\n",
            "iter:  651 train_cost:  0.62955046 train_acc:  0.8203125 test_cost:  0.63045347 test_acc:  0.828125\n",
            "iter:  652 train_cost:  0.72690713 train_acc:  0.796875 test_cost:  0.48598054 test_acc:  0.828125\n",
            "iter:  653 train_cost:  0.826704 train_acc:  0.75 test_cost:  0.53094155 test_acc:  0.8515625\n",
            "iter:  654 train_cost:  0.75108796 train_acc:  0.78125 test_cost:  0.4458261 test_acc:  0.84375\n",
            "iter:  655 train_cost:  0.46956503 train_acc:  0.875 test_cost:  0.6776396 test_acc:  0.78125\n",
            "iter:  656 train_cost:  0.8265394 train_acc:  0.734375 test_cost:  0.45314145 test_acc:  0.8984375\n",
            "iter:  657 train_cost:  0.49525207 train_acc:  0.859375 test_cost:  0.5059977 test_acc:  0.875\n",
            "iter:  658 train_cost:  0.55678976 train_acc:  0.8203125 test_cost:  0.55387807 test_acc:  0.8203125\n",
            "iter:  659 train_cost:  0.47744012 train_acc:  0.84375 test_cost:  0.50833416 test_acc:  0.796875\n",
            "iter:  660 train_cost:  0.5496788 train_acc:  0.84375 test_cost:  0.42416888 test_acc:  0.875\n",
            "iter:  661 train_cost:  0.68592334 train_acc:  0.8046875 test_cost:  0.54512715 test_acc:  0.8515625\n",
            "iter:  662 train_cost:  0.56527865 train_acc:  0.7890625 test_cost:  0.55353105 test_acc:  0.8203125\n",
            "iter:  663 train_cost:  0.52746373 train_acc:  0.8125 test_cost:  0.66599256 test_acc:  0.8046875\n",
            "iter:  664 train_cost:  0.6336063 train_acc:  0.8046875 test_cost:  0.63341135 test_acc:  0.8046875\n",
            "iter:  665 train_cost:  0.4410154 train_acc:  0.8671875 test_cost:  0.68530047 test_acc:  0.796875\n",
            "iter:  666 train_cost:  0.51475966 train_acc:  0.8125 test_cost:  0.59623814 test_acc:  0.796875\n",
            "iter:  667 train_cost:  0.59229547 train_acc:  0.796875 test_cost:  0.529416 test_acc:  0.8046875\n",
            "iter:  668 train_cost:  0.5872584 train_acc:  0.8359375 test_cost:  0.5318812 test_acc:  0.84375\n",
            "iter:  669 train_cost:  0.5982634 train_acc:  0.8046875 test_cost:  0.5268278 test_acc:  0.8515625\n",
            "iter:  670 train_cost:  0.80606836 train_acc:  0.7265625 test_cost:  0.6139163 test_acc:  0.828125\n",
            "iter:  671 train_cost:  0.63406384 train_acc:  0.796875 test_cost:  0.63929975 test_acc:  0.796875\n",
            "iter:  672 train_cost:  0.7318289 train_acc:  0.8046875 test_cost:  0.6521281 test_acc:  0.796875\n",
            "iter:  673 train_cost:  0.6308603 train_acc:  0.828125 test_cost:  0.59054595 test_acc:  0.8125\n",
            "iter:  674 train_cost:  0.38848484 train_acc:  0.875 test_cost:  0.6831639 test_acc:  0.7890625\n",
            "iter:  675 train_cost:  0.72238564 train_acc:  0.7734375 test_cost:  0.5341758 test_acc:  0.8359375\n",
            "iter:  676 train_cost:  0.5917206 train_acc:  0.78125 test_cost:  0.7064674 test_acc:  0.765625\n",
            "iter:  677 train_cost:  0.5157361 train_acc:  0.828125 test_cost:  0.5392231 test_acc:  0.8515625\n",
            "iter:  678 train_cost:  0.6742637 train_acc:  0.78125 test_cost:  0.3815328 test_acc:  0.90625\n",
            "iter:  679 train_cost:  0.5189858 train_acc:  0.828125 test_cost:  0.59239733 test_acc:  0.828125\n",
            "iter:  680 train_cost:  0.4955218 train_acc:  0.859375 test_cost:  0.6405632 test_acc:  0.8046875\n",
            "iter:  681 train_cost:  0.44488782 train_acc:  0.8671875 test_cost:  0.66990644 test_acc:  0.8359375\n",
            "iter:  682 train_cost:  0.66076815 train_acc:  0.8046875 test_cost:  0.5816598 test_acc:  0.828125\n",
            "iter:  683 train_cost:  0.599826 train_acc:  0.765625 test_cost:  0.5129949 test_acc:  0.8203125\n",
            "iter:  684 train_cost:  0.57686716 train_acc:  0.8203125 test_cost:  0.7477753 test_acc:  0.796875\n",
            "iter:  685 train_cost:  0.6656692 train_acc:  0.8046875 test_cost:  0.8212474 test_acc:  0.7578125\n",
            "iter:  686 train_cost:  0.52488965 train_acc:  0.859375 test_cost:  0.51867324 test_acc:  0.8359375\n",
            "iter:  687 train_cost:  0.58749396 train_acc:  0.8125 test_cost:  0.7990911 test_acc:  0.78125\n",
            "iter:  688 train_cost:  0.73695827 train_acc:  0.796875 test_cost:  0.46332678 test_acc:  0.8203125\n",
            "iter:  689 train_cost:  0.43312824 train_acc:  0.875 test_cost:  0.5713353 test_acc:  0.828125\n",
            "iter:  690 train_cost:  0.59973437 train_acc:  0.828125 test_cost:  0.6633084 test_acc:  0.78125\n",
            "iter:  691 train_cost:  0.48668057 train_acc:  0.828125 test_cost:  0.60960305 test_acc:  0.8046875\n",
            "iter:  692 train_cost:  0.57875025 train_acc:  0.796875 test_cost:  0.5535538 test_acc:  0.8359375\n",
            "iter:  693 train_cost:  0.58265215 train_acc:  0.8203125 test_cost:  0.66892827 test_acc:  0.796875\n",
            "iter:  694 train_cost:  0.5335669 train_acc:  0.8515625 test_cost:  0.6348084 test_acc:  0.828125\n",
            "iter:  695 train_cost:  0.679688 train_acc:  0.8046875 test_cost:  0.5041247 test_acc:  0.8203125\n",
            "iter:  696 train_cost:  0.40193772 train_acc:  0.8984375 test_cost:  0.50039923 test_acc:  0.8359375\n",
            "iter:  697 train_cost:  0.62756944 train_acc:  0.7734375 test_cost:  0.59857875 test_acc:  0.8359375\n",
            "iter:  698 train_cost:  0.63763773 train_acc:  0.828125 test_cost:  0.6464932 test_acc:  0.8046875\n",
            "iter:  699 train_cost:  0.4188925 train_acc:  0.8828125 test_cost:  0.47557718 test_acc:  0.84375\n",
            "iter:  700 train_cost:  0.60626745 train_acc:  0.8046875 test_cost:  0.65375453 test_acc:  0.8046875\n",
            "iter:  701 train_cost:  0.6447083 train_acc:  0.8203125 test_cost:  0.477436 test_acc:  0.8671875\n",
            "iter:  702 train_cost:  0.6810987 train_acc:  0.8046875 test_cost:  0.6027857 test_acc:  0.8359375\n",
            "iter:  703 train_cost:  0.7104511 train_acc:  0.8359375 test_cost:  0.4676611 test_acc:  0.859375\n",
            "iter:  704 train_cost:  0.6457578 train_acc:  0.8046875 test_cost:  0.51053894 test_acc:  0.8203125\n",
            "iter:  705 train_cost:  0.57656336 train_acc:  0.8203125 test_cost:  0.3741274 test_acc:  0.890625\n",
            "iter:  706 train_cost:  0.43002042 train_acc:  0.8515625 test_cost:  0.7248689 test_acc:  0.765625\n",
            "iter:  707 train_cost:  0.37910885 train_acc:  0.859375 test_cost:  0.6836252 test_acc:  0.796875\n",
            "iter:  708 train_cost:  0.5853946 train_acc:  0.8046875 test_cost:  0.6511836 test_acc:  0.8203125\n",
            "iter:  709 train_cost:  0.66790104 train_acc:  0.8203125 test_cost:  0.58377695 test_acc:  0.8125\n",
            "iter:  710 train_cost:  0.5755551 train_acc:  0.84375 test_cost:  0.59631306 test_acc:  0.8203125\n",
            "iter:  711 train_cost:  0.30771464 train_acc:  0.90625 test_cost:  0.5457038 test_acc:  0.8359375\n",
            "iter:  712 train_cost:  0.41407615 train_acc:  0.8828125 test_cost:  0.6140863 test_acc:  0.828125\n",
            "iter:  713 train_cost:  0.50628656 train_acc:  0.8359375 test_cost:  0.6435147 test_acc:  0.8359375\n",
            "iter:  714 train_cost:  0.50916296 train_acc:  0.8515625 test_cost:  0.62584317 test_acc:  0.8046875\n",
            "iter:  715 train_cost:  0.5230906 train_acc:  0.8359375 test_cost:  0.4070875 test_acc:  0.875\n",
            "iter:  716 train_cost:  0.5280466 train_acc:  0.8125 test_cost:  0.6295665 test_acc:  0.765625\n",
            "iter:  717 train_cost:  0.4399075 train_acc:  0.8359375 test_cost:  0.6514467 test_acc:  0.796875\n",
            "iter:  718 train_cost:  0.56645465 train_acc:  0.8515625 test_cost:  0.5161973 test_acc:  0.84375\n",
            "iter:  719 train_cost:  0.5243932 train_acc:  0.84375 test_cost:  0.5226003 test_acc:  0.84375\n",
            "iter:  720 train_cost:  0.6138705 train_acc:  0.8125 test_cost:  0.55080557 test_acc:  0.78125\n",
            "iter:  721 train_cost:  0.6720912 train_acc:  0.7890625 test_cost:  0.43401983 test_acc:  0.8671875\n",
            "iter:  722 train_cost:  0.60746765 train_acc:  0.8046875 test_cost:  0.5483755 test_acc:  0.8359375\n",
            "iter:  723 train_cost:  0.6136031 train_acc:  0.8359375 test_cost:  0.69625664 test_acc:  0.7734375\n",
            "iter:  724 train_cost:  0.40934685 train_acc:  0.859375 test_cost:  0.58529603 test_acc:  0.7734375\n",
            "iter:  725 train_cost:  0.60913825 train_acc:  0.828125 test_cost:  0.56920207 test_acc:  0.8125\n",
            "iter:  726 train_cost:  0.5160153 train_acc:  0.8046875 test_cost:  0.5360917 test_acc:  0.8203125\n",
            "iter:  727 train_cost:  0.8438478 train_acc:  0.78125 test_cost:  0.43906528 test_acc:  0.8515625\n",
            "iter:  728 train_cost:  0.5125861 train_acc:  0.8203125 test_cost:  0.4404354 test_acc:  0.859375\n",
            "iter:  729 train_cost:  0.46879786 train_acc:  0.8515625 test_cost:  0.51144874 test_acc:  0.8359375\n",
            "iter:  730 train_cost:  0.673578 train_acc:  0.796875 test_cost:  0.43891987 test_acc:  0.84375\n",
            "iter:  731 train_cost:  0.40702087 train_acc:  0.859375 test_cost:  0.5863495 test_acc:  0.8359375\n",
            "iter:  732 train_cost:  0.43047678 train_acc:  0.828125 test_cost:  0.57511085 test_acc:  0.8125\n",
            "iter:  733 train_cost:  0.43683285 train_acc:  0.859375 test_cost:  0.6384345 test_acc:  0.796875\n",
            "iter:  734 train_cost:  0.5537796 train_acc:  0.8046875 test_cost:  0.5170998 test_acc:  0.8359375\n",
            "iter:  735 train_cost:  0.5218313 train_acc:  0.8125 test_cost:  0.5177279 test_acc:  0.8359375\n",
            "iter:  736 train_cost:  0.6754446 train_acc:  0.7734375 test_cost:  0.474836 test_acc:  0.84375\n",
            "iter:  737 train_cost:  0.6288719 train_acc:  0.796875 test_cost:  0.60702467 test_acc:  0.8046875\n",
            "iter:  738 train_cost:  0.48936364 train_acc:  0.8359375 test_cost:  0.6734688 test_acc:  0.796875\n",
            "iter:  739 train_cost:  0.42516434 train_acc:  0.8671875 test_cost:  0.4577355 test_acc:  0.8984375\n",
            "iter:  740 train_cost:  0.55286247 train_acc:  0.8125 test_cost:  0.43871963 test_acc:  0.8359375\n",
            "iter:  741 train_cost:  0.56923544 train_acc:  0.828125 test_cost:  0.69437057 test_acc:  0.78125\n",
            "iter:  742 train_cost:  0.68805814 train_acc:  0.765625 test_cost:  0.5682157 test_acc:  0.828125\n",
            "iter:  743 train_cost:  0.43878952 train_acc:  0.859375 test_cost:  0.47878975 test_acc:  0.8515625\n",
            "iter:  744 train_cost:  0.50801075 train_acc:  0.8125 test_cost:  0.7671382 test_acc:  0.8046875\n",
            "iter:  745 train_cost:  0.54298294 train_acc:  0.8359375 test_cost:  0.5131484 test_acc:  0.828125\n",
            "iter:  746 train_cost:  0.43240088 train_acc:  0.84375 test_cost:  0.6069776 test_acc:  0.7890625\n",
            "iter:  747 train_cost:  0.62525994 train_acc:  0.8125 test_cost:  0.5099361 test_acc:  0.8515625\n",
            "iter:  748 train_cost:  0.64353263 train_acc:  0.8046875 test_cost:  0.5935189 test_acc:  0.8125\n",
            "iter:  749 train_cost:  0.677401 train_acc:  0.7734375 test_cost:  0.5620283 test_acc:  0.8203125\n",
            "iter:  750 train_cost:  0.70059335 train_acc:  0.734375 test_cost:  0.59661925 test_acc:  0.8203125\n",
            "iter:  751 train_cost:  0.45727223 train_acc:  0.84375 test_cost:  0.33244747 test_acc:  0.8828125\n",
            "iter:  752 train_cost:  0.5068102 train_acc:  0.84375 test_cost:  0.6091729 test_acc:  0.828125\n",
            "iter:  753 train_cost:  0.59640485 train_acc:  0.8359375 test_cost:  0.6686075 test_acc:  0.78125\n",
            "iter:  754 train_cost:  0.5869179 train_acc:  0.8515625 test_cost:  0.55884933 test_acc:  0.828125\n",
            "iter:  755 train_cost:  0.3763873 train_acc:  0.875 test_cost:  0.56550336 test_acc:  0.8203125\n",
            "iter:  756 train_cost:  0.77395546 train_acc:  0.796875 test_cost:  0.56894624 test_acc:  0.8046875\n",
            "iter:  757 train_cost:  0.4992358 train_acc:  0.828125 test_cost:  0.33086714 test_acc:  0.8984375\n",
            "iter:  758 train_cost:  0.44531915 train_acc:  0.8828125 test_cost:  0.72793454 test_acc:  0.796875\n",
            "iter:  759 train_cost:  0.6750655 train_acc:  0.828125 test_cost:  0.5156577 test_acc:  0.828125\n",
            "iter:  760 train_cost:  0.6766367 train_acc:  0.8125 test_cost:  0.49807724 test_acc:  0.8515625\n",
            "iter:  761 train_cost:  0.56991005 train_acc:  0.8203125 test_cost:  0.5537663 test_acc:  0.78125\n",
            "iter:  762 train_cost:  0.65998447 train_acc:  0.78125 test_cost:  0.5049529 test_acc:  0.8359375\n",
            "iter:  763 train_cost:  0.55139625 train_acc:  0.84375 test_cost:  0.45222092 test_acc:  0.8671875\n",
            "iter:  764 train_cost:  0.5632373 train_acc:  0.8515625 test_cost:  0.41203824 test_acc:  0.890625\n",
            "iter:  765 train_cost:  0.5641317 train_acc:  0.8515625 test_cost:  0.48269212 test_acc:  0.8515625\n",
            "iter:  766 train_cost:  0.6785999 train_acc:  0.7890625 test_cost:  0.7546226 test_acc:  0.78125\n",
            "iter:  767 train_cost:  0.42328945 train_acc:  0.828125 test_cost:  0.5053906 test_acc:  0.8515625\n",
            "iter:  768 train_cost:  0.4923941 train_acc:  0.8515625 test_cost:  0.50098515 test_acc:  0.84375\n",
            "iter:  769 train_cost:  0.7748722 train_acc:  0.7890625 test_cost:  0.7303491 test_acc:  0.78125\n",
            "iter:  770 train_cost:  0.5697037 train_acc:  0.8515625 test_cost:  0.69821954 test_acc:  0.796875\n",
            "iter:  771 train_cost:  0.4717836 train_acc:  0.8515625 test_cost:  0.710588 test_acc:  0.7890625\n",
            "iter:  772 train_cost:  0.31589815 train_acc:  0.8515625 test_cost:  0.5474025 test_acc:  0.859375\n",
            "iter:  773 train_cost:  0.52101195 train_acc:  0.859375 test_cost:  0.30082363 test_acc:  0.9296875\n",
            "iter:  774 train_cost:  0.4809603 train_acc:  0.8359375 test_cost:  0.54736674 test_acc:  0.8515625\n",
            "iter:  775 train_cost:  0.78633404 train_acc:  0.7734375 test_cost:  0.6168237 test_acc:  0.8046875\n",
            "iter:  776 train_cost:  0.5011109 train_acc:  0.859375 test_cost:  0.42952305 test_acc:  0.8828125\n",
            "iter:  777 train_cost:  0.7213848 train_acc:  0.7578125 test_cost:  0.5101033 test_acc:  0.859375\n",
            "iter:  778 train_cost:  0.64642626 train_acc:  0.7734375 test_cost:  0.46512192 test_acc:  0.828125\n",
            "iter:  779 train_cost:  0.5274501 train_acc:  0.8359375 test_cost:  0.4945858 test_acc:  0.8515625\n",
            "iter:  780 train_cost:  0.49782792 train_acc:  0.859375 test_cost:  0.67335445 test_acc:  0.8046875\n",
            "iter:  781 train_cost:  0.5024668 train_acc:  0.84375 test_cost:  0.50352216 test_acc:  0.8203125\n",
            "iter:  782 train_cost:  0.50765693 train_acc:  0.8125 test_cost:  0.69368875 test_acc:  0.765625\n",
            "iter:  783 train_cost:  0.38130632 train_acc:  0.890625 test_cost:  0.34051803 test_acc:  0.9140625\n",
            "iter:  784 train_cost:  0.66623265 train_acc:  0.7890625 test_cost:  0.5379381 test_acc:  0.8359375\n",
            "iter:  785 train_cost:  0.72797513 train_acc:  0.7890625 test_cost:  0.48303765 test_acc:  0.8515625\n",
            "iter:  786 train_cost:  0.5102017 train_acc:  0.859375 test_cost:  0.6784528 test_acc:  0.8203125\n",
            "iter:  787 train_cost:  0.4281316 train_acc:  0.875 test_cost:  0.46418682 test_acc:  0.8515625\n",
            "iter:  788 train_cost:  0.48029602 train_acc:  0.875 test_cost:  0.66158223 test_acc:  0.84375\n",
            "iter:  789 train_cost:  0.5250377 train_acc:  0.7890625 test_cost:  0.5257268 test_acc:  0.8046875\n",
            "iter:  790 train_cost:  0.6772902 train_acc:  0.7734375 test_cost:  0.42349103 test_acc:  0.8515625\n",
            "iter:  791 train_cost:  0.52020556 train_acc:  0.8515625 test_cost:  0.4929349 test_acc:  0.84375\n",
            "iter:  792 train_cost:  0.6856984 train_acc:  0.8046875 test_cost:  0.4692604 test_acc:  0.875\n",
            "iter:  793 train_cost:  0.51549166 train_acc:  0.828125 test_cost:  0.5392946 test_acc:  0.875\n",
            "iter:  794 train_cost:  0.509249 train_acc:  0.8203125 test_cost:  0.58668643 test_acc:  0.84375\n",
            "iter:  795 train_cost:  0.45470288 train_acc:  0.8671875 test_cost:  0.5638908 test_acc:  0.796875\n",
            "iter:  796 train_cost:  0.6925474 train_acc:  0.7578125 test_cost:  0.5331407 test_acc:  0.828125\n",
            "iter:  797 train_cost:  0.3558411 train_acc:  0.8828125 test_cost:  0.6711735 test_acc:  0.765625\n",
            "iter:  798 train_cost:  0.5150472 train_acc:  0.8203125 test_cost:  0.35932642 test_acc:  0.8671875\n",
            "iter:  799 train_cost:  0.5462184 train_acc:  0.8359375 test_cost:  0.3914742 test_acc:  0.84375\n",
            "iter:  800 train_cost:  0.5080904 train_acc:  0.8515625 test_cost:  0.55815816 test_acc:  0.8125\n",
            "iter:  801 train_cost:  0.42013413 train_acc:  0.8515625 test_cost:  0.6908661 test_acc:  0.78125\n",
            "iter:  802 train_cost:  0.48547783 train_acc:  0.8671875 test_cost:  0.35737282 test_acc:  0.8671875\n",
            "iter:  803 train_cost:  0.39503232 train_acc:  0.8515625 test_cost:  0.606813 test_acc:  0.8515625\n",
            "iter:  804 train_cost:  0.5939502 train_acc:  0.8046875 test_cost:  0.57042176 test_acc:  0.8125\n",
            "iter:  805 train_cost:  0.62348557 train_acc:  0.8203125 test_cost:  0.5192709 test_acc:  0.875\n",
            "iter:  806 train_cost:  0.6156764 train_acc:  0.8125 test_cost:  0.4329015 test_acc:  0.875\n",
            "iter:  807 train_cost:  0.48564094 train_acc:  0.8671875 test_cost:  0.57726806 test_acc:  0.8359375\n",
            "iter:  808 train_cost:  0.5411298 train_acc:  0.84375 test_cost:  0.5494127 test_acc:  0.84375\n",
            "iter:  809 train_cost:  0.59020233 train_acc:  0.8125 test_cost:  0.61354244 test_acc:  0.8359375\n",
            "iter:  810 train_cost:  0.58415556 train_acc:  0.8046875 test_cost:  0.4124688 test_acc:  0.8671875\n",
            "iter:  811 train_cost:  0.55743915 train_acc:  0.8125 test_cost:  0.41436523 test_acc:  0.875\n",
            "iter:  812 train_cost:  0.57214403 train_acc:  0.8359375 test_cost:  0.36109233 test_acc:  0.8828125\n",
            "iter:  813 train_cost:  0.58030283 train_acc:  0.78125 test_cost:  0.51695347 test_acc:  0.84375\n",
            "iter:  814 train_cost:  0.5699932 train_acc:  0.8125 test_cost:  0.40413195 test_acc:  0.8671875\n",
            "iter:  815 train_cost:  0.7086743 train_acc:  0.7578125 test_cost:  0.66152275 test_acc:  0.7890625\n",
            "iter:  816 train_cost:  0.670184 train_acc:  0.7890625 test_cost:  0.7075703 test_acc:  0.8203125\n",
            "iter:  817 train_cost:  0.64919865 train_acc:  0.84375 test_cost:  0.6669702 test_acc:  0.7890625\n",
            "iter:  818 train_cost:  0.525717 train_acc:  0.84375 test_cost:  0.41867408 test_acc:  0.84375\n",
            "iter:  819 train_cost:  0.5927475 train_acc:  0.8046875 test_cost:  0.5300578 test_acc:  0.84375\n",
            "iter:  820 train_cost:  0.5326178 train_acc:  0.8203125 test_cost:  0.5310626 test_acc:  0.828125\n",
            "iter:  821 train_cost:  0.5280632 train_acc:  0.828125 test_cost:  0.4934788 test_acc:  0.8359375\n",
            "iter:  822 train_cost:  0.5831465 train_acc:  0.8203125 test_cost:  0.700518 test_acc:  0.828125\n",
            "iter:  823 train_cost:  0.5671685 train_acc:  0.828125 test_cost:  0.44059452 test_acc:  0.8828125\n",
            "iter:  824 train_cost:  0.41913503 train_acc:  0.859375 test_cost:  0.66122764 test_acc:  0.7890625\n",
            "iter:  825 train_cost:  0.6200222 train_acc:  0.8203125 test_cost:  0.56641495 test_acc:  0.8359375\n",
            "iter:  826 train_cost:  0.5042378 train_acc:  0.8125 test_cost:  0.46996617 test_acc:  0.84375\n",
            "iter:  827 train_cost:  0.5003161 train_acc:  0.8359375 test_cost:  0.5400024 test_acc:  0.84375\n",
            "iter:  828 train_cost:  0.6154353 train_acc:  0.8125 test_cost:  0.33852303 test_acc:  0.875\n",
            "iter:  829 train_cost:  0.7101455 train_acc:  0.75 test_cost:  0.4370348 test_acc:  0.875\n",
            "iter:  830 train_cost:  0.5111228 train_acc:  0.859375 test_cost:  0.48832873 test_acc:  0.8671875\n",
            "iter:  831 train_cost:  0.5011631 train_acc:  0.828125 test_cost:  0.57364273 test_acc:  0.828125\n",
            "iter:  832 train_cost:  0.6612972 train_acc:  0.78125 test_cost:  0.5893282 test_acc:  0.84375\n",
            "iter:  833 train_cost:  0.45932055 train_acc:  0.8671875 test_cost:  0.61618394 test_acc:  0.8046875\n",
            "iter:  834 train_cost:  0.55888534 train_acc:  0.875 test_cost:  0.3034703 test_acc:  0.9140625\n",
            "iter:  835 train_cost:  0.54276806 train_acc:  0.8515625 test_cost:  0.49010462 test_acc:  0.8359375\n",
            "iter:  836 train_cost:  0.41331816 train_acc:  0.8359375 test_cost:  0.4928354 test_acc:  0.84375\n",
            "iter:  837 train_cost:  0.5208879 train_acc:  0.828125 test_cost:  0.42190295 test_acc:  0.890625\n",
            "iter:  838 train_cost:  0.40996766 train_acc:  0.875 test_cost:  0.5000676 test_acc:  0.8359375\n",
            "iter:  839 train_cost:  0.683506 train_acc:  0.796875 test_cost:  0.5281197 test_acc:  0.828125\n",
            "iter:  840 train_cost:  0.441424 train_acc:  0.8671875 test_cost:  0.5342703 test_acc:  0.8359375\n",
            "iter:  841 train_cost:  0.46029636 train_acc:  0.875 test_cost:  0.6862613 test_acc:  0.8125\n",
            "iter:  842 train_cost:  0.6069376 train_acc:  0.8203125 test_cost:  0.5290493 test_acc:  0.8125\n",
            "iter:  843 train_cost:  0.42771232 train_acc:  0.890625 test_cost:  0.4402317 test_acc:  0.859375\n",
            "iter:  844 train_cost:  0.5183943 train_acc:  0.859375 test_cost:  0.517929 test_acc:  0.859375\n",
            "iter:  845 train_cost:  0.5451124 train_acc:  0.828125 test_cost:  0.47324115 test_acc:  0.8515625\n",
            "iter:  846 train_cost:  0.45043567 train_acc:  0.828125 test_cost:  0.5712462 test_acc:  0.8203125\n",
            "iter:  847 train_cost:  0.43307692 train_acc:  0.8671875 test_cost:  0.5647196 test_acc:  0.8046875\n",
            "iter:  848 train_cost:  0.63694745 train_acc:  0.7890625 test_cost:  0.72328186 test_acc:  0.796875\n",
            "iter:  849 train_cost:  0.44414973 train_acc:  0.890625 test_cost:  0.35229903 test_acc:  0.8984375\n",
            "iter:  850 train_cost:  0.6330631 train_acc:  0.78125 test_cost:  0.4686423 test_acc:  0.8359375\n",
            "iter:  851 train_cost:  0.4370185 train_acc:  0.84375 test_cost:  0.51974076 test_acc:  0.8671875\n",
            "iter:  852 train_cost:  0.8015015 train_acc:  0.828125 test_cost:  0.5171407 test_acc:  0.8359375\n",
            "iter:  853 train_cost:  0.46811962 train_acc:  0.859375 test_cost:  0.5447465 test_acc:  0.8125\n",
            "iter:  854 train_cost:  0.45495638 train_acc:  0.859375 test_cost:  0.6442251 test_acc:  0.78125\n",
            "iter:  855 train_cost:  0.5771369 train_acc:  0.84375 test_cost:  0.49848723 test_acc:  0.8359375\n",
            "iter:  856 train_cost:  0.58502 train_acc:  0.8359375 test_cost:  0.44935602 test_acc:  0.828125\n",
            "iter:  857 train_cost:  0.49292395 train_acc:  0.828125 test_cost:  0.2855354 test_acc:  0.8984375\n",
            "iter:  858 train_cost:  0.5350151 train_acc:  0.8515625 test_cost:  0.6281799 test_acc:  0.8125\n",
            "iter:  859 train_cost:  0.5661179 train_acc:  0.8125 test_cost:  0.5401831 test_acc:  0.859375\n",
            "iter:  860 train_cost:  0.5614367 train_acc:  0.828125 test_cost:  0.39757663 test_acc:  0.84375\n",
            "iter:  861 train_cost:  0.65035 train_acc:  0.7890625 test_cost:  0.58051926 test_acc:  0.84375\n",
            "iter:  862 train_cost:  0.4225748 train_acc:  0.890625 test_cost:  0.48342904 test_acc:  0.828125\n",
            "iter:  863 train_cost:  0.49555045 train_acc:  0.8515625 test_cost:  0.59405893 test_acc:  0.8359375\n",
            "iter:  864 train_cost:  0.47124913 train_acc:  0.8671875 test_cost:  0.41800964 test_acc:  0.8671875\n",
            "iter:  865 train_cost:  0.58234656 train_acc:  0.8046875 test_cost:  0.510761 test_acc:  0.875\n",
            "iter:  866 train_cost:  0.30298704 train_acc:  0.9375 test_cost:  0.6083818 test_acc:  0.828125\n",
            "iter:  867 train_cost:  0.30018806 train_acc:  0.921875 test_cost:  0.36804947 test_acc:  0.875\n",
            "iter:  868 train_cost:  0.60009056 train_acc:  0.8046875 test_cost:  0.7756275 test_acc:  0.765625\n",
            "iter:  869 train_cost:  0.667583 train_acc:  0.828125 test_cost:  0.45736814 test_acc:  0.84375\n",
            "iter:  870 train_cost:  0.5306263 train_acc:  0.859375 test_cost:  0.5613734 test_acc:  0.8125\n",
            "iter:  871 train_cost:  0.4777574 train_acc:  0.859375 test_cost:  0.47511247 test_acc:  0.8671875\n",
            "iter:  872 train_cost:  0.45636812 train_acc:  0.828125 test_cost:  0.49241817 test_acc:  0.828125\n",
            "iter:  873 train_cost:  0.55932087 train_acc:  0.84375 test_cost:  0.5246602 test_acc:  0.8046875\n",
            "iter:  874 train_cost:  0.53810966 train_acc:  0.796875 test_cost:  0.50095665 test_acc:  0.8671875\n",
            "iter:  875 train_cost:  0.4612842 train_acc:  0.8671875 test_cost:  0.610591 test_acc:  0.828125\n",
            "iter:  876 train_cost:  0.6296226 train_acc:  0.8125 test_cost:  0.65394783 test_acc:  0.8515625\n",
            "iter:  877 train_cost:  0.53871495 train_acc:  0.8046875 test_cost:  0.40729645 test_acc:  0.859375\n",
            "iter:  878 train_cost:  0.4335162 train_acc:  0.875 test_cost:  0.6424327 test_acc:  0.8046875\n",
            "iter:  879 train_cost:  0.50747496 train_acc:  0.84375 test_cost:  0.44493803 test_acc:  0.8828125\n",
            "iter:  880 train_cost:  0.5177448 train_acc:  0.84375 test_cost:  0.39762604 test_acc:  0.9140625\n",
            "iter:  881 train_cost:  0.4703135 train_acc:  0.8203125 test_cost:  0.4257567 test_acc:  0.8671875\n",
            "iter:  882 train_cost:  0.48801395 train_acc:  0.859375 test_cost:  0.4094767 test_acc:  0.8203125\n",
            "iter:  883 train_cost:  0.5879934 train_acc:  0.796875 test_cost:  0.42858937 test_acc:  0.8515625\n",
            "iter:  884 train_cost:  0.63038766 train_acc:  0.796875 test_cost:  0.3174855 test_acc:  0.890625\n",
            "iter:  885 train_cost:  0.5019958 train_acc:  0.8671875 test_cost:  0.63729346 test_acc:  0.8125\n",
            "iter:  886 train_cost:  0.44954547 train_acc:  0.859375 test_cost:  0.39684343 test_acc:  0.859375\n",
            "iter:  887 train_cost:  0.4984281 train_acc:  0.8515625 test_cost:  0.66220754 test_acc:  0.8125\n",
            "iter:  888 train_cost:  0.43437457 train_acc:  0.875 test_cost:  0.29838604 test_acc:  0.8984375\n",
            "iter:  889 train_cost:  0.6054011 train_acc:  0.8046875 test_cost:  0.44819868 test_acc:  0.859375\n",
            "iter:  890 train_cost:  0.46677166 train_acc:  0.84375 test_cost:  0.5236843 test_acc:  0.8203125\n",
            "iter:  891 train_cost:  0.528658 train_acc:  0.8359375 test_cost:  0.37241688 test_acc:  0.90625\n",
            "iter:  892 train_cost:  0.49280956 train_acc:  0.8203125 test_cost:  0.54675305 test_acc:  0.84375\n",
            "iter:  893 train_cost:  0.53321123 train_acc:  0.8125 test_cost:  0.46379983 test_acc:  0.9140625\n",
            "iter:  894 train_cost:  0.56660855 train_acc:  0.8046875 test_cost:  0.45509875 test_acc:  0.8203125\n",
            "iter:  895 train_cost:  0.44849497 train_acc:  0.84375 test_cost:  0.4707802 test_acc:  0.8515625\n",
            "iter:  896 train_cost:  0.3937048 train_acc:  0.875 test_cost:  0.61879855 test_acc:  0.8046875\n",
            "iter:  897 train_cost:  0.5454664 train_acc:  0.8515625 test_cost:  0.5651305 test_acc:  0.8046875\n",
            "iter:  898 train_cost:  0.42326283 train_acc:  0.8671875 test_cost:  0.7881261 test_acc:  0.7421875\n",
            "iter:  899 train_cost:  0.42310667 train_acc:  0.8671875 test_cost:  0.45859995 test_acc:  0.8359375\n",
            "iter:  900 train_cost:  0.39094073 train_acc:  0.8515625 test_cost:  0.649647 test_acc:  0.8359375\n",
            "iter:  901 train_cost:  0.5446633 train_acc:  0.8203125 test_cost:  0.48380992 test_acc:  0.8671875\n",
            "iter:  902 train_cost:  0.3821323 train_acc:  0.8984375 test_cost:  0.4101193 test_acc:  0.875\n",
            "iter:  903 train_cost:  0.47619003 train_acc:  0.875 test_cost:  0.6378924 test_acc:  0.78125\n",
            "iter:  904 train_cost:  0.4987973 train_acc:  0.8125 test_cost:  0.5474824 test_acc:  0.8203125\n",
            "iter:  905 train_cost:  0.51265883 train_acc:  0.828125 test_cost:  0.47542334 test_acc:  0.84375\n",
            "iter:  906 train_cost:  0.7153437 train_acc:  0.7890625 test_cost:  0.41554433 test_acc:  0.859375\n",
            "iter:  907 train_cost:  0.6237576 train_acc:  0.8203125 test_cost:  0.48541248 test_acc:  0.84375\n",
            "iter:  908 train_cost:  0.49822962 train_acc:  0.859375 test_cost:  0.6046312 test_acc:  0.7890625\n",
            "iter:  909 train_cost:  0.33153284 train_acc:  0.8828125 test_cost:  0.57324123 test_acc:  0.8125\n",
            "iter:  910 train_cost:  0.5168619 train_acc:  0.8515625 test_cost:  0.5788955 test_acc:  0.796875\n",
            "iter:  911 train_cost:  0.3378526 train_acc:  0.8828125 test_cost:  0.45709544 test_acc:  0.859375\n",
            "iter:  912 train_cost:  0.35222206 train_acc:  0.890625 test_cost:  0.42301893 test_acc:  0.8671875\n",
            "iter:  913 train_cost:  0.77255714 train_acc:  0.765625 test_cost:  0.61725736 test_acc:  0.7734375\n",
            "iter:  914 train_cost:  0.49784058 train_acc:  0.828125 test_cost:  0.41563368 test_acc:  0.8671875\n",
            "iter:  915 train_cost:  0.50094974 train_acc:  0.8203125 test_cost:  0.40926543 test_acc:  0.890625\n",
            "iter:  916 train_cost:  0.53513587 train_acc:  0.8359375 test_cost:  0.345683 test_acc:  0.8984375\n",
            "iter:  917 train_cost:  0.37302393 train_acc:  0.8828125 test_cost:  0.53304285 test_acc:  0.859375\n",
            "iter:  918 train_cost:  0.6046612 train_acc:  0.828125 test_cost:  0.3859117 test_acc:  0.8671875\n",
            "iter:  919 train_cost:  0.5007533 train_acc:  0.890625 test_cost:  0.50842535 test_acc:  0.828125\n",
            "iter:  920 train_cost:  0.4015795 train_acc:  0.859375 test_cost:  0.61738217 test_acc:  0.8671875\n",
            "iter:  921 train_cost:  0.52157664 train_acc:  0.8515625 test_cost:  0.57253397 test_acc:  0.8515625\n",
            "iter:  922 train_cost:  0.47081512 train_acc:  0.8671875 test_cost:  0.45936 test_acc:  0.8515625\n",
            "iter:  923 train_cost:  0.65470934 train_acc:  0.7734375 test_cost:  0.48892194 test_acc:  0.859375\n",
            "iter:  924 train_cost:  0.57393754 train_acc:  0.828125 test_cost:  0.47381914 test_acc:  0.8359375\n",
            "iter:  925 train_cost:  0.46110392 train_acc:  0.8671875 test_cost:  0.74999714 test_acc:  0.765625\n",
            "iter:  926 train_cost:  0.45974928 train_acc:  0.8515625 test_cost:  0.4352811 test_acc:  0.84375\n",
            "iter:  927 train_cost:  0.4282754 train_acc:  0.8203125 test_cost:  0.42038107 test_acc:  0.8515625\n",
            "iter:  928 train_cost:  0.4854299 train_acc:  0.8203125 test_cost:  0.3151205 test_acc:  0.90625\n",
            "iter:  929 train_cost:  0.43832257 train_acc:  0.84375 test_cost:  0.55790657 test_acc:  0.828125\n",
            "iter:  930 train_cost:  0.43915838 train_acc:  0.875 test_cost:  0.2863878 test_acc:  0.921875\n",
            "iter:  931 train_cost:  0.42483747 train_acc:  0.8671875 test_cost:  0.51791406 test_acc:  0.828125\n",
            "iter:  932 train_cost:  0.55204725 train_acc:  0.859375 test_cost:  0.5234316 test_acc:  0.8203125\n",
            "iter:  933 train_cost:  0.55842936 train_acc:  0.828125 test_cost:  0.49129826 test_acc:  0.859375\n",
            "iter:  934 train_cost:  0.37994513 train_acc:  0.859375 test_cost:  0.36170518 test_acc:  0.8828125\n",
            "iter:  935 train_cost:  0.47017524 train_acc:  0.8203125 test_cost:  0.5892184 test_acc:  0.7890625\n",
            "iter:  936 train_cost:  0.46251154 train_acc:  0.8671875 test_cost:  0.4046162 test_acc:  0.8828125\n",
            "iter:  937 train_cost:  0.4565913 train_acc:  0.8515625 test_cost:  0.59095836 test_acc:  0.8359375\n",
            "iter:  938 train_cost:  0.2386406 train_acc:  0.9296875 test_cost:  0.5525127 test_acc:  0.8203125\n",
            "iter:  939 train_cost:  0.47806004 train_acc:  0.8671875 test_cost:  0.47805917 test_acc:  0.859375\n",
            "iter:  940 train_cost:  0.5439302 train_acc:  0.796875 test_cost:  0.33282828 test_acc:  0.9140625\n",
            "iter:  941 train_cost:  0.40042698 train_acc:  0.890625 test_cost:  0.3702865 test_acc:  0.890625\n",
            "iter:  942 train_cost:  0.3843549 train_acc:  0.8515625 test_cost:  0.528264 test_acc:  0.8515625\n",
            "iter:  943 train_cost:  0.5201177 train_acc:  0.8359375 test_cost:  0.3343261 test_acc:  0.890625\n",
            "iter:  944 train_cost:  0.4730385 train_acc:  0.8359375 test_cost:  0.39845178 test_acc:  0.890625\n",
            "iter:  945 train_cost:  0.5084472 train_acc:  0.796875 test_cost:  0.40050048 test_acc:  0.8515625\n",
            "iter:  946 train_cost:  0.39238125 train_acc:  0.8515625 test_cost:  0.46547982 test_acc:  0.8515625\n",
            "iter:  947 train_cost:  0.5689149 train_acc:  0.828125 test_cost:  0.48961425 test_acc:  0.828125\n",
            "iter:  948 train_cost:  0.59581697 train_acc:  0.8203125 test_cost:  0.36371332 test_acc:  0.890625\n",
            "iter:  949 train_cost:  0.59696466 train_acc:  0.796875 test_cost:  0.39854363 test_acc:  0.8671875\n",
            "iter:  950 train_cost:  0.5604733 train_acc:  0.859375 test_cost:  0.4562287 test_acc:  0.84375\n",
            "iter:  951 train_cost:  0.38885725 train_acc:  0.8671875 test_cost:  0.5089686 test_acc:  0.8359375\n",
            "iter:  952 train_cost:  0.46978384 train_acc:  0.828125 test_cost:  0.4993484 test_acc:  0.859375\n",
            "iter:  953 train_cost:  0.5015929 train_acc:  0.8203125 test_cost:  0.5647488 test_acc:  0.828125\n",
            "iter:  954 train_cost:  0.674351 train_acc:  0.7890625 test_cost:  0.43401045 test_acc:  0.90625\n",
            "iter:  955 train_cost:  0.42908895 train_acc:  0.875 test_cost:  0.48471344 test_acc:  0.8359375\n",
            "iter:  956 train_cost:  0.47908974 train_acc:  0.828125 test_cost:  0.55054986 test_acc:  0.8125\n",
            "iter:  957 train_cost:  0.42693007 train_acc:  0.8671875 test_cost:  0.6014487 test_acc:  0.8046875\n",
            "iter:  958 train_cost:  0.4898492 train_acc:  0.828125 test_cost:  0.4510212 test_acc:  0.8671875\n",
            "iter:  959 train_cost:  0.46368748 train_acc:  0.890625 test_cost:  0.542042 test_acc:  0.8671875\n",
            "iter:  960 train_cost:  0.46018404 train_acc:  0.8359375 test_cost:  0.4133634 test_acc:  0.8828125\n",
            "iter:  961 train_cost:  0.54108953 train_acc:  0.8203125 test_cost:  0.40188694 test_acc:  0.8984375\n",
            "iter:  962 train_cost:  0.47710785 train_acc:  0.84375 test_cost:  0.54930484 test_acc:  0.8359375\n",
            "iter:  963 train_cost:  0.45668697 train_acc:  0.875 test_cost:  0.62485737 test_acc:  0.796875\n",
            "iter:  964 train_cost:  0.48144978 train_acc:  0.8359375 test_cost:  0.4026363 test_acc:  0.90625\n",
            "iter:  965 train_cost:  0.5152571 train_acc:  0.8359375 test_cost:  0.40391433 test_acc:  0.890625\n",
            "iter:  966 train_cost:  0.5049138 train_acc:  0.84375 test_cost:  0.67037004 test_acc:  0.8046875\n",
            "iter:  967 train_cost:  0.4685654 train_acc:  0.859375 test_cost:  0.5621385 test_acc:  0.84375\n",
            "iter:  968 train_cost:  0.5332431 train_acc:  0.859375 test_cost:  0.2941463 test_acc:  0.8984375\n",
            "iter:  969 train_cost:  0.48901942 train_acc:  0.84375 test_cost:  0.512614 test_acc:  0.8359375\n",
            "iter:  970 train_cost:  0.442662 train_acc:  0.8359375 test_cost:  0.6626632 test_acc:  0.78125\n",
            "iter:  971 train_cost:  0.5094352 train_acc:  0.84375 test_cost:  0.60138243 test_acc:  0.828125\n",
            "iter:  972 train_cost:  0.54980516 train_acc:  0.8359375 test_cost:  0.36106157 test_acc:  0.8828125\n",
            "iter:  973 train_cost:  0.39369437 train_acc:  0.8671875 test_cost:  0.51414406 test_acc:  0.8359375\n",
            "iter:  974 train_cost:  0.5009123 train_acc:  0.8828125 test_cost:  0.44900483 test_acc:  0.859375\n",
            "iter:  975 train_cost:  0.570397 train_acc:  0.7890625 test_cost:  0.45822033 test_acc:  0.875\n",
            "iter:  976 train_cost:  0.41960567 train_acc:  0.8671875 test_cost:  0.3120101 test_acc:  0.921875\n",
            "iter:  977 train_cost:  0.5234438 train_acc:  0.8203125 test_cost:  0.5104918 test_acc:  0.84375\n",
            "iter:  978 train_cost:  0.7156 train_acc:  0.796875 test_cost:  0.5597158 test_acc:  0.8515625\n",
            "iter:  979 train_cost:  0.36460996 train_acc:  0.8828125 test_cost:  0.4166636 test_acc:  0.8984375\n",
            "iter:  980 train_cost:  0.47659326 train_acc:  0.8359375 test_cost:  0.32701144 test_acc:  0.875\n",
            "iter:  981 train_cost:  0.5209826 train_acc:  0.8125 test_cost:  0.42883384 test_acc:  0.8515625\n",
            "iter:  982 train_cost:  0.37699777 train_acc:  0.8671875 test_cost:  0.3428341 test_acc:  0.90625\n",
            "iter:  983 train_cost:  0.42307252 train_acc:  0.890625 test_cost:  0.66469604 test_acc:  0.8125\n",
            "iter:  984 train_cost:  0.3951717 train_acc:  0.875 test_cost:  0.51568675 test_acc:  0.828125\n",
            "iter:  985 train_cost:  0.5333428 train_acc:  0.859375 test_cost:  0.5429159 test_acc:  0.828125\n",
            "iter:  986 train_cost:  0.34724167 train_acc:  0.8515625 test_cost:  0.4444489 test_acc:  0.8671875\n",
            "iter:  987 train_cost:  0.47805297 train_acc:  0.8359375 test_cost:  0.41123432 test_acc:  0.859375\n",
            "iter:  988 train_cost:  0.35636327 train_acc:  0.8984375 test_cost:  0.5534282 test_acc:  0.8359375\n",
            "iter:  989 train_cost:  0.5465841 train_acc:  0.8515625 test_cost:  0.42026567 test_acc:  0.859375\n",
            "iter:  990 train_cost:  0.42484647 train_acc:  0.859375 test_cost:  0.46543342 test_acc:  0.8125\n",
            "iter:  991 train_cost:  0.46879238 train_acc:  0.8671875 test_cost:  0.54255164 test_acc:  0.828125\n",
            "iter:  992 train_cost:  0.52493834 train_acc:  0.828125 test_cost:  0.41969174 test_acc:  0.84375\n",
            "iter:  993 train_cost:  0.47199988 train_acc:  0.8671875 test_cost:  0.38610446 test_acc:  0.875\n",
            "iter:  994 train_cost:  0.44241673 train_acc:  0.8671875 test_cost:  0.5344897 test_acc:  0.828125\n",
            "iter:  995 train_cost:  0.35682577 train_acc:  0.890625 test_cost:  0.5511315 test_acc:  0.7890625\n",
            "iter:  996 train_cost:  0.39213306 train_acc:  0.8984375 test_cost:  0.52500284 test_acc:  0.8671875\n",
            "iter:  997 train_cost:  0.49456918 train_acc:  0.8203125 test_cost:  0.43190825 test_acc:  0.8671875\n",
            "iter:  998 train_cost:  0.41394526 train_acc:  0.84375 test_cost:  0.43445882 test_acc:  0.890625\n",
            "iter:  999 train_cost:  0.36630768 train_acc:  0.8828125 test_cost:  0.46410888 test_acc:  0.828125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.682025999999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDkPoIsB90MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10.579477999999998 TPU\n",
        "# 10.682025999999999 CPU\n",
        "# 8.640348 GPU \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZtWbWQ92P_",
        "colab_type": "text"
      },
      "source": [
        "# CNN using Tensorflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6k9YPZQ95z5",
        "colab_type": "code",
        "outputId": "498b8504-e987-4178-e9e8-6541d22f6239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1703
        }
      },
      "source": [
        "n_classes=10\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    print('con1_before max',conv1.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=1)\n",
        "    print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    print('con2_before max',conv2.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=1)\n",
        "    print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "con1_before max [None, 28, 28, 32]\n",
            "con1_after max [None, 28, 28, 32]\n",
            "con2_before max [None, 28, 28, 64]\n",
            "con2_after max [None, 28, 28, 64]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[{{node softmax_cross_entropy_with_logits_sg_4}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2c408dd24289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[node softmax_cross_entropy_with_logits_sg_4 (defined at <ipython-input-35-2c408dd24289>:74) ]]\n\nCaused by op 'softmax_cross_entropy_with_logits_sg_4', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-2c408dd24289>\", line 74, in <module>\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2561, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, axis=dim, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2370, in softmax_cross_entropy_with_logits_v2\n    labels=labels, logits=logits, axis=axis, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2471, in softmax_cross_entropy_with_logits_v2_helper\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7862, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[node softmax_cross_entropy_with_logits_sg_4 (defined at <ipython-input-35-2c408dd24289>:74) ]]\n"
          ]
        }
      ]
    }
  ]
}