{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-Learning-Lab-2020/blob/emir/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7df1t27dZKuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae4adbc3-fe24-4d33-aeb2-05d7c520c723"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "25a12f6a-89d9-4e58-a0cf-8745f56daba4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "iter:  0 cost:  0.6879047\n",
            "iter:  1 cost:  0.68631524\n",
            "iter:  2 cost:  0.68473023\n",
            "iter:  3 cost:  0.6831498\n",
            "iter:  4 cost:  0.681574\n",
            "iter:  5 cost:  0.6800027\n",
            "iter:  6 cost:  0.6784364\n",
            "iter:  7 cost:  0.67687476\n",
            "iter:  8 cost:  0.67531794\n",
            "iter:  9 cost:  0.6737662\n",
            "predicted  [[-0.7650659   0.00451315  0.00786659  0.5631976 ]\n",
            " [-0.5296149   0.2853642  -0.4739438   0.69286066]\n",
            " [-1.3174443   0.00464206 -0.2609762   1.5602987 ]\n",
            " [-0.44987223  0.33479223 -0.11355152 -0.25863725]\n",
            " [-0.58847183  0.39806193 -0.39936644  0.29487485]\n",
            " [-0.12359159  0.4432073  -0.33676994 -0.31151545]\n",
            " [-0.53357214 -0.07495052  0.08453731  0.42268765]\n",
            " [-0.47955635  0.2855067  -0.2736042   0.228679  ]\n",
            " [ 0.07792095  0.9119682  -0.65320265 -0.98212945]\n",
            " [-0.77881306 -0.10078262  0.07784212  0.6918165 ]]\n",
            "real  [[0.70952204 0.78983078 0.51656836 0.81312133]\n",
            " [0.43322909 0.14536171 0.23177686 0.3570239 ]\n",
            " [0.53496329 0.15602576 0.0090037  0.26408821]\n",
            " [0.88324248 0.88696179 0.69469661 0.69804211]\n",
            " [0.30550758 0.49625514 0.69371357 0.1127927 ]\n",
            " [0.7937821  0.43826076 0.65977494 0.39794103]\n",
            " [0.08480129 0.79991493 0.04917729 0.4727579 ]\n",
            " [0.58182974 0.43817923 0.03948507 0.79866491]\n",
            " [0.00307474 0.68137196 0.64326793 0.26314505]\n",
            " [0.31078643 0.79945364 0.87198985 0.0554    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2aqi3Fu-AJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "c150d581-6d86-4d8f-b1e5-57c429c682a4"
      },
      "source": [
        "sess = tf.Session() \n",
        "sess.run(init)\n",
        "    \n",
        "for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "print('predicted ', y_p_p)\n",
        "print('real ', y_gr)\n",
        "\n",
        "#sess.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  1.5497437\n",
            "iter:  1 cost:  1.5473305\n",
            "iter:  2 cost:  1.5449209\n",
            "iter:  3 cost:  1.5425149\n",
            "iter:  4 cost:  1.5401127\n",
            "iter:  5 cost:  1.5377144\n",
            "iter:  6 cost:  1.5353199\n",
            "iter:  7 cost:  1.5329292\n",
            "iter:  8 cost:  1.5305426\n",
            "iter:  9 cost:  1.5281599\n",
            "predicted  [[ 0.7919632   0.81016856 -1.8356575   1.2828511 ]\n",
            " [-0.19975388  0.29692623 -0.92760223  0.536528  ]\n",
            " [ 0.43201846  0.61893326 -2.0028565   1.647118  ]\n",
            " [ 1.0835544   1.4074482  -2.5273604   1.2424406 ]\n",
            " [ 0.45294785  0.99143875 -2.0317495   1.0307344 ]\n",
            " [ 0.34437707  0.8684148  -1.4756199   0.49229777]\n",
            " [ 0.5792516   0.4888251  -1.1526108   0.88357246]\n",
            " [ 0.42447832  0.80478084 -1.6440432   0.8596892 ]\n",
            " [ 0.51570535  1.5627092  -2.4298751   0.547484  ]\n",
            " [ 0.71880275  0.61937195 -1.5435345   1.2164376 ]]\n",
            "real  [[0.70952204 0.78983078 0.51656836 0.81312133]\n",
            " [0.43322909 0.14536171 0.23177686 0.3570239 ]\n",
            " [0.53496329 0.15602576 0.0090037  0.26408821]\n",
            " [0.88324248 0.88696179 0.69469661 0.69804211]\n",
            " [0.30550758 0.49625514 0.69371357 0.1127927 ]\n",
            " [0.7937821  0.43826076 0.65977494 0.39794103]\n",
            " [0.08480129 0.79991493 0.04917729 0.4727579 ]\n",
            " [0.58182974 0.43817923 0.03948507 0.79866491]\n",
            " [0.00307474 0.68137196 0.64326793 0.26314505]\n",
            " [0.31078643 0.79945364 0.87198985 0.0554    ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6qknOSWeKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7403b92-bfd5-4729-90dd-abf101839420"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPSS03avw5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "b56c4267-fc51-4bf8-e21c-d04e184b2d1e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#      O   O   O\n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "\n",
        "\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "#F(WX+b)\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "#placeholders/inputs:outpus --> Variables/Weights --> Model --> cost --> optimizer --> initilize all variables --> start the session\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  1.3747197\n",
            "iter:  1 cost:  1.3571972\n",
            "iter:  2 cost:  1.3398707\n",
            "iter:  3 cost:  1.3227433\n",
            "iter:  4 cost:  1.305818\n",
            "iter:  5 cost:  1.2890977\n",
            "iter:  6 cost:  1.2725844\n",
            "iter:  7 cost:  1.2562801\n",
            "iter:  8 cost:  1.240186\n",
            "iter:  9 cost:  1.224303\n",
            "predicted  [[-0.49553448 -0.32934055  2.2853956   1.0342578 ]\n",
            " [-0.5915999  -0.12358764  2.2168155   0.8822452 ]\n",
            " [-0.5718505  -0.11707768  2.2199898   0.83789814]\n",
            " [-0.55210674 -0.02264899  2.192658    0.63888586]\n",
            " [-0.5430312  -0.12619165  2.21796     0.7780385 ]\n",
            " [-0.442331   -0.2704625   2.2880855   0.86878955]\n",
            " [-0.50752854 -0.22720224  2.2534919   0.8841039 ]\n",
            " [-0.54281336  0.03235716  2.1785827   0.5345362 ]\n",
            " [-0.4952021  -0.12381646  2.230531    0.7026081 ]\n",
            " [-0.5234042  -0.2988435   2.2742784   1.0374424 ]]\n",
            "real  [[0.45240918 0.83295494 0.31481309 0.79053004]\n",
            " [0.1816843  0.09955046 0.4485272  0.87439565]\n",
            " [0.97557775 0.25179652 0.49892242 0.50017244]\n",
            " [0.23076734 0.78193309 0.6157077  0.26782214]\n",
            " [0.30728375 0.42175732 0.84576711 0.76690903]\n",
            " [0.98233381 0.20735125 0.01395137 0.60350582]\n",
            " [0.49123506 0.51784512 0.9341518  0.25546836]\n",
            " [0.96164575 0.80278578 0.78475864 0.65469991]\n",
            " [0.86660732 0.92707013 0.28921119 0.17556915]\n",
            " [0.0306744  0.33356693 0.6658722  0.48460009]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhCMk9VyVj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "008af08f-e019-403e-eab4-0936c0bf9368"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "display(mnist.train.images.shape) # 28*28 = 784\n",
        "\n",
        "image =mnist.train.images[0].reshape((28,28))\n",
        "#MNIST data input (img shape: 28*28)\n",
        "imshow(image)\n",
        "\n",
        "print(mnist.train.labels[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-b22520be68e0>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXapqksBX95W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "tic = time.clock()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeRp0T10834",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6803d38a-b0d8-4860-fca1-0dd4ce19940d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# training data\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "# testing data\n",
        "X_test = mnist.test.images\n",
        "Y_test = mnist.test.labels\n",
        "\n",
        "# training data\n",
        "X_val = mnist.validation.images\n",
        "Y_val = mnist.validation.labels\n",
        "\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 784\n",
        "num_h1_n = 100\n",
        "num_h2_n = 100\n",
        "num_outputs = 10\n",
        "\n",
        "\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "\n",
        "## 3 images, y_p=[[0.1,0.0,0,0.9],[0.9,0.1,0,0.],[0,0.9,0,0.1]] \n",
        "\n",
        "# tf.argmax(y_p, 1) [3,0,1] \n",
        "\n",
        "# 3 images, y=[[0,0.0,0,1],[0,1,0,0],[0,1,0,0]] \n",
        "\n",
        "# tf.argmax(y, 1) [3,1,1]\n",
        "\n",
        "# tf.equal [True,False,True]--[1,0,1]--- 2/3 \n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "tic = time.clock()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    #print('predicted ', y_p_p)\n",
        "    #print('real ', y_gr)\n",
        "\n",
        "\n",
        "\n",
        "toc = time.clock()\n",
        "toc-tic"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-9d619b82b806>:58: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "iter:  0 train_cost:  6.8561287 train_acc:  0.140625 test_cost:  6.819843 test_acc:  0.1328125\n",
            "iter:  1 train_cost:  5.9924164 train_acc:  0.171875 test_cost:  7.004061 test_acc:  0.1328125\n",
            "iter:  2 train_cost:  6.9703116 train_acc:  0.0703125 test_cost:  6.2818594 test_acc:  0.1328125\n",
            "iter:  3 train_cost:  6.728799 train_acc:  0.109375 test_cost:  6.56114 test_acc:  0.171875\n",
            "iter:  4 train_cost:  6.2838235 train_acc:  0.1796875 test_cost:  5.662182 test_acc:  0.1875\n",
            "iter:  5 train_cost:  5.71724 train_acc:  0.1953125 test_cost:  6.6707525 test_acc:  0.09375\n",
            "iter:  6 train_cost:  5.3714666 train_acc:  0.15625 test_cost:  6.179183 test_acc:  0.140625\n",
            "iter:  7 train_cost:  5.3649054 train_acc:  0.1875 test_cost:  5.652121 test_acc:  0.1640625\n",
            "iter:  8 train_cost:  5.6931667 train_acc:  0.171875 test_cost:  6.0765095 test_acc:  0.125\n",
            "iter:  9 train_cost:  5.570997 train_acc:  0.1640625 test_cost:  5.565876 test_acc:  0.1953125\n",
            "iter:  10 train_cost:  5.1663074 train_acc:  0.1953125 test_cost:  5.1147203 test_acc:  0.1484375\n",
            "iter:  11 train_cost:  5.0759788 train_acc:  0.171875 test_cost:  5.572069 test_acc:  0.1640625\n",
            "iter:  12 train_cost:  5.544573 train_acc:  0.1640625 test_cost:  5.203045 test_acc:  0.1640625\n",
            "iter:  13 train_cost:  5.395954 train_acc:  0.171875 test_cost:  4.730632 test_acc:  0.2265625\n",
            "iter:  14 train_cost:  4.583529 train_acc:  0.1875 test_cost:  4.748041 test_acc:  0.1953125\n",
            "iter:  15 train_cost:  4.912457 train_acc:  0.140625 test_cost:  4.8816285 test_acc:  0.1796875\n",
            "iter:  16 train_cost:  4.4087095 train_acc:  0.1875 test_cost:  4.937852 test_acc:  0.15625\n",
            "iter:  17 train_cost:  4.7278013 train_acc:  0.1875 test_cost:  5.2641015 test_acc:  0.171875\n",
            "iter:  18 train_cost:  5.007991 train_acc:  0.15625 test_cost:  4.436778 test_acc:  0.171875\n",
            "iter:  19 train_cost:  4.0004325 train_acc:  0.1953125 test_cost:  4.579603 test_acc:  0.1484375\n",
            "iter:  20 train_cost:  4.122945 train_acc:  0.1953125 test_cost:  4.52973 test_acc:  0.1640625\n",
            "iter:  21 train_cost:  4.0698996 train_acc:  0.1875 test_cost:  4.057803 test_acc:  0.1875\n",
            "iter:  22 train_cost:  4.4412403 train_acc:  0.15625 test_cost:  4.163864 test_acc:  0.15625\n",
            "iter:  23 train_cost:  3.6971447 train_acc:  0.234375 test_cost:  3.9835305 test_acc:  0.1484375\n",
            "iter:  24 train_cost:  4.2085266 train_acc:  0.203125 test_cost:  4.3715134 test_acc:  0.15625\n",
            "iter:  25 train_cost:  3.7233315 train_acc:  0.203125 test_cost:  4.004221 test_acc:  0.2109375\n",
            "iter:  26 train_cost:  3.4357648 train_acc:  0.203125 test_cost:  3.929855 test_acc:  0.15625\n",
            "iter:  27 train_cost:  3.5021877 train_acc:  0.2734375 test_cost:  3.9761934 test_acc:  0.1640625\n",
            "iter:  28 train_cost:  3.9980829 train_acc:  0.2265625 test_cost:  3.8714378 test_acc:  0.21875\n",
            "iter:  29 train_cost:  3.6315196 train_acc:  0.1953125 test_cost:  3.5882158 test_acc:  0.234375\n",
            "iter:  30 train_cost:  3.3492851 train_acc:  0.2265625 test_cost:  3.6381245 test_acc:  0.1953125\n",
            "iter:  31 train_cost:  3.575925 train_acc:  0.2578125 test_cost:  3.570907 test_acc:  0.1796875\n",
            "iter:  32 train_cost:  3.6799252 train_acc:  0.140625 test_cost:  3.6413374 test_acc:  0.203125\n",
            "iter:  33 train_cost:  3.766152 train_acc:  0.2109375 test_cost:  3.2443492 test_acc:  0.2734375\n",
            "iter:  34 train_cost:  3.4641082 train_acc:  0.234375 test_cost:  3.369997 test_acc:  0.2265625\n",
            "iter:  35 train_cost:  3.2651393 train_acc:  0.2109375 test_cost:  3.5083747 test_acc:  0.2109375\n",
            "iter:  36 train_cost:  3.2565117 train_acc:  0.2578125 test_cost:  4.1038 test_acc:  0.15625\n",
            "iter:  37 train_cost:  3.2153692 train_acc:  0.28125 test_cost:  3.1366982 test_acc:  0.3046875\n",
            "iter:  38 train_cost:  3.074035 train_acc:  0.2578125 test_cost:  3.172986 test_acc:  0.2421875\n",
            "iter:  39 train_cost:  3.1561284 train_acc:  0.2421875 test_cost:  2.8737123 test_acc:  0.1875\n",
            "iter:  40 train_cost:  3.0858746 train_acc:  0.21875 test_cost:  3.3795018 test_acc:  0.2734375\n",
            "iter:  41 train_cost:  2.8603666 train_acc:  0.2734375 test_cost:  3.0826912 test_acc:  0.2578125\n",
            "iter:  42 train_cost:  3.2633452 train_acc:  0.25 test_cost:  3.1409287 test_acc:  0.1953125\n",
            "iter:  43 train_cost:  2.884368 train_acc:  0.25 test_cost:  3.142871 test_acc:  0.2890625\n",
            "iter:  44 train_cost:  3.1970713 train_acc:  0.21875 test_cost:  2.9777462 test_acc:  0.265625\n",
            "iter:  45 train_cost:  2.9663286 train_acc:  0.2890625 test_cost:  2.8970015 test_acc:  0.3359375\n",
            "iter:  46 train_cost:  2.8684807 train_acc:  0.296875 test_cost:  3.448069 test_acc:  0.203125\n",
            "iter:  47 train_cost:  3.029359 train_acc:  0.21875 test_cost:  2.7586744 test_acc:  0.2890625\n",
            "iter:  48 train_cost:  2.8655965 train_acc:  0.265625 test_cost:  3.0188339 test_acc:  0.3125\n",
            "iter:  49 train_cost:  2.703947 train_acc:  0.296875 test_cost:  2.9982047 test_acc:  0.234375\n",
            "iter:  50 train_cost:  2.588323 train_acc:  0.3125 test_cost:  2.4979715 test_acc:  0.3125\n",
            "iter:  51 train_cost:  2.9284613 train_acc:  0.2734375 test_cost:  2.9699695 test_acc:  0.2421875\n",
            "iter:  52 train_cost:  2.586535 train_acc:  0.28125 test_cost:  2.8029833 test_acc:  0.28125\n",
            "iter:  53 train_cost:  2.5919867 train_acc:  0.3046875 test_cost:  2.9747076 test_acc:  0.2734375\n",
            "iter:  54 train_cost:  2.373828 train_acc:  0.3671875 test_cost:  2.7179987 test_acc:  0.3203125\n",
            "iter:  55 train_cost:  2.5202403 train_acc:  0.3671875 test_cost:  2.925503 test_acc:  0.2734375\n",
            "iter:  56 train_cost:  2.661909 train_acc:  0.2890625 test_cost:  2.6909657 test_acc:  0.28125\n",
            "iter:  57 train_cost:  2.5551982 train_acc:  0.359375 test_cost:  2.7579834 test_acc:  0.2890625\n",
            "iter:  58 train_cost:  2.7729623 train_acc:  0.3046875 test_cost:  2.512659 test_acc:  0.296875\n",
            "iter:  59 train_cost:  2.6271868 train_acc:  0.2578125 test_cost:  2.5899565 test_acc:  0.3125\n",
            "iter:  60 train_cost:  2.900109 train_acc:  0.328125 test_cost:  2.2704067 test_acc:  0.3515625\n",
            "iter:  61 train_cost:  2.6813943 train_acc:  0.2734375 test_cost:  2.597691 test_acc:  0.3046875\n",
            "iter:  62 train_cost:  2.6294641 train_acc:  0.2890625 test_cost:  2.346715 test_acc:  0.3203125\n",
            "iter:  63 train_cost:  2.53826 train_acc:  0.3203125 test_cost:  2.399193 test_acc:  0.3359375\n",
            "iter:  64 train_cost:  2.66303 train_acc:  0.328125 test_cost:  2.7457213 test_acc:  0.2265625\n",
            "iter:  65 train_cost:  2.869123 train_acc:  0.2890625 test_cost:  2.3497975 test_acc:  0.3828125\n",
            "iter:  66 train_cost:  2.696069 train_acc:  0.3125 test_cost:  2.4716868 test_acc:  0.3125\n",
            "iter:  67 train_cost:  2.3879864 train_acc:  0.3671875 test_cost:  2.5824962 test_acc:  0.3203125\n",
            "iter:  68 train_cost:  2.3690693 train_acc:  0.421875 test_cost:  2.43911 test_acc:  0.3984375\n",
            "iter:  69 train_cost:  2.3830066 train_acc:  0.34375 test_cost:  2.3824868 test_acc:  0.34375\n",
            "iter:  70 train_cost:  2.833352 train_acc:  0.234375 test_cost:  2.2835345 test_acc:  0.3984375\n",
            "iter:  71 train_cost:  2.3770218 train_acc:  0.34375 test_cost:  2.4155738 test_acc:  0.3359375\n",
            "iter:  72 train_cost:  2.2323987 train_acc:  0.3671875 test_cost:  1.9111972 test_acc:  0.4140625\n",
            "iter:  73 train_cost:  2.662134 train_acc:  0.3125 test_cost:  2.5494866 test_acc:  0.296875\n",
            "iter:  74 train_cost:  2.1930954 train_acc:  0.4140625 test_cost:  2.321652 test_acc:  0.390625\n",
            "iter:  75 train_cost:  1.7109395 train_acc:  0.4765625 test_cost:  2.3651094 test_acc:  0.296875\n",
            "iter:  76 train_cost:  2.2008877 train_acc:  0.34375 test_cost:  2.1665664 test_acc:  0.4453125\n",
            "iter:  77 train_cost:  2.2610304 train_acc:  0.40625 test_cost:  2.1317995 test_acc:  0.375\n",
            "iter:  78 train_cost:  1.953233 train_acc:  0.375 test_cost:  1.935901 test_acc:  0.421875\n",
            "iter:  79 train_cost:  2.205906 train_acc:  0.4140625 test_cost:  2.0088577 test_acc:  0.359375\n",
            "iter:  80 train_cost:  2.15834 train_acc:  0.359375 test_cost:  2.4503198 test_acc:  0.390625\n",
            "iter:  81 train_cost:  2.175582 train_acc:  0.40625 test_cost:  2.0392091 test_acc:  0.3515625\n",
            "iter:  82 train_cost:  2.3294654 train_acc:  0.3359375 test_cost:  2.3225074 test_acc:  0.4140625\n",
            "iter:  83 train_cost:  2.1890783 train_acc:  0.328125 test_cost:  2.3809536 test_acc:  0.328125\n",
            "iter:  84 train_cost:  2.3300228 train_acc:  0.328125 test_cost:  2.5504313 test_acc:  0.296875\n",
            "iter:  85 train_cost:  2.0487862 train_acc:  0.40625 test_cost:  1.9355214 test_acc:  0.4296875\n",
            "iter:  86 train_cost:  2.190836 train_acc:  0.3984375 test_cost:  2.1242049 test_acc:  0.375\n",
            "iter:  87 train_cost:  2.2538838 train_acc:  0.359375 test_cost:  1.9730644 test_acc:  0.5\n",
            "iter:  88 train_cost:  2.0553288 train_acc:  0.4140625 test_cost:  1.8864975 test_acc:  0.4609375\n",
            "iter:  89 train_cost:  1.945101 train_acc:  0.4296875 test_cost:  1.9530687 test_acc:  0.4453125\n",
            "iter:  90 train_cost:  1.932896 train_acc:  0.4609375 test_cost:  1.9131118 test_acc:  0.4453125\n",
            "iter:  91 train_cost:  1.9715759 train_acc:  0.3984375 test_cost:  1.9099865 test_acc:  0.40625\n",
            "iter:  92 train_cost:  2.3422399 train_acc:  0.34375 test_cost:  1.8408295 test_acc:  0.4453125\n",
            "iter:  93 train_cost:  1.935131 train_acc:  0.4296875 test_cost:  1.8972316 test_acc:  0.4375\n",
            "iter:  94 train_cost:  1.992482 train_acc:  0.4609375 test_cost:  1.9646155 test_acc:  0.4453125\n",
            "iter:  95 train_cost:  2.1485066 train_acc:  0.4296875 test_cost:  1.9137027 test_acc:  0.421875\n",
            "iter:  96 train_cost:  2.178475 train_acc:  0.390625 test_cost:  1.652381 test_acc:  0.484375\n",
            "iter:  97 train_cost:  1.8774409 train_acc:  0.484375 test_cost:  1.8659779 test_acc:  0.4765625\n",
            "iter:  98 train_cost:  1.928925 train_acc:  0.453125 test_cost:  2.0301635 test_acc:  0.390625\n",
            "iter:  99 train_cost:  1.9919293 train_acc:  0.390625 test_cost:  1.9408262 test_acc:  0.4296875\n",
            "iter:  100 train_cost:  2.0211577 train_acc:  0.4375 test_cost:  2.1792333 test_acc:  0.421875\n",
            "iter:  101 train_cost:  2.0657792 train_acc:  0.3828125 test_cost:  1.8084841 test_acc:  0.453125\n",
            "iter:  102 train_cost:  1.8347353 train_acc:  0.4375 test_cost:  1.9892446 test_acc:  0.34375\n",
            "iter:  103 train_cost:  2.04745 train_acc:  0.3828125 test_cost:  1.9161127 test_acc:  0.421875\n",
            "iter:  104 train_cost:  2.0193367 train_acc:  0.4609375 test_cost:  2.005471 test_acc:  0.453125\n",
            "iter:  105 train_cost:  1.993656 train_acc:  0.4375 test_cost:  1.2553966 test_acc:  0.5625\n",
            "iter:  106 train_cost:  1.9572139 train_acc:  0.4375 test_cost:  1.637877 test_acc:  0.5\n",
            "iter:  107 train_cost:  2.0286744 train_acc:  0.40625 test_cost:  2.0405502 test_acc:  0.3984375\n",
            "iter:  108 train_cost:  1.7558633 train_acc:  0.40625 test_cost:  1.6502529 test_acc:  0.484375\n",
            "iter:  109 train_cost:  1.6355519 train_acc:  0.4609375 test_cost:  2.069784 test_acc:  0.375\n",
            "iter:  110 train_cost:  1.6673969 train_acc:  0.484375 test_cost:  1.9171867 test_acc:  0.4296875\n",
            "iter:  111 train_cost:  1.7896426 train_acc:  0.4765625 test_cost:  1.6699862 test_acc:  0.421875\n",
            "iter:  112 train_cost:  1.8127148 train_acc:  0.515625 test_cost:  1.6083195 test_acc:  0.5078125\n",
            "iter:  113 train_cost:  1.6749886 train_acc:  0.4921875 test_cost:  1.7849298 test_acc:  0.4375\n",
            "iter:  114 train_cost:  1.7088082 train_acc:  0.4609375 test_cost:  1.7442327 test_acc:  0.4375\n",
            "iter:  115 train_cost:  1.7924054 train_acc:  0.4140625 test_cost:  2.1748517 test_acc:  0.34375\n",
            "iter:  116 train_cost:  1.6465328 train_acc:  0.46875 test_cost:  1.7128913 test_acc:  0.4375\n",
            "iter:  117 train_cost:  1.7084801 train_acc:  0.46875 test_cost:  1.5333738 test_acc:  0.5234375\n",
            "iter:  118 train_cost:  1.9321704 train_acc:  0.4453125 test_cost:  1.4280093 test_acc:  0.5625\n",
            "iter:  119 train_cost:  1.6779798 train_acc:  0.4765625 test_cost:  1.8957918 test_acc:  0.4140625\n",
            "iter:  120 train_cost:  1.6526859 train_acc:  0.453125 test_cost:  1.6335394 test_acc:  0.4921875\n",
            "iter:  121 train_cost:  1.8297317 train_acc:  0.3984375 test_cost:  1.7266504 test_acc:  0.4921875\n",
            "iter:  122 train_cost:  1.5182517 train_acc:  0.46875 test_cost:  1.3622096 test_acc:  0.5546875\n",
            "iter:  123 train_cost:  1.7406207 train_acc:  0.4765625 test_cost:  1.7689639 test_acc:  0.46875\n",
            "iter:  124 train_cost:  1.5360744 train_acc:  0.515625 test_cost:  1.542059 test_acc:  0.4921875\n",
            "iter:  125 train_cost:  1.7705812 train_acc:  0.5 test_cost:  1.5824866 test_acc:  0.5078125\n",
            "iter:  126 train_cost:  1.5597737 train_acc:  0.5234375 test_cost:  1.6156585 test_acc:  0.5078125\n",
            "iter:  127 train_cost:  1.7048502 train_acc:  0.4453125 test_cost:  1.4083359 test_acc:  0.515625\n",
            "iter:  128 train_cost:  1.4685999 train_acc:  0.5546875 test_cost:  1.4864283 test_acc:  0.5625\n",
            "iter:  129 train_cost:  1.5122526 train_acc:  0.5 test_cost:  1.6605874 test_acc:  0.484375\n",
            "iter:  130 train_cost:  1.5815973 train_acc:  0.546875 test_cost:  1.7498457 test_acc:  0.5234375\n",
            "iter:  131 train_cost:  1.5850935 train_acc:  0.5234375 test_cost:  1.8284824 test_acc:  0.4140625\n",
            "iter:  132 train_cost:  1.5617819 train_acc:  0.515625 test_cost:  1.5031961 test_acc:  0.515625\n",
            "iter:  133 train_cost:  1.565572 train_acc:  0.4609375 test_cost:  1.6001267 test_acc:  0.484375\n",
            "iter:  134 train_cost:  1.6349647 train_acc:  0.5234375 test_cost:  1.5536582 test_acc:  0.46875\n",
            "iter:  135 train_cost:  1.5469489 train_acc:  0.59375 test_cost:  1.4569933 test_acc:  0.53125\n",
            "iter:  136 train_cost:  1.5529034 train_acc:  0.5 test_cost:  1.5745146 test_acc:  0.53125\n",
            "iter:  137 train_cost:  1.8769429 train_acc:  0.4609375 test_cost:  1.3853781 test_acc:  0.5625\n",
            "iter:  138 train_cost:  1.5756176 train_acc:  0.484375 test_cost:  1.5229182 test_acc:  0.5\n",
            "iter:  139 train_cost:  1.795897 train_acc:  0.4609375 test_cost:  2.0154858 test_acc:  0.4296875\n",
            "iter:  140 train_cost:  1.4180009 train_acc:  0.5859375 test_cost:  1.7147609 test_acc:  0.4609375\n",
            "iter:  141 train_cost:  1.4288801 train_acc:  0.5703125 test_cost:  1.6419064 test_acc:  0.484375\n",
            "iter:  142 train_cost:  1.5638447 train_acc:  0.515625 test_cost:  1.5299697 test_acc:  0.5234375\n",
            "iter:  143 train_cost:  1.5133593 train_acc:  0.5234375 test_cost:  1.6090212 test_acc:  0.5078125\n",
            "iter:  144 train_cost:  1.4129786 train_acc:  0.5390625 test_cost:  1.4740305 test_acc:  0.5234375\n",
            "iter:  145 train_cost:  1.6812165 train_acc:  0.4609375 test_cost:  1.2747952 test_acc:  0.6171875\n",
            "iter:  146 train_cost:  1.5352399 train_acc:  0.515625 test_cost:  1.7177076 test_acc:  0.4609375\n",
            "iter:  147 train_cost:  1.5615022 train_acc:  0.53125 test_cost:  1.7770796 test_acc:  0.4609375\n",
            "iter:  148 train_cost:  1.5590932 train_acc:  0.4921875 test_cost:  1.3394856 test_acc:  0.546875\n",
            "iter:  149 train_cost:  1.1838133 train_acc:  0.6015625 test_cost:  1.3573469 test_acc:  0.5546875\n",
            "iter:  150 train_cost:  1.2079189 train_acc:  0.5546875 test_cost:  1.4714979 test_acc:  0.515625\n",
            "iter:  151 train_cost:  1.4713221 train_acc:  0.5625 test_cost:  1.3881063 test_acc:  0.5625\n",
            "iter:  152 train_cost:  1.5438352 train_acc:  0.5 test_cost:  1.3898251 test_acc:  0.5390625\n",
            "iter:  153 train_cost:  1.3817167 train_acc:  0.578125 test_cost:  1.470715 test_acc:  0.484375\n",
            "iter:  154 train_cost:  1.3675814 train_acc:  0.5703125 test_cost:  1.4409205 test_acc:  0.5390625\n",
            "iter:  155 train_cost:  1.4469328 train_acc:  0.53125 test_cost:  1.6292198 test_acc:  0.546875\n",
            "iter:  156 train_cost:  1.5575721 train_acc:  0.515625 test_cost:  1.6462382 test_acc:  0.484375\n",
            "iter:  157 train_cost:  1.5207791 train_acc:  0.4921875 test_cost:  1.5364442 test_acc:  0.515625\n",
            "iter:  158 train_cost:  1.6527905 train_acc:  0.4921875 test_cost:  1.4558322 test_acc:  0.546875\n",
            "iter:  159 train_cost:  1.4278295 train_acc:  0.546875 test_cost:  1.448441 test_acc:  0.5546875\n",
            "iter:  160 train_cost:  1.3601773 train_acc:  0.6015625 test_cost:  1.2687786 test_acc:  0.5859375\n",
            "iter:  161 train_cost:  1.4825423 train_acc:  0.5 test_cost:  1.2025816 test_acc:  0.6015625\n",
            "iter:  162 train_cost:  1.619194 train_acc:  0.484375 test_cost:  1.6243155 test_acc:  0.453125\n",
            "iter:  163 train_cost:  1.1438416 train_acc:  0.5625 test_cost:  1.0611986 test_acc:  0.6484375\n",
            "iter:  164 train_cost:  1.2763892 train_acc:  0.6640625 test_cost:  1.152409 test_acc:  0.59375\n",
            "iter:  165 train_cost:  1.3694038 train_acc:  0.53125 test_cost:  1.333585 test_acc:  0.5390625\n",
            "iter:  166 train_cost:  1.4129646 train_acc:  0.5859375 test_cost:  1.6653006 test_acc:  0.4765625\n",
            "iter:  167 train_cost:  1.3445398 train_acc:  0.484375 test_cost:  1.5477297 test_acc:  0.53125\n",
            "iter:  168 train_cost:  1.2789631 train_acc:  0.5859375 test_cost:  1.1474326 test_acc:  0.6171875\n",
            "iter:  169 train_cost:  1.3681321 train_acc:  0.5625 test_cost:  1.2866194 test_acc:  0.5859375\n",
            "iter:  170 train_cost:  1.2817982 train_acc:  0.640625 test_cost:  1.6393466 test_acc:  0.5\n",
            "iter:  171 train_cost:  1.2588093 train_acc:  0.5703125 test_cost:  1.2996964 test_acc:  0.5546875\n",
            "iter:  172 train_cost:  1.2610142 train_acc:  0.6328125 test_cost:  1.1430475 test_acc:  0.6015625\n",
            "iter:  173 train_cost:  1.0894341 train_acc:  0.6796875 test_cost:  1.279294 test_acc:  0.625\n",
            "iter:  174 train_cost:  1.5202571 train_acc:  0.5625 test_cost:  1.7070022 test_acc:  0.484375\n",
            "iter:  175 train_cost:  1.4028054 train_acc:  0.6328125 test_cost:  1.2359207 test_acc:  0.5234375\n",
            "iter:  176 train_cost:  1.101674 train_acc:  0.625 test_cost:  1.595958 test_acc:  0.5390625\n",
            "iter:  177 train_cost:  1.5233233 train_acc:  0.4765625 test_cost:  1.2911702 test_acc:  0.5703125\n",
            "iter:  178 train_cost:  1.1775043 train_acc:  0.6328125 test_cost:  1.4320652 test_acc:  0.546875\n",
            "iter:  179 train_cost:  1.2025193 train_acc:  0.609375 test_cost:  1.3070477 test_acc:  0.6171875\n",
            "iter:  180 train_cost:  1.2937245 train_acc:  0.5546875 test_cost:  1.1705198 test_acc:  0.625\n",
            "iter:  181 train_cost:  1.6830376 train_acc:  0.484375 test_cost:  1.3710486 test_acc:  0.578125\n",
            "iter:  182 train_cost:  1.2927462 train_acc:  0.640625 test_cost:  1.1629262 test_acc:  0.5859375\n",
            "iter:  183 train_cost:  1.455037 train_acc:  0.5703125 test_cost:  1.2372227 test_acc:  0.6171875\n",
            "iter:  184 train_cost:  1.4379473 train_acc:  0.5 test_cost:  1.2325467 test_acc:  0.5625\n",
            "iter:  185 train_cost:  1.3895482 train_acc:  0.546875 test_cost:  1.1603966 test_acc:  0.59375\n",
            "iter:  186 train_cost:  1.1530235 train_acc:  0.59375 test_cost:  1.4724215 test_acc:  0.546875\n",
            "iter:  187 train_cost:  1.3453627 train_acc:  0.5 test_cost:  1.2726395 test_acc:  0.625\n",
            "iter:  188 train_cost:  1.1495706 train_acc:  0.578125 test_cost:  1.6718184 test_acc:  0.5\n",
            "iter:  189 train_cost:  1.1856738 train_acc:  0.6171875 test_cost:  1.1735028 test_acc:  0.6484375\n",
            "iter:  190 train_cost:  1.5425315 train_acc:  0.5859375 test_cost:  1.3959396 test_acc:  0.59375\n",
            "iter:  191 train_cost:  1.2699088 train_acc:  0.625 test_cost:  1.2479395 test_acc:  0.640625\n",
            "iter:  192 train_cost:  1.0968988 train_acc:  0.6015625 test_cost:  1.3655236 test_acc:  0.5703125\n",
            "iter:  193 train_cost:  1.5005665 train_acc:  0.5546875 test_cost:  1.1760871 test_acc:  0.6171875\n",
            "iter:  194 train_cost:  1.0876982 train_acc:  0.6328125 test_cost:  1.2772902 test_acc:  0.609375\n",
            "iter:  195 train_cost:  1.260296 train_acc:  0.5859375 test_cost:  1.0262709 test_acc:  0.7109375\n",
            "iter:  196 train_cost:  1.3537642 train_acc:  0.53125 test_cost:  1.293469 test_acc:  0.53125\n",
            "iter:  197 train_cost:  1.663207 train_acc:  0.5234375 test_cost:  1.419993 test_acc:  0.5625\n",
            "iter:  198 train_cost:  1.2991034 train_acc:  0.5703125 test_cost:  1.1998453 test_acc:  0.6015625\n",
            "iter:  199 train_cost:  1.1355677 train_acc:  0.640625 test_cost:  1.3249152 test_acc:  0.609375\n",
            "iter:  200 train_cost:  1.2204711 train_acc:  0.609375 test_cost:  1.3132486 test_acc:  0.609375\n",
            "iter:  201 train_cost:  1.2350583 train_acc:  0.5859375 test_cost:  1.0932543 test_acc:  0.6328125\n",
            "iter:  202 train_cost:  1.2691147 train_acc:  0.640625 test_cost:  1.3087451 test_acc:  0.5625\n",
            "iter:  203 train_cost:  1.2185029 train_acc:  0.6328125 test_cost:  1.1315196 test_acc:  0.6171875\n",
            "iter:  204 train_cost:  1.3185685 train_acc:  0.59375 test_cost:  1.3228683 test_acc:  0.5625\n",
            "iter:  205 train_cost:  1.1464671 train_acc:  0.640625 test_cost:  1.2542629 test_acc:  0.640625\n",
            "iter:  206 train_cost:  1.2852776 train_acc:  0.6015625 test_cost:  1.0447308 test_acc:  0.6484375\n",
            "iter:  207 train_cost:  1.146806 train_acc:  0.6015625 test_cost:  1.0785296 test_acc:  0.6484375\n",
            "iter:  208 train_cost:  1.1940706 train_acc:  0.59375 test_cost:  1.0904679 test_acc:  0.6015625\n",
            "iter:  209 train_cost:  1.3883996 train_acc:  0.578125 test_cost:  1.2447546 test_acc:  0.609375\n",
            "iter:  210 train_cost:  1.2188926 train_acc:  0.65625 test_cost:  1.0798415 test_acc:  0.640625\n",
            "iter:  211 train_cost:  1.0715028 train_acc:  0.6015625 test_cost:  1.0878134 test_acc:  0.65625\n",
            "iter:  212 train_cost:  1.3763608 train_acc:  0.59375 test_cost:  1.0732923 test_acc:  0.6484375\n",
            "iter:  213 train_cost:  1.3681073 train_acc:  0.53125 test_cost:  1.2355189 test_acc:  0.578125\n",
            "iter:  214 train_cost:  1.197299 train_acc:  0.625 test_cost:  1.0850389 test_acc:  0.6484375\n",
            "iter:  215 train_cost:  1.0237557 train_acc:  0.640625 test_cost:  1.2255018 test_acc:  0.546875\n",
            "iter:  216 train_cost:  1.1108947 train_acc:  0.6171875 test_cost:  1.1773294 test_acc:  0.6171875\n",
            "iter:  217 train_cost:  1.2179693 train_acc:  0.6015625 test_cost:  1.0308486 test_acc:  0.6875\n",
            "iter:  218 train_cost:  1.1365962 train_acc:  0.640625 test_cost:  0.91610956 test_acc:  0.703125\n",
            "iter:  219 train_cost:  1.0395489 train_acc:  0.625 test_cost:  0.9721038 test_acc:  0.65625\n",
            "iter:  220 train_cost:  0.968246 train_acc:  0.734375 test_cost:  0.93811977 test_acc:  0.6953125\n",
            "iter:  221 train_cost:  1.0930452 train_acc:  0.671875 test_cost:  1.2022705 test_acc:  0.6171875\n",
            "iter:  222 train_cost:  1.0793769 train_acc:  0.6171875 test_cost:  1.1399095 test_acc:  0.6484375\n",
            "iter:  223 train_cost:  0.92593956 train_acc:  0.671875 test_cost:  1.1012174 test_acc:  0.6640625\n",
            "iter:  224 train_cost:  1.1556265 train_acc:  0.65625 test_cost:  1.1881099 test_acc:  0.6328125\n",
            "iter:  225 train_cost:  1.1063544 train_acc:  0.6015625 test_cost:  1.1916444 test_acc:  0.671875\n",
            "iter:  226 train_cost:  0.9851328 train_acc:  0.6796875 test_cost:  0.9467577 test_acc:  0.7109375\n",
            "iter:  227 train_cost:  1.1681222 train_acc:  0.65625 test_cost:  0.9158323 test_acc:  0.7265625\n",
            "iter:  228 train_cost:  1.2948349 train_acc:  0.609375 test_cost:  1.2164745 test_acc:  0.5859375\n",
            "iter:  229 train_cost:  1.0873454 train_acc:  0.703125 test_cost:  1.2840374 test_acc:  0.59375\n",
            "iter:  230 train_cost:  1.3567245 train_acc:  0.5625 test_cost:  0.95398265 test_acc:  0.703125\n",
            "iter:  231 train_cost:  1.2144456 train_acc:  0.6171875 test_cost:  1.211305 test_acc:  0.6015625\n",
            "iter:  232 train_cost:  1.1063073 train_acc:  0.6328125 test_cost:  1.2026931 test_acc:  0.6640625\n",
            "iter:  233 train_cost:  1.1698943 train_acc:  0.6171875 test_cost:  1.0966438 test_acc:  0.6640625\n",
            "iter:  234 train_cost:  1.1341568 train_acc:  0.6328125 test_cost:  1.1182349 test_acc:  0.640625\n",
            "iter:  235 train_cost:  1.016016 train_acc:  0.6796875 test_cost:  1.0093572 test_acc:  0.6796875\n",
            "iter:  236 train_cost:  1.2310143 train_acc:  0.6171875 test_cost:  1.2339315 test_acc:  0.609375\n",
            "iter:  237 train_cost:  1.0734334 train_acc:  0.65625 test_cost:  1.271189 test_acc:  0.5703125\n",
            "iter:  238 train_cost:  1.0896266 train_acc:  0.6640625 test_cost:  1.077313 test_acc:  0.6953125\n",
            "iter:  239 train_cost:  0.9514797 train_acc:  0.671875 test_cost:  1.2894721 test_acc:  0.59375\n",
            "iter:  240 train_cost:  1.0388391 train_acc:  0.6953125 test_cost:  1.0433065 test_acc:  0.6796875\n",
            "iter:  241 train_cost:  0.9469224 train_acc:  0.6953125 test_cost:  1.0153043 test_acc:  0.6171875\n",
            "iter:  242 train_cost:  1.0458195 train_acc:  0.6796875 test_cost:  0.83212245 test_acc:  0.734375\n",
            "iter:  243 train_cost:  0.9127744 train_acc:  0.6875 test_cost:  0.9291157 test_acc:  0.6953125\n",
            "iter:  244 train_cost:  1.0198663 train_acc:  0.7109375 test_cost:  0.9295336 test_acc:  0.6875\n",
            "iter:  245 train_cost:  1.0826417 train_acc:  0.703125 test_cost:  1.1217632 test_acc:  0.6171875\n",
            "iter:  246 train_cost:  1.2027404 train_acc:  0.6328125 test_cost:  1.4249761 test_acc:  0.6171875\n",
            "iter:  247 train_cost:  1.0169672 train_acc:  0.640625 test_cost:  1.0410762 test_acc:  0.6328125\n",
            "iter:  248 train_cost:  1.1056511 train_acc:  0.6484375 test_cost:  0.97816396 test_acc:  0.6640625\n",
            "iter:  249 train_cost:  1.0414779 train_acc:  0.671875 test_cost:  0.866297 test_acc:  0.6875\n",
            "iter:  250 train_cost:  0.96349525 train_acc:  0.703125 test_cost:  1.0815446 test_acc:  0.640625\n",
            "iter:  251 train_cost:  1.1764134 train_acc:  0.6875 test_cost:  1.2131398 test_acc:  0.609375\n",
            "iter:  252 train_cost:  0.99585855 train_acc:  0.65625 test_cost:  0.97283757 test_acc:  0.6875\n",
            "iter:  253 train_cost:  1.0910335 train_acc:  0.6875 test_cost:  0.9042346 test_acc:  0.65625\n",
            "iter:  254 train_cost:  0.99274474 train_acc:  0.640625 test_cost:  0.9784621 test_acc:  0.640625\n",
            "iter:  255 train_cost:  1.0523108 train_acc:  0.6328125 test_cost:  0.7880898 test_acc:  0.734375\n",
            "iter:  256 train_cost:  1.0295599 train_acc:  0.6640625 test_cost:  1.1215789 test_acc:  0.6640625\n",
            "iter:  257 train_cost:  0.81658024 train_acc:  0.7421875 test_cost:  1.1848311 test_acc:  0.65625\n",
            "iter:  258 train_cost:  0.9703045 train_acc:  0.671875 test_cost:  0.9300191 test_acc:  0.703125\n",
            "iter:  259 train_cost:  0.80996466 train_acc:  0.7109375 test_cost:  1.1694252 test_acc:  0.640625\n",
            "iter:  260 train_cost:  1.034801 train_acc:  0.640625 test_cost:  1.2070172 test_acc:  0.609375\n",
            "iter:  261 train_cost:  1.2923741 train_acc:  0.59375 test_cost:  0.9316729 test_acc:  0.671875\n",
            "iter:  262 train_cost:  1.0760523 train_acc:  0.6484375 test_cost:  0.9981164 test_acc:  0.671875\n",
            "iter:  263 train_cost:  1.1784601 train_acc:  0.6484375 test_cost:  0.95075494 test_acc:  0.671875\n",
            "iter:  264 train_cost:  0.9878763 train_acc:  0.6875 test_cost:  0.9913332 test_acc:  0.6796875\n",
            "iter:  265 train_cost:  1.2721972 train_acc:  0.6640625 test_cost:  0.94148046 test_acc:  0.703125\n",
            "iter:  266 train_cost:  1.0530767 train_acc:  0.6875 test_cost:  0.9561622 test_acc:  0.6796875\n",
            "iter:  267 train_cost:  1.2376544 train_acc:  0.6015625 test_cost:  0.8725281 test_acc:  0.71875\n",
            "iter:  268 train_cost:  1.0281339 train_acc:  0.65625 test_cost:  0.9645251 test_acc:  0.6953125\n",
            "iter:  269 train_cost:  0.90657187 train_acc:  0.703125 test_cost:  0.8060676 test_acc:  0.7578125\n",
            "iter:  270 train_cost:  1.0818381 train_acc:  0.6953125 test_cost:  0.9888083 test_acc:  0.703125\n",
            "iter:  271 train_cost:  1.2239827 train_acc:  0.65625 test_cost:  0.97217965 test_acc:  0.703125\n",
            "iter:  272 train_cost:  1.1884086 train_acc:  0.640625 test_cost:  1.1510216 test_acc:  0.6640625\n",
            "iter:  273 train_cost:  1.2284987 train_acc:  0.609375 test_cost:  0.9974097 test_acc:  0.6875\n",
            "iter:  274 train_cost:  0.8926707 train_acc:  0.6875 test_cost:  1.0526348 test_acc:  0.703125\n",
            "iter:  275 train_cost:  0.8527372 train_acc:  0.7734375 test_cost:  1.0094717 test_acc:  0.65625\n",
            "iter:  276 train_cost:  0.95541584 train_acc:  0.6796875 test_cost:  1.077766 test_acc:  0.6328125\n",
            "iter:  277 train_cost:  0.8192214 train_acc:  0.7890625 test_cost:  1.0026383 test_acc:  0.703125\n",
            "iter:  278 train_cost:  0.9449615 train_acc:  0.703125 test_cost:  0.8539598 test_acc:  0.6953125\n",
            "iter:  279 train_cost:  0.99944913 train_acc:  0.671875 test_cost:  0.9436314 test_acc:  0.6875\n",
            "iter:  280 train_cost:  1.3506489 train_acc:  0.6328125 test_cost:  0.80038255 test_acc:  0.75\n",
            "iter:  281 train_cost:  1.0836442 train_acc:  0.6484375 test_cost:  0.9897662 test_acc:  0.65625\n",
            "iter:  282 train_cost:  0.9512689 train_acc:  0.6640625 test_cost:  0.86957633 test_acc:  0.7265625\n",
            "iter:  283 train_cost:  1.0524281 train_acc:  0.6640625 test_cost:  0.92816794 test_acc:  0.703125\n",
            "iter:  284 train_cost:  0.9301044 train_acc:  0.7265625 test_cost:  0.9913043 test_acc:  0.65625\n",
            "iter:  285 train_cost:  1.0566051 train_acc:  0.640625 test_cost:  1.110112 test_acc:  0.625\n",
            "iter:  286 train_cost:  1.1157353 train_acc:  0.6484375 test_cost:  0.9000851 test_acc:  0.6875\n",
            "iter:  287 train_cost:  0.9416203 train_acc:  0.640625 test_cost:  0.8230788 test_acc:  0.765625\n",
            "iter:  288 train_cost:  1.127516 train_acc:  0.703125 test_cost:  0.87132215 test_acc:  0.703125\n",
            "iter:  289 train_cost:  0.978385 train_acc:  0.6171875 test_cost:  1.0045631 test_acc:  0.7109375\n",
            "iter:  290 train_cost:  0.8952691 train_acc:  0.703125 test_cost:  0.9474533 test_acc:  0.734375\n",
            "iter:  291 train_cost:  0.82069325 train_acc:  0.6796875 test_cost:  1.0598468 test_acc:  0.6796875\n",
            "iter:  292 train_cost:  1.0385672 train_acc:  0.6484375 test_cost:  0.907673 test_acc:  0.7265625\n",
            "iter:  293 train_cost:  1.0229715 train_acc:  0.71875 test_cost:  1.2376473 test_acc:  0.609375\n",
            "iter:  294 train_cost:  1.1248788 train_acc:  0.6484375 test_cost:  0.9575273 test_acc:  0.65625\n",
            "iter:  295 train_cost:  0.9519945 train_acc:  0.703125 test_cost:  0.88717675 test_acc:  0.65625\n",
            "iter:  296 train_cost:  0.8511741 train_acc:  0.7109375 test_cost:  0.7070074 test_acc:  0.8046875\n",
            "iter:  297 train_cost:  0.86599016 train_acc:  0.6953125 test_cost:  1.0620747 test_acc:  0.65625\n",
            "iter:  298 train_cost:  0.93396807 train_acc:  0.7109375 test_cost:  0.78619623 test_acc:  0.703125\n",
            "iter:  299 train_cost:  0.95074916 train_acc:  0.671875 test_cost:  0.95155966 test_acc:  0.671875\n",
            "iter:  300 train_cost:  0.98551506 train_acc:  0.6875 test_cost:  0.9758054 test_acc:  0.734375\n",
            "iter:  301 train_cost:  1.0766981 train_acc:  0.6640625 test_cost:  0.904174 test_acc:  0.71875\n",
            "iter:  302 train_cost:  0.93093073 train_acc:  0.6796875 test_cost:  0.8390138 test_acc:  0.734375\n",
            "iter:  303 train_cost:  0.8623283 train_acc:  0.703125 test_cost:  0.987925 test_acc:  0.671875\n",
            "iter:  304 train_cost:  0.84256613 train_acc:  0.7265625 test_cost:  0.87778807 test_acc:  0.75\n",
            "iter:  305 train_cost:  1.0468768 train_acc:  0.6953125 test_cost:  1.090342 test_acc:  0.609375\n",
            "iter:  306 train_cost:  1.0365839 train_acc:  0.671875 test_cost:  1.1165972 test_acc:  0.6328125\n",
            "iter:  307 train_cost:  1.04336 train_acc:  0.71875 test_cost:  0.75313354 test_acc:  0.75\n",
            "iter:  308 train_cost:  0.9917524 train_acc:  0.6328125 test_cost:  0.9816786 test_acc:  0.7109375\n",
            "iter:  309 train_cost:  0.9096222 train_acc:  0.7109375 test_cost:  1.1666055 test_acc:  0.6484375\n",
            "iter:  310 train_cost:  1.1123877 train_acc:  0.6171875 test_cost:  1.1461321 test_acc:  0.6328125\n",
            "iter:  311 train_cost:  0.9039725 train_acc:  0.671875 test_cost:  0.8782538 test_acc:  0.703125\n",
            "iter:  312 train_cost:  1.1085162 train_acc:  0.6640625 test_cost:  0.8734922 test_acc:  0.6640625\n",
            "iter:  313 train_cost:  0.83359784 train_acc:  0.7421875 test_cost:  0.9443974 test_acc:  0.6796875\n",
            "iter:  314 train_cost:  1.0293379 train_acc:  0.640625 test_cost:  0.9854229 test_acc:  0.7109375\n",
            "iter:  315 train_cost:  0.79390454 train_acc:  0.734375 test_cost:  0.93806815 test_acc:  0.7109375\n",
            "iter:  316 train_cost:  0.7144345 train_acc:  0.796875 test_cost:  0.87032104 test_acc:  0.734375\n",
            "iter:  317 train_cost:  1.0401084 train_acc:  0.6640625 test_cost:  0.8303795 test_acc:  0.7578125\n",
            "iter:  318 train_cost:  0.9735464 train_acc:  0.6328125 test_cost:  1.0056303 test_acc:  0.6484375\n",
            "iter:  319 train_cost:  0.6971617 train_acc:  0.7890625 test_cost:  0.91146564 test_acc:  0.6875\n",
            "iter:  320 train_cost:  0.9950233 train_acc:  0.6953125 test_cost:  1.0796256 test_acc:  0.6796875\n",
            "iter:  321 train_cost:  0.943541 train_acc:  0.6640625 test_cost:  0.95234513 test_acc:  0.6875\n",
            "iter:  322 train_cost:  0.9470874 train_acc:  0.6640625 test_cost:  0.8253007 test_acc:  0.734375\n",
            "iter:  323 train_cost:  1.1950936 train_acc:  0.6171875 test_cost:  0.66369665 test_acc:  0.71875\n",
            "iter:  324 train_cost:  1.0060173 train_acc:  0.6640625 test_cost:  0.94455075 test_acc:  0.703125\n",
            "iter:  325 train_cost:  1.0681535 train_acc:  0.703125 test_cost:  0.9056682 test_acc:  0.6796875\n",
            "iter:  326 train_cost:  1.0868267 train_acc:  0.671875 test_cost:  0.85041034 test_acc:  0.765625\n",
            "iter:  327 train_cost:  1.0399268 train_acc:  0.6328125 test_cost:  0.8787185 test_acc:  0.734375\n",
            "iter:  328 train_cost:  0.9138271 train_acc:  0.734375 test_cost:  0.7893734 test_acc:  0.734375\n",
            "iter:  329 train_cost:  0.9476832 train_acc:  0.75 test_cost:  0.9966092 test_acc:  0.703125\n",
            "iter:  330 train_cost:  0.8564983 train_acc:  0.7421875 test_cost:  0.98733926 test_acc:  0.71875\n",
            "iter:  331 train_cost:  0.8789784 train_acc:  0.7265625 test_cost:  0.8366208 test_acc:  0.7109375\n",
            "iter:  332 train_cost:  0.8809 train_acc:  0.6875 test_cost:  0.6440433 test_acc:  0.7734375\n",
            "iter:  333 train_cost:  0.7930561 train_acc:  0.703125 test_cost:  0.73931545 test_acc:  0.75\n",
            "iter:  334 train_cost:  0.9433677 train_acc:  0.6875 test_cost:  0.97295034 test_acc:  0.6875\n",
            "iter:  335 train_cost:  0.98244107 train_acc:  0.671875 test_cost:  0.97249967 test_acc:  0.703125\n",
            "iter:  336 train_cost:  0.8632405 train_acc:  0.7109375 test_cost:  0.93631804 test_acc:  0.6640625\n",
            "iter:  337 train_cost:  1.2397082 train_acc:  0.640625 test_cost:  0.87566435 test_acc:  0.7109375\n",
            "iter:  338 train_cost:  0.8351149 train_acc:  0.6796875 test_cost:  0.95198333 test_acc:  0.734375\n",
            "iter:  339 train_cost:  0.95422196 train_acc:  0.6875 test_cost:  1.1319826 test_acc:  0.671875\n",
            "iter:  340 train_cost:  0.68631196 train_acc:  0.765625 test_cost:  1.0797848 test_acc:  0.6796875\n",
            "iter:  341 train_cost:  0.8040025 train_acc:  0.7109375 test_cost:  0.94780016 test_acc:  0.65625\n",
            "iter:  342 train_cost:  0.8697218 train_acc:  0.6796875 test_cost:  0.90992224 test_acc:  0.7109375\n",
            "iter:  343 train_cost:  0.761274 train_acc:  0.7578125 test_cost:  0.93459165 test_acc:  0.6875\n",
            "iter:  344 train_cost:  0.87194586 train_acc:  0.78125 test_cost:  0.824728 test_acc:  0.7578125\n",
            "iter:  345 train_cost:  0.6921232 train_acc:  0.7734375 test_cost:  0.81717 test_acc:  0.6875\n",
            "iter:  346 train_cost:  0.79394984 train_acc:  0.734375 test_cost:  0.79564446 test_acc:  0.734375\n",
            "iter:  347 train_cost:  1.0263522 train_acc:  0.703125 test_cost:  0.8563141 test_acc:  0.71875\n",
            "iter:  348 train_cost:  0.8696496 train_acc:  0.734375 test_cost:  0.9027693 test_acc:  0.6875\n",
            "iter:  349 train_cost:  0.9329163 train_acc:  0.7109375 test_cost:  0.9344224 test_acc:  0.7265625\n",
            "iter:  350 train_cost:  0.7852222 train_acc:  0.765625 test_cost:  0.64694583 test_acc:  0.8046875\n",
            "iter:  351 train_cost:  0.6934296 train_acc:  0.7421875 test_cost:  0.8396132 test_acc:  0.765625\n",
            "iter:  352 train_cost:  0.7049533 train_acc:  0.765625 test_cost:  0.7828082 test_acc:  0.7421875\n",
            "iter:  353 train_cost:  0.7022597 train_acc:  0.7578125 test_cost:  0.723963 test_acc:  0.765625\n",
            "iter:  354 train_cost:  0.8308524 train_acc:  0.75 test_cost:  0.91386294 test_acc:  0.71875\n",
            "iter:  355 train_cost:  0.90930724 train_acc:  0.6640625 test_cost:  0.8109145 test_acc:  0.7109375\n",
            "iter:  356 train_cost:  0.85814744 train_acc:  0.71875 test_cost:  0.9342517 test_acc:  0.703125\n",
            "iter:  357 train_cost:  0.7061283 train_acc:  0.75 test_cost:  0.7118298 test_acc:  0.796875\n",
            "iter:  358 train_cost:  0.7886164 train_acc:  0.75 test_cost:  0.7778722 test_acc:  0.7421875\n",
            "iter:  359 train_cost:  0.87331223 train_acc:  0.6953125 test_cost:  0.93552095 test_acc:  0.6875\n",
            "iter:  360 train_cost:  0.70424795 train_acc:  0.796875 test_cost:  0.9351967 test_acc:  0.7265625\n",
            "iter:  361 train_cost:  0.84600925 train_acc:  0.7421875 test_cost:  0.8730347 test_acc:  0.7421875\n",
            "iter:  362 train_cost:  0.8967793 train_acc:  0.75 test_cost:  0.81854486 test_acc:  0.71875\n",
            "iter:  363 train_cost:  0.85505605 train_acc:  0.6953125 test_cost:  0.684875 test_acc:  0.7890625\n",
            "iter:  364 train_cost:  0.81596255 train_acc:  0.71875 test_cost:  0.76176 test_acc:  0.7421875\n",
            "iter:  365 train_cost:  0.78038967 train_acc:  0.7421875 test_cost:  0.82208115 test_acc:  0.6875\n",
            "iter:  366 train_cost:  0.99095684 train_acc:  0.65625 test_cost:  0.85447675 test_acc:  0.734375\n",
            "iter:  367 train_cost:  0.88131654 train_acc:  0.7578125 test_cost:  0.99084795 test_acc:  0.6875\n",
            "iter:  368 train_cost:  0.7671826 train_acc:  0.8046875 test_cost:  0.6076185 test_acc:  0.78125\n",
            "iter:  369 train_cost:  1.0206467 train_acc:  0.6328125 test_cost:  0.7505482 test_acc:  0.765625\n",
            "iter:  370 train_cost:  0.73312414 train_acc:  0.78125 test_cost:  0.7845225 test_acc:  0.765625\n",
            "iter:  371 train_cost:  0.89404416 train_acc:  0.7421875 test_cost:  0.6869218 test_acc:  0.7890625\n",
            "iter:  372 train_cost:  0.9394721 train_acc:  0.734375 test_cost:  0.71018064 test_acc:  0.78125\n",
            "iter:  373 train_cost:  0.7132602 train_acc:  0.7265625 test_cost:  0.912576 test_acc:  0.6875\n",
            "iter:  374 train_cost:  0.80946875 train_acc:  0.7578125 test_cost:  0.8063122 test_acc:  0.703125\n",
            "iter:  375 train_cost:  0.7975093 train_acc:  0.7578125 test_cost:  0.76842546 test_acc:  0.78125\n",
            "iter:  376 train_cost:  0.8138176 train_acc:  0.7421875 test_cost:  0.6754327 test_acc:  0.8046875\n",
            "iter:  377 train_cost:  1.0748467 train_acc:  0.6953125 test_cost:  0.7194357 test_acc:  0.8046875\n",
            "iter:  378 train_cost:  1.0601432 train_acc:  0.65625 test_cost:  0.784629 test_acc:  0.75\n",
            "iter:  379 train_cost:  0.71628857 train_acc:  0.75 test_cost:  0.68148696 test_acc:  0.7734375\n",
            "iter:  380 train_cost:  0.74158055 train_acc:  0.765625 test_cost:  0.78880566 test_acc:  0.71875\n",
            "iter:  381 train_cost:  0.73037565 train_acc:  0.7890625 test_cost:  0.94424975 test_acc:  0.703125\n",
            "iter:  382 train_cost:  0.8849416 train_acc:  0.734375 test_cost:  0.73179686 test_acc:  0.7578125\n",
            "iter:  383 train_cost:  0.98776925 train_acc:  0.7109375 test_cost:  0.7361708 test_acc:  0.6953125\n",
            "iter:  384 train_cost:  0.7634225 train_acc:  0.78125 test_cost:  0.913536 test_acc:  0.703125\n",
            "iter:  385 train_cost:  0.7311481 train_acc:  0.7421875 test_cost:  0.60818803 test_acc:  0.8203125\n",
            "iter:  386 train_cost:  0.84056497 train_acc:  0.7890625 test_cost:  0.95905745 test_acc:  0.734375\n",
            "iter:  387 train_cost:  0.80321527 train_acc:  0.75 test_cost:  0.9210091 test_acc:  0.71875\n",
            "iter:  388 train_cost:  0.86454713 train_acc:  0.7265625 test_cost:  0.87640995 test_acc:  0.734375\n",
            "iter:  389 train_cost:  0.93356365 train_acc:  0.6875 test_cost:  0.75390667 test_acc:  0.7578125\n",
            "iter:  390 train_cost:  0.89319146 train_acc:  0.703125 test_cost:  0.67678356 test_acc:  0.78125\n",
            "iter:  391 train_cost:  0.8632297 train_acc:  0.7109375 test_cost:  0.81792134 test_acc:  0.71875\n",
            "iter:  392 train_cost:  1.1100408 train_acc:  0.734375 test_cost:  0.6521591 test_acc:  0.75\n",
            "iter:  393 train_cost:  0.92842716 train_acc:  0.7421875 test_cost:  0.783201 test_acc:  0.75\n",
            "iter:  394 train_cost:  0.88108087 train_acc:  0.7734375 test_cost:  0.76024127 test_acc:  0.71875\n",
            "iter:  395 train_cost:  0.84819627 train_acc:  0.75 test_cost:  0.7949859 test_acc:  0.7421875\n",
            "iter:  396 train_cost:  1.1300318 train_acc:  0.6640625 test_cost:  0.85083544 test_acc:  0.7421875\n",
            "iter:  397 train_cost:  0.82735425 train_acc:  0.7109375 test_cost:  0.84948623 test_acc:  0.75\n",
            "iter:  398 train_cost:  0.6868829 train_acc:  0.765625 test_cost:  0.75159883 test_acc:  0.734375\n",
            "iter:  399 train_cost:  0.7400462 train_acc:  0.75 test_cost:  0.7679503 test_acc:  0.7265625\n",
            "iter:  400 train_cost:  0.88032424 train_acc:  0.6953125 test_cost:  0.81177706 test_acc:  0.7265625\n",
            "iter:  401 train_cost:  0.87675476 train_acc:  0.7265625 test_cost:  1.1311265 test_acc:  0.6328125\n",
            "iter:  402 train_cost:  0.627808 train_acc:  0.828125 test_cost:  0.6864002 test_acc:  0.7578125\n",
            "iter:  403 train_cost:  0.7726328 train_acc:  0.7578125 test_cost:  0.72250986 test_acc:  0.7578125\n",
            "iter:  404 train_cost:  0.7937674 train_acc:  0.75 test_cost:  0.74558663 test_acc:  0.7734375\n",
            "iter:  405 train_cost:  0.69238013 train_acc:  0.796875 test_cost:  0.7157871 test_acc:  0.7578125\n",
            "iter:  406 train_cost:  0.79693294 train_acc:  0.6875 test_cost:  0.7771175 test_acc:  0.75\n",
            "iter:  407 train_cost:  0.69328904 train_acc:  0.7734375 test_cost:  0.80681574 test_acc:  0.7578125\n",
            "iter:  408 train_cost:  0.8293805 train_acc:  0.7265625 test_cost:  0.7536919 test_acc:  0.703125\n",
            "iter:  409 train_cost:  0.87092596 train_acc:  0.71875 test_cost:  0.81009585 test_acc:  0.78125\n",
            "iter:  410 train_cost:  1.0250952 train_acc:  0.7109375 test_cost:  0.8357083 test_acc:  0.6875\n",
            "iter:  411 train_cost:  0.8165412 train_acc:  0.765625 test_cost:  0.7163366 test_acc:  0.7734375\n",
            "iter:  412 train_cost:  0.82365924 train_acc:  0.6875 test_cost:  1.0815784 test_acc:  0.7109375\n",
            "iter:  413 train_cost:  0.77441597 train_acc:  0.7421875 test_cost:  0.82122517 test_acc:  0.734375\n",
            "iter:  414 train_cost:  0.88501906 train_acc:  0.7265625 test_cost:  0.7756251 test_acc:  0.78125\n",
            "iter:  415 train_cost:  0.76508784 train_acc:  0.75 test_cost:  0.70302135 test_acc:  0.78125\n",
            "iter:  416 train_cost:  0.73040354 train_acc:  0.75 test_cost:  0.8074648 test_acc:  0.71875\n",
            "iter:  417 train_cost:  0.81508523 train_acc:  0.71875 test_cost:  0.79919434 test_acc:  0.7421875\n",
            "iter:  418 train_cost:  0.8370323 train_acc:  0.7265625 test_cost:  0.6263914 test_acc:  0.8046875\n",
            "iter:  419 train_cost:  0.6461992 train_acc:  0.8125 test_cost:  0.7684516 test_acc:  0.7265625\n",
            "iter:  420 train_cost:  0.79525936 train_acc:  0.75 test_cost:  0.73480225 test_acc:  0.7734375\n",
            "iter:  421 train_cost:  0.69758344 train_acc:  0.7578125 test_cost:  0.78705 test_acc:  0.765625\n",
            "iter:  422 train_cost:  0.757381 train_acc:  0.7734375 test_cost:  0.5535601 test_acc:  0.8125\n",
            "iter:  423 train_cost:  0.79762524 train_acc:  0.7578125 test_cost:  0.6927635 test_acc:  0.796875\n",
            "iter:  424 train_cost:  0.7729106 train_acc:  0.7109375 test_cost:  0.6877284 test_acc:  0.7578125\n",
            "iter:  425 train_cost:  0.7085542 train_acc:  0.71875 test_cost:  0.8125988 test_acc:  0.734375\n",
            "iter:  426 train_cost:  0.67440534 train_acc:  0.7265625 test_cost:  0.655035 test_acc:  0.8359375\n",
            "iter:  427 train_cost:  0.67171276 train_acc:  0.78125 test_cost:  0.7969861 test_acc:  0.7578125\n",
            "iter:  428 train_cost:  0.86479396 train_acc:  0.7109375 test_cost:  0.9762295 test_acc:  0.6796875\n",
            "iter:  429 train_cost:  0.6439977 train_acc:  0.796875 test_cost:  0.6539159 test_acc:  0.765625\n",
            "iter:  430 train_cost:  0.79149055 train_acc:  0.7421875 test_cost:  0.735217 test_acc:  0.796875\n",
            "iter:  431 train_cost:  0.72084373 train_acc:  0.765625 test_cost:  0.7016907 test_acc:  0.8125\n",
            "iter:  432 train_cost:  0.95615405 train_acc:  0.65625 test_cost:  0.8038861 test_acc:  0.7421875\n",
            "iter:  433 train_cost:  0.77423716 train_acc:  0.75 test_cost:  0.78901 test_acc:  0.7578125\n",
            "iter:  434 train_cost:  0.7725092 train_acc:  0.7578125 test_cost:  0.61354053 test_acc:  0.8046875\n",
            "iter:  435 train_cost:  0.68308383 train_acc:  0.796875 test_cost:  0.72466266 test_acc:  0.7578125\n",
            "iter:  436 train_cost:  0.7655066 train_acc:  0.7265625 test_cost:  0.8220034 test_acc:  0.765625\n",
            "iter:  437 train_cost:  0.8374779 train_acc:  0.765625 test_cost:  0.84949565 test_acc:  0.734375\n",
            "iter:  438 train_cost:  0.9137319 train_acc:  0.7421875 test_cost:  0.6528093 test_acc:  0.765625\n",
            "iter:  439 train_cost:  0.66378444 train_acc:  0.78125 test_cost:  0.68853617 test_acc:  0.78125\n",
            "iter:  440 train_cost:  0.70050275 train_acc:  0.8125 test_cost:  0.5864152 test_acc:  0.7421875\n",
            "iter:  441 train_cost:  0.6913693 train_acc:  0.8046875 test_cost:  0.79610157 test_acc:  0.75\n",
            "iter:  442 train_cost:  0.6562408 train_acc:  0.7734375 test_cost:  0.67716646 test_acc:  0.7734375\n",
            "iter:  443 train_cost:  0.6587858 train_acc:  0.796875 test_cost:  0.60165715 test_acc:  0.78125\n",
            "iter:  444 train_cost:  0.7955775 train_acc:  0.796875 test_cost:  0.6858902 test_acc:  0.7578125\n",
            "iter:  445 train_cost:  0.6377758 train_acc:  0.796875 test_cost:  0.66619384 test_acc:  0.796875\n",
            "iter:  446 train_cost:  0.733054 train_acc:  0.75 test_cost:  0.83656013 test_acc:  0.765625\n",
            "iter:  447 train_cost:  0.65279186 train_acc:  0.796875 test_cost:  0.61604726 test_acc:  0.8203125\n",
            "iter:  448 train_cost:  0.9921947 train_acc:  0.7265625 test_cost:  0.9882217 test_acc:  0.7265625\n",
            "iter:  449 train_cost:  0.7423099 train_acc:  0.7734375 test_cost:  0.64230907 test_acc:  0.78125\n",
            "iter:  450 train_cost:  0.5359963 train_acc:  0.8515625 test_cost:  0.74011827 test_acc:  0.765625\n",
            "iter:  451 train_cost:  0.73947084 train_acc:  0.7734375 test_cost:  0.73009664 test_acc:  0.765625\n",
            "iter:  452 train_cost:  0.8245464 train_acc:  0.734375 test_cost:  0.7146455 test_acc:  0.7421875\n",
            "iter:  453 train_cost:  0.66969526 train_acc:  0.765625 test_cost:  0.75580186 test_acc:  0.75\n",
            "iter:  454 train_cost:  0.6451099 train_acc:  0.8203125 test_cost:  0.8632912 test_acc:  0.7578125\n",
            "iter:  455 train_cost:  0.5396742 train_acc:  0.8515625 test_cost:  0.6980548 test_acc:  0.7734375\n",
            "iter:  456 train_cost:  0.6047934 train_acc:  0.78125 test_cost:  0.61030734 test_acc:  0.7734375\n",
            "iter:  457 train_cost:  0.6143522 train_acc:  0.8125 test_cost:  0.8499704 test_acc:  0.734375\n",
            "iter:  458 train_cost:  0.8497055 train_acc:  0.7578125 test_cost:  0.7224261 test_acc:  0.765625\n",
            "iter:  459 train_cost:  0.5689603 train_acc:  0.84375 test_cost:  0.46321446 test_acc:  0.8359375\n",
            "iter:  460 train_cost:  0.7654232 train_acc:  0.7734375 test_cost:  0.48479778 test_acc:  0.875\n",
            "iter:  461 train_cost:  0.5428869 train_acc:  0.7734375 test_cost:  0.66902655 test_acc:  0.796875\n",
            "iter:  462 train_cost:  0.66441035 train_acc:  0.7890625 test_cost:  0.687644 test_acc:  0.75\n",
            "iter:  463 train_cost:  0.8826191 train_acc:  0.7578125 test_cost:  0.7459917 test_acc:  0.765625\n",
            "iter:  464 train_cost:  0.6789817 train_acc:  0.765625 test_cost:  0.6741098 test_acc:  0.78125\n",
            "iter:  465 train_cost:  0.46354884 train_acc:  0.84375 test_cost:  0.68044543 test_acc:  0.75\n",
            "iter:  466 train_cost:  0.683507 train_acc:  0.75 test_cost:  0.92149645 test_acc:  0.71875\n",
            "iter:  467 train_cost:  0.8599663 train_acc:  0.734375 test_cost:  0.812567 test_acc:  0.734375\n",
            "iter:  468 train_cost:  0.824286 train_acc:  0.7109375 test_cost:  0.73900527 test_acc:  0.7890625\n",
            "iter:  469 train_cost:  0.76005226 train_acc:  0.7265625 test_cost:  0.7248868 test_acc:  0.78125\n",
            "iter:  470 train_cost:  0.54777694 train_acc:  0.8203125 test_cost:  0.6638417 test_acc:  0.828125\n",
            "iter:  471 train_cost:  0.77838933 train_acc:  0.7578125 test_cost:  0.75108975 test_acc:  0.765625\n",
            "iter:  472 train_cost:  0.6486656 train_acc:  0.8046875 test_cost:  0.56481445 test_acc:  0.8359375\n",
            "iter:  473 train_cost:  0.6180884 train_acc:  0.8046875 test_cost:  0.6086299 test_acc:  0.8125\n",
            "iter:  474 train_cost:  0.65089506 train_acc:  0.7734375 test_cost:  0.6924143 test_acc:  0.8046875\n",
            "iter:  475 train_cost:  0.72104096 train_acc:  0.7578125 test_cost:  0.6588576 test_acc:  0.765625\n",
            "iter:  476 train_cost:  0.75443983 train_acc:  0.75 test_cost:  0.7225844 test_acc:  0.7578125\n",
            "iter:  477 train_cost:  0.6757763 train_acc:  0.765625 test_cost:  0.50016415 test_acc:  0.8046875\n",
            "iter:  478 train_cost:  0.57302 train_acc:  0.7734375 test_cost:  0.56448627 test_acc:  0.8125\n",
            "iter:  479 train_cost:  0.5529214 train_acc:  0.8359375 test_cost:  0.74707544 test_acc:  0.78125\n",
            "iter:  480 train_cost:  0.75033206 train_acc:  0.8046875 test_cost:  0.6126261 test_acc:  0.828125\n",
            "iter:  481 train_cost:  0.8016103 train_acc:  0.796875 test_cost:  0.66845673 test_acc:  0.78125\n",
            "iter:  482 train_cost:  0.62280613 train_acc:  0.78125 test_cost:  0.8396589 test_acc:  0.734375\n",
            "iter:  483 train_cost:  0.7731247 train_acc:  0.7421875 test_cost:  0.7785946 test_acc:  0.765625\n",
            "iter:  484 train_cost:  0.6926169 train_acc:  0.796875 test_cost:  0.8226018 test_acc:  0.765625\n",
            "iter:  485 train_cost:  0.7038726 train_acc:  0.8046875 test_cost:  0.766256 test_acc:  0.7578125\n",
            "iter:  486 train_cost:  0.7094877 train_acc:  0.75 test_cost:  0.69499403 test_acc:  0.734375\n",
            "iter:  487 train_cost:  0.94647825 train_acc:  0.7734375 test_cost:  0.60894567 test_acc:  0.765625\n",
            "iter:  488 train_cost:  0.6509979 train_acc:  0.828125 test_cost:  0.511235 test_acc:  0.84375\n",
            "iter:  489 train_cost:  0.64326453 train_acc:  0.78125 test_cost:  0.809247 test_acc:  0.8046875\n",
            "iter:  490 train_cost:  0.40473038 train_acc:  0.84375 test_cost:  0.6854048 test_acc:  0.765625\n",
            "iter:  491 train_cost:  0.8141607 train_acc:  0.7109375 test_cost:  0.6587602 test_acc:  0.8046875\n",
            "iter:  492 train_cost:  0.9356921 train_acc:  0.71875 test_cost:  0.63668454 test_acc:  0.8125\n",
            "iter:  493 train_cost:  0.52928656 train_acc:  0.84375 test_cost:  0.8679261 test_acc:  0.734375\n",
            "iter:  494 train_cost:  0.7580006 train_acc:  0.7265625 test_cost:  0.58023417 test_acc:  0.7734375\n",
            "iter:  495 train_cost:  0.6754552 train_acc:  0.7890625 test_cost:  0.5551045 test_acc:  0.84375\n",
            "iter:  496 train_cost:  0.5898248 train_acc:  0.8046875 test_cost:  0.61039996 test_acc:  0.8203125\n",
            "iter:  497 train_cost:  0.72576034 train_acc:  0.765625 test_cost:  0.6151222 test_acc:  0.8046875\n",
            "iter:  498 train_cost:  0.7016035 train_acc:  0.765625 test_cost:  0.7157292 test_acc:  0.734375\n",
            "iter:  499 train_cost:  0.7566098 train_acc:  0.7578125 test_cost:  0.6925826 test_acc:  0.796875\n",
            "iter:  500 train_cost:  0.86287737 train_acc:  0.7109375 test_cost:  0.55360734 test_acc:  0.828125\n",
            "iter:  501 train_cost:  0.7066191 train_acc:  0.75 test_cost:  0.84451693 test_acc:  0.734375\n",
            "iter:  502 train_cost:  0.7363119 train_acc:  0.7578125 test_cost:  0.57746494 test_acc:  0.8203125\n",
            "iter:  503 train_cost:  0.5701568 train_acc:  0.78125 test_cost:  0.70169646 test_acc:  0.796875\n",
            "iter:  504 train_cost:  0.69900084 train_acc:  0.8203125 test_cost:  0.58691585 test_acc:  0.7890625\n",
            "iter:  505 train_cost:  0.72183394 train_acc:  0.8046875 test_cost:  0.7134856 test_acc:  0.7421875\n",
            "iter:  506 train_cost:  0.9556534 train_acc:  0.6953125 test_cost:  0.70623934 test_acc:  0.75\n",
            "iter:  507 train_cost:  0.65060836 train_acc:  0.8046875 test_cost:  0.4903535 test_acc:  0.859375\n",
            "iter:  508 train_cost:  0.7346931 train_acc:  0.8046875 test_cost:  0.6613855 test_acc:  0.765625\n",
            "iter:  509 train_cost:  0.6424937 train_acc:  0.78125 test_cost:  0.6129377 test_acc:  0.7890625\n",
            "iter:  510 train_cost:  0.58991873 train_acc:  0.8125 test_cost:  0.58023095 test_acc:  0.8046875\n",
            "iter:  511 train_cost:  0.6681979 train_acc:  0.7578125 test_cost:  0.6069142 test_acc:  0.7734375\n",
            "iter:  512 train_cost:  0.8409708 train_acc:  0.75 test_cost:  0.674134 test_acc:  0.75\n",
            "iter:  513 train_cost:  0.5982348 train_acc:  0.8203125 test_cost:  0.84264755 test_acc:  0.734375\n",
            "iter:  514 train_cost:  0.8354373 train_acc:  0.8046875 test_cost:  0.8171462 test_acc:  0.78125\n",
            "iter:  515 train_cost:  0.77058077 train_acc:  0.796875 test_cost:  0.6341493 test_acc:  0.8046875\n",
            "iter:  516 train_cost:  0.58043295 train_acc:  0.84375 test_cost:  0.53709006 test_acc:  0.828125\n",
            "iter:  517 train_cost:  0.65250874 train_acc:  0.8125 test_cost:  0.8587761 test_acc:  0.765625\n",
            "iter:  518 train_cost:  0.6972509 train_acc:  0.78125 test_cost:  0.8352623 test_acc:  0.734375\n",
            "iter:  519 train_cost:  0.45286638 train_acc:  0.84375 test_cost:  0.733958 test_acc:  0.7421875\n",
            "iter:  520 train_cost:  0.6541511 train_acc:  0.8125 test_cost:  0.5727352 test_acc:  0.8203125\n",
            "iter:  521 train_cost:  0.78337276 train_acc:  0.7890625 test_cost:  0.73612386 test_acc:  0.75\n",
            "iter:  522 train_cost:  0.6182019 train_acc:  0.796875 test_cost:  0.67000437 test_acc:  0.796875\n",
            "iter:  523 train_cost:  0.5993532 train_acc:  0.8359375 test_cost:  0.83246 test_acc:  0.78125\n",
            "iter:  524 train_cost:  0.8304535 train_acc:  0.765625 test_cost:  0.6023725 test_acc:  0.828125\n",
            "iter:  525 train_cost:  0.5445342 train_acc:  0.8671875 test_cost:  0.6258188 test_acc:  0.828125\n",
            "iter:  526 train_cost:  0.69583297 train_acc:  0.7734375 test_cost:  0.68961626 test_acc:  0.78125\n",
            "iter:  527 train_cost:  0.6050235 train_acc:  0.8046875 test_cost:  0.66058826 test_acc:  0.8046875\n",
            "iter:  528 train_cost:  0.63150245 train_acc:  0.796875 test_cost:  0.78597033 test_acc:  0.75\n",
            "iter:  529 train_cost:  0.6899638 train_acc:  0.8203125 test_cost:  0.77830756 test_acc:  0.75\n",
            "iter:  530 train_cost:  0.881258 train_acc:  0.7265625 test_cost:  0.6612568 test_acc:  0.78125\n",
            "iter:  531 train_cost:  0.58805346 train_acc:  0.7734375 test_cost:  0.6705408 test_acc:  0.7890625\n",
            "iter:  532 train_cost:  0.5811506 train_acc:  0.78125 test_cost:  0.6835102 test_acc:  0.796875\n",
            "iter:  533 train_cost:  0.71879655 train_acc:  0.7265625 test_cost:  0.7234547 test_acc:  0.7734375\n",
            "iter:  534 train_cost:  0.76707697 train_acc:  0.78125 test_cost:  0.58263206 test_acc:  0.8203125\n",
            "iter:  535 train_cost:  0.6388298 train_acc:  0.7734375 test_cost:  0.6815258 test_acc:  0.7890625\n",
            "iter:  536 train_cost:  0.646201 train_acc:  0.8125 test_cost:  0.6658655 test_acc:  0.7890625\n",
            "iter:  537 train_cost:  0.50779915 train_acc:  0.8046875 test_cost:  0.62846136 test_acc:  0.78125\n",
            "iter:  538 train_cost:  0.5861896 train_acc:  0.8046875 test_cost:  0.77111006 test_acc:  0.7421875\n",
            "iter:  539 train_cost:  0.6022299 train_acc:  0.8203125 test_cost:  0.552434 test_acc:  0.828125\n",
            "iter:  540 train_cost:  0.6141852 train_acc:  0.8359375 test_cost:  0.6502663 test_acc:  0.78125\n",
            "iter:  541 train_cost:  0.5915627 train_acc:  0.796875 test_cost:  0.6837674 test_acc:  0.7734375\n",
            "iter:  542 train_cost:  0.56619465 train_acc:  0.8359375 test_cost:  0.56463337 test_acc:  0.8046875\n",
            "iter:  543 train_cost:  0.6598023 train_acc:  0.765625 test_cost:  0.5480236 test_acc:  0.8515625\n",
            "iter:  544 train_cost:  0.61876285 train_acc:  0.8046875 test_cost:  0.6032717 test_acc:  0.828125\n",
            "iter:  545 train_cost:  0.5664017 train_acc:  0.8125 test_cost:  0.79277205 test_acc:  0.734375\n",
            "iter:  546 train_cost:  0.7545693 train_acc:  0.75 test_cost:  0.6162775 test_acc:  0.8125\n",
            "iter:  547 train_cost:  0.5702156 train_acc:  0.7890625 test_cost:  0.7010332 test_acc:  0.8046875\n",
            "iter:  548 train_cost:  0.7411692 train_acc:  0.7265625 test_cost:  0.7281954 test_acc:  0.78125\n",
            "iter:  549 train_cost:  0.5799943 train_acc:  0.8046875 test_cost:  0.59359854 test_acc:  0.8125\n",
            "iter:  550 train_cost:  0.69541544 train_acc:  0.8046875 test_cost:  0.5509351 test_acc:  0.8125\n",
            "iter:  551 train_cost:  0.73082465 train_acc:  0.75 test_cost:  0.669011 test_acc:  0.78125\n",
            "iter:  552 train_cost:  0.6752314 train_acc:  0.7578125 test_cost:  0.61137986 test_acc:  0.8515625\n",
            "iter:  553 train_cost:  0.46466592 train_acc:  0.8515625 test_cost:  0.47836453 test_acc:  0.8359375\n",
            "iter:  554 train_cost:  0.67826307 train_acc:  0.7890625 test_cost:  0.6188153 test_acc:  0.6953125\n",
            "iter:  555 train_cost:  0.80680716 train_acc:  0.734375 test_cost:  0.60144323 test_acc:  0.7890625\n",
            "iter:  556 train_cost:  0.73161316 train_acc:  0.75 test_cost:  0.5637413 test_acc:  0.8203125\n",
            "iter:  557 train_cost:  0.4957042 train_acc:  0.8125 test_cost:  0.5436385 test_acc:  0.796875\n",
            "iter:  558 train_cost:  0.4177021 train_acc:  0.890625 test_cost:  0.78492033 test_acc:  0.7578125\n",
            "iter:  559 train_cost:  0.62554675 train_acc:  0.8203125 test_cost:  0.5633354 test_acc:  0.796875\n",
            "iter:  560 train_cost:  0.7632422 train_acc:  0.796875 test_cost:  0.52463865 test_acc:  0.828125\n",
            "iter:  561 train_cost:  0.7386024 train_acc:  0.765625 test_cost:  0.83323085 test_acc:  0.71875\n",
            "iter:  562 train_cost:  0.6126183 train_acc:  0.828125 test_cost:  0.63685787 test_acc:  0.8203125\n",
            "iter:  563 train_cost:  0.81981397 train_acc:  0.7109375 test_cost:  0.5842924 test_acc:  0.859375\n",
            "iter:  564 train_cost:  0.5682003 train_acc:  0.8046875 test_cost:  0.65242994 test_acc:  0.8046875\n",
            "iter:  565 train_cost:  0.5611524 train_acc:  0.8203125 test_cost:  0.47920078 test_acc:  0.890625\n",
            "iter:  566 train_cost:  0.6131927 train_acc:  0.8203125 test_cost:  0.6297934 test_acc:  0.78125\n",
            "iter:  567 train_cost:  0.51854956 train_acc:  0.828125 test_cost:  0.63581955 test_acc:  0.8046875\n",
            "iter:  568 train_cost:  0.53091097 train_acc:  0.8515625 test_cost:  0.57043976 test_acc:  0.8125\n",
            "iter:  569 train_cost:  0.622517 train_acc:  0.8203125 test_cost:  0.8602357 test_acc:  0.734375\n",
            "iter:  570 train_cost:  0.57699376 train_acc:  0.8203125 test_cost:  0.5774851 test_acc:  0.8203125\n",
            "iter:  571 train_cost:  0.6872878 train_acc:  0.78125 test_cost:  0.5460345 test_acc:  0.8203125\n",
            "iter:  572 train_cost:  0.58828795 train_acc:  0.828125 test_cost:  0.7046639 test_acc:  0.7578125\n",
            "iter:  573 train_cost:  0.5758548 train_acc:  0.796875 test_cost:  0.5164689 test_acc:  0.8203125\n",
            "iter:  574 train_cost:  0.46493676 train_acc:  0.84375 test_cost:  0.74172527 test_acc:  0.7734375\n",
            "iter:  575 train_cost:  0.53302425 train_acc:  0.84375 test_cost:  0.6948562 test_acc:  0.78125\n",
            "iter:  576 train_cost:  0.4911483 train_acc:  0.8203125 test_cost:  0.6249877 test_acc:  0.8203125\n",
            "iter:  577 train_cost:  0.7289003 train_acc:  0.7890625 test_cost:  0.4965824 test_acc:  0.8359375\n",
            "iter:  578 train_cost:  0.9063424 train_acc:  0.7265625 test_cost:  0.585168 test_acc:  0.828125\n",
            "iter:  579 train_cost:  0.8455961 train_acc:  0.71875 test_cost:  0.56586385 test_acc:  0.8359375\n",
            "iter:  580 train_cost:  0.5820356 train_acc:  0.796875 test_cost:  0.74229604 test_acc:  0.8125\n",
            "iter:  581 train_cost:  0.60509646 train_acc:  0.8359375 test_cost:  0.6867188 test_acc:  0.7890625\n",
            "iter:  582 train_cost:  0.69677234 train_acc:  0.7890625 test_cost:  0.54297835 test_acc:  0.8125\n",
            "iter:  583 train_cost:  0.59983397 train_acc:  0.8203125 test_cost:  0.7494744 test_acc:  0.8203125\n",
            "iter:  584 train_cost:  0.6650257 train_acc:  0.7890625 test_cost:  0.5524442 test_acc:  0.8125\n",
            "iter:  585 train_cost:  0.49853778 train_acc:  0.875 test_cost:  0.5714937 test_acc:  0.8203125\n",
            "iter:  586 train_cost:  0.6232544 train_acc:  0.8046875 test_cost:  0.6818179 test_acc:  0.7734375\n",
            "iter:  587 train_cost:  0.55856574 train_acc:  0.796875 test_cost:  0.52712184 test_acc:  0.8359375\n",
            "iter:  588 train_cost:  0.6096988 train_acc:  0.765625 test_cost:  0.5405965 test_acc:  0.875\n",
            "iter:  589 train_cost:  0.7043848 train_acc:  0.8125 test_cost:  0.75661683 test_acc:  0.8125\n",
            "iter:  590 train_cost:  0.6086799 train_acc:  0.828125 test_cost:  0.6832143 test_acc:  0.75\n",
            "iter:  591 train_cost:  0.4009473 train_acc:  0.859375 test_cost:  0.5563899 test_acc:  0.7890625\n",
            "iter:  592 train_cost:  0.80962586 train_acc:  0.7578125 test_cost:  0.58592117 test_acc:  0.8125\n",
            "iter:  593 train_cost:  0.6565105 train_acc:  0.8125 test_cost:  0.5910357 test_acc:  0.78125\n",
            "iter:  594 train_cost:  0.69088984 train_acc:  0.7890625 test_cost:  0.537042 test_acc:  0.8125\n",
            "iter:  595 train_cost:  0.68032587 train_acc:  0.8203125 test_cost:  0.7990173 test_acc:  0.71875\n",
            "iter:  596 train_cost:  0.61180687 train_acc:  0.8125 test_cost:  0.6512768 test_acc:  0.796875\n",
            "iter:  597 train_cost:  0.4600324 train_acc:  0.8515625 test_cost:  0.6321261 test_acc:  0.796875\n",
            "iter:  598 train_cost:  0.6766859 train_acc:  0.78125 test_cost:  0.55930454 test_acc:  0.84375\n",
            "iter:  599 train_cost:  0.5462017 train_acc:  0.7890625 test_cost:  0.6231053 test_acc:  0.78125\n",
            "iter:  600 train_cost:  0.55104387 train_acc:  0.828125 test_cost:  0.6980252 test_acc:  0.78125\n",
            "iter:  601 train_cost:  0.51913893 train_acc:  0.8359375 test_cost:  0.567308 test_acc:  0.796875\n",
            "iter:  602 train_cost:  0.49025905 train_acc:  0.828125 test_cost:  0.6205307 test_acc:  0.78125\n",
            "iter:  603 train_cost:  0.7412334 train_acc:  0.765625 test_cost:  0.5843857 test_acc:  0.828125\n",
            "iter:  604 train_cost:  0.5509645 train_acc:  0.8203125 test_cost:  0.6592066 test_acc:  0.796875\n",
            "iter:  605 train_cost:  0.61122894 train_acc:  0.8125 test_cost:  0.67649204 test_acc:  0.796875\n",
            "iter:  606 train_cost:  0.5688539 train_acc:  0.8046875 test_cost:  0.67489403 test_acc:  0.7890625\n",
            "iter:  607 train_cost:  0.54008764 train_acc:  0.828125 test_cost:  0.704553 test_acc:  0.7578125\n",
            "iter:  608 train_cost:  0.44534975 train_acc:  0.8359375 test_cost:  0.6447747 test_acc:  0.8125\n",
            "iter:  609 train_cost:  0.568198 train_acc:  0.828125 test_cost:  0.6820498 test_acc:  0.78125\n",
            "iter:  610 train_cost:  0.54462683 train_acc:  0.828125 test_cost:  0.6048635 test_acc:  0.7734375\n",
            "iter:  611 train_cost:  0.623598 train_acc:  0.828125 test_cost:  0.5807942 test_acc:  0.828125\n",
            "iter:  612 train_cost:  0.55899155 train_acc:  0.828125 test_cost:  0.5530329 test_acc:  0.8203125\n",
            "iter:  613 train_cost:  0.7042129 train_acc:  0.796875 test_cost:  0.40859365 test_acc:  0.890625\n",
            "iter:  614 train_cost:  0.57519734 train_acc:  0.7890625 test_cost:  0.64240795 test_acc:  0.8203125\n",
            "iter:  615 train_cost:  0.605237 train_acc:  0.859375 test_cost:  0.7356896 test_acc:  0.8125\n",
            "iter:  616 train_cost:  0.48419863 train_acc:  0.859375 test_cost:  0.6428634 test_acc:  0.8125\n",
            "iter:  617 train_cost:  0.73669094 train_acc:  0.7578125 test_cost:  0.52131385 test_acc:  0.8203125\n",
            "iter:  618 train_cost:  0.6821426 train_acc:  0.8125 test_cost:  0.6274441 test_acc:  0.8203125\n",
            "iter:  619 train_cost:  0.630882 train_acc:  0.8125 test_cost:  0.48413473 test_acc:  0.828125\n",
            "iter:  620 train_cost:  0.5696229 train_acc:  0.8359375 test_cost:  0.5441895 test_acc:  0.8203125\n",
            "iter:  621 train_cost:  0.5962168 train_acc:  0.8125 test_cost:  0.5693351 test_acc:  0.7734375\n",
            "iter:  622 train_cost:  0.6477872 train_acc:  0.796875 test_cost:  0.73031163 test_acc:  0.7421875\n",
            "iter:  623 train_cost:  0.544255 train_acc:  0.859375 test_cost:  0.48303208 test_acc:  0.8203125\n",
            "iter:  624 train_cost:  0.56632113 train_acc:  0.828125 test_cost:  0.7243115 test_acc:  0.7421875\n",
            "iter:  625 train_cost:  0.7229826 train_acc:  0.7578125 test_cost:  0.5017827 test_acc:  0.8515625\n",
            "iter:  626 train_cost:  0.5690024 train_acc:  0.7890625 test_cost:  0.5673452 test_acc:  0.828125\n",
            "iter:  627 train_cost:  0.6452725 train_acc:  0.7890625 test_cost:  0.6971631 test_acc:  0.78125\n",
            "iter:  628 train_cost:  0.6767123 train_acc:  0.765625 test_cost:  0.57723624 test_acc:  0.84375\n",
            "iter:  629 train_cost:  0.62147677 train_acc:  0.78125 test_cost:  0.4783841 test_acc:  0.84375\n",
            "iter:  630 train_cost:  0.5553751 train_acc:  0.78125 test_cost:  0.7300479 test_acc:  0.78125\n",
            "iter:  631 train_cost:  0.4986474 train_acc:  0.8515625 test_cost:  0.65961564 test_acc:  0.8203125\n",
            "iter:  632 train_cost:  0.49410993 train_acc:  0.8359375 test_cost:  0.6425843 test_acc:  0.796875\n",
            "iter:  633 train_cost:  0.64209336 train_acc:  0.7734375 test_cost:  0.68672764 test_acc:  0.7890625\n",
            "iter:  634 train_cost:  0.62855756 train_acc:  0.78125 test_cost:  0.5543455 test_acc:  0.8515625\n",
            "iter:  635 train_cost:  0.6281801 train_acc:  0.828125 test_cost:  0.4044294 test_acc:  0.8984375\n",
            "iter:  636 train_cost:  0.69219685 train_acc:  0.75 test_cost:  0.45853737 test_acc:  0.875\n",
            "iter:  637 train_cost:  0.72869396 train_acc:  0.8046875 test_cost:  0.53148437 test_acc:  0.84375\n",
            "iter:  638 train_cost:  0.6243171 train_acc:  0.796875 test_cost:  0.6334934 test_acc:  0.8125\n",
            "iter:  639 train_cost:  0.61006016 train_acc:  0.796875 test_cost:  0.46442252 test_acc:  0.8828125\n",
            "iter:  640 train_cost:  0.5308088 train_acc:  0.8359375 test_cost:  0.649813 test_acc:  0.796875\n",
            "iter:  641 train_cost:  0.5900557 train_acc:  0.828125 test_cost:  0.497797 test_acc:  0.8203125\n",
            "iter:  642 train_cost:  0.6657926 train_acc:  0.828125 test_cost:  0.694731 test_acc:  0.7421875\n",
            "iter:  643 train_cost:  0.6064304 train_acc:  0.84375 test_cost:  0.5272225 test_acc:  0.828125\n",
            "iter:  644 train_cost:  0.52942955 train_acc:  0.8359375 test_cost:  0.6355565 test_acc:  0.8125\n",
            "iter:  645 train_cost:  0.5726449 train_acc:  0.8046875 test_cost:  0.65852547 test_acc:  0.7734375\n",
            "iter:  646 train_cost:  0.65167785 train_acc:  0.8125 test_cost:  0.568557 test_acc:  0.84375\n",
            "iter:  647 train_cost:  0.63733447 train_acc:  0.8359375 test_cost:  0.52787715 test_acc:  0.8359375\n",
            "iter:  648 train_cost:  0.65792584 train_acc:  0.7578125 test_cost:  0.4782497 test_acc:  0.8359375\n",
            "iter:  649 train_cost:  0.5487232 train_acc:  0.8515625 test_cost:  0.56158817 test_acc:  0.828125\n",
            "iter:  650 train_cost:  0.4265404 train_acc:  0.859375 test_cost:  0.52507114 test_acc:  0.8515625\n",
            "iter:  651 train_cost:  0.62671435 train_acc:  0.8203125 test_cost:  0.53975475 test_acc:  0.8515625\n",
            "iter:  652 train_cost:  0.57297623 train_acc:  0.8125 test_cost:  0.7642658 test_acc:  0.796875\n",
            "iter:  653 train_cost:  0.51169354 train_acc:  0.828125 test_cost:  0.70835495 test_acc:  0.78125\n",
            "iter:  654 train_cost:  0.5305146 train_acc:  0.8203125 test_cost:  0.6946744 test_acc:  0.71875\n",
            "iter:  655 train_cost:  0.66377115 train_acc:  0.8046875 test_cost:  0.5657567 test_acc:  0.8046875\n",
            "iter:  656 train_cost:  0.5767748 train_acc:  0.8125 test_cost:  0.5771102 test_acc:  0.8203125\n",
            "iter:  657 train_cost:  0.5534749 train_acc:  0.8046875 test_cost:  0.60674393 test_acc:  0.828125\n",
            "iter:  658 train_cost:  0.585876 train_acc:  0.796875 test_cost:  0.6136898 test_acc:  0.8125\n",
            "iter:  659 train_cost:  0.65141225 train_acc:  0.8125 test_cost:  0.64103955 test_acc:  0.7890625\n",
            "iter:  660 train_cost:  0.5658447 train_acc:  0.7890625 test_cost:  0.3425864 test_acc:  0.9140625\n",
            "iter:  661 train_cost:  0.54896533 train_acc:  0.8203125 test_cost:  0.46015245 test_acc:  0.84375\n",
            "iter:  662 train_cost:  0.52797043 train_acc:  0.8515625 test_cost:  0.6278567 test_acc:  0.8203125\n",
            "iter:  663 train_cost:  0.6901524 train_acc:  0.78125 test_cost:  0.4925905 test_acc:  0.8125\n",
            "iter:  664 train_cost:  0.61613476 train_acc:  0.796875 test_cost:  0.6227629 test_acc:  0.796875\n",
            "iter:  665 train_cost:  0.45532453 train_acc:  0.8515625 test_cost:  0.51882243 test_acc:  0.84375\n",
            "iter:  666 train_cost:  0.5398158 train_acc:  0.796875 test_cost:  0.61073357 test_acc:  0.796875\n",
            "iter:  667 train_cost:  0.6063553 train_acc:  0.7890625 test_cost:  0.48761326 test_acc:  0.8359375\n",
            "iter:  668 train_cost:  0.6574576 train_acc:  0.78125 test_cost:  0.4886427 test_acc:  0.8515625\n",
            "iter:  669 train_cost:  0.38395846 train_acc:  0.8984375 test_cost:  0.44577807 test_acc:  0.8671875\n",
            "iter:  670 train_cost:  0.764668 train_acc:  0.7734375 test_cost:  0.5561147 test_acc:  0.859375\n",
            "iter:  671 train_cost:  0.4303159 train_acc:  0.84375 test_cost:  0.5571486 test_acc:  0.796875\n",
            "iter:  672 train_cost:  0.53922343 train_acc:  0.8359375 test_cost:  0.64546907 test_acc:  0.7890625\n",
            "iter:  673 train_cost:  0.5846971 train_acc:  0.859375 test_cost:  0.6493869 test_acc:  0.765625\n",
            "iter:  674 train_cost:  0.59870243 train_acc:  0.828125 test_cost:  0.705108 test_acc:  0.7734375\n",
            "iter:  675 train_cost:  0.6051012 train_acc:  0.78125 test_cost:  0.6516857 test_acc:  0.75\n",
            "iter:  676 train_cost:  0.5471217 train_acc:  0.8203125 test_cost:  0.47922853 test_acc:  0.8515625\n",
            "iter:  677 train_cost:  0.76283044 train_acc:  0.7578125 test_cost:  0.46734664 test_acc:  0.8359375\n",
            "iter:  678 train_cost:  0.48352218 train_acc:  0.84375 test_cost:  0.58126837 test_acc:  0.8359375\n",
            "iter:  679 train_cost:  0.53370565 train_acc:  0.8359375 test_cost:  0.63548917 test_acc:  0.8125\n",
            "iter:  680 train_cost:  0.51497424 train_acc:  0.828125 test_cost:  0.8127677 test_acc:  0.7890625\n",
            "iter:  681 train_cost:  0.48671538 train_acc:  0.859375 test_cost:  0.6576357 test_acc:  0.8203125\n",
            "iter:  682 train_cost:  0.5032101 train_acc:  0.859375 test_cost:  0.5908308 test_acc:  0.8046875\n",
            "iter:  683 train_cost:  0.80187297 train_acc:  0.7890625 test_cost:  0.62089914 test_acc:  0.84375\n",
            "iter:  684 train_cost:  0.499422 train_acc:  0.8359375 test_cost:  0.59425557 test_acc:  0.796875\n",
            "iter:  685 train_cost:  0.59082454 train_acc:  0.8046875 test_cost:  0.4637336 test_acc:  0.875\n",
            "iter:  686 train_cost:  0.6964227 train_acc:  0.8046875 test_cost:  0.5116136 test_acc:  0.828125\n",
            "iter:  687 train_cost:  0.5914595 train_acc:  0.796875 test_cost:  0.6885698 test_acc:  0.7890625\n",
            "iter:  688 train_cost:  0.812381 train_acc:  0.7421875 test_cost:  0.5528741 test_acc:  0.7890625\n",
            "iter:  689 train_cost:  0.45034307 train_acc:  0.875 test_cost:  0.7088915 test_acc:  0.7578125\n",
            "iter:  690 train_cost:  0.57171035 train_acc:  0.828125 test_cost:  0.4511149 test_acc:  0.8125\n",
            "iter:  691 train_cost:  0.4239365 train_acc:  0.8828125 test_cost:  0.7604636 test_acc:  0.7734375\n",
            "iter:  692 train_cost:  0.49556345 train_acc:  0.8515625 test_cost:  0.45472357 test_acc:  0.8671875\n",
            "iter:  693 train_cost:  0.43581915 train_acc:  0.8515625 test_cost:  0.44160923 test_acc:  0.890625\n",
            "iter:  694 train_cost:  0.6490637 train_acc:  0.8203125 test_cost:  0.62174374 test_acc:  0.8359375\n",
            "iter:  695 train_cost:  0.6986579 train_acc:  0.7890625 test_cost:  0.475384 test_acc:  0.8359375\n",
            "iter:  696 train_cost:  0.6116245 train_acc:  0.8046875 test_cost:  0.5399149 test_acc:  0.8515625\n",
            "iter:  697 train_cost:  0.5766839 train_acc:  0.8515625 test_cost:  0.5204773 test_acc:  0.84375\n",
            "iter:  698 train_cost:  0.5014187 train_acc:  0.859375 test_cost:  0.70227695 test_acc:  0.765625\n",
            "iter:  699 train_cost:  0.6668373 train_acc:  0.78125 test_cost:  0.35371548 test_acc:  0.8984375\n",
            "iter:  700 train_cost:  0.698459 train_acc:  0.7890625 test_cost:  0.36166131 test_acc:  0.8828125\n",
            "iter:  701 train_cost:  0.6724261 train_acc:  0.78125 test_cost:  0.5980643 test_acc:  0.765625\n",
            "iter:  702 train_cost:  0.60577726 train_acc:  0.828125 test_cost:  0.64184594 test_acc:  0.78125\n",
            "iter:  703 train_cost:  0.5683291 train_acc:  0.8203125 test_cost:  0.5736066 test_acc:  0.828125\n",
            "iter:  704 train_cost:  0.5390154 train_acc:  0.8359375 test_cost:  0.5152453 test_acc:  0.84375\n",
            "iter:  705 train_cost:  0.38576162 train_acc:  0.890625 test_cost:  0.57353497 test_acc:  0.8203125\n",
            "iter:  706 train_cost:  0.6188549 train_acc:  0.8125 test_cost:  0.47541085 test_acc:  0.8359375\n",
            "iter:  707 train_cost:  0.6003728 train_acc:  0.859375 test_cost:  0.65285707 test_acc:  0.796875\n",
            "iter:  708 train_cost:  0.49418622 train_acc:  0.859375 test_cost:  0.6041617 test_acc:  0.8359375\n",
            "iter:  709 train_cost:  0.45887992 train_acc:  0.8671875 test_cost:  0.53294545 test_acc:  0.84375\n",
            "iter:  710 train_cost:  0.60891503 train_acc:  0.859375 test_cost:  0.706086 test_acc:  0.828125\n",
            "iter:  711 train_cost:  0.4747124 train_acc:  0.8515625 test_cost:  0.57679546 test_acc:  0.796875\n",
            "iter:  712 train_cost:  0.6058082 train_acc:  0.8359375 test_cost:  0.622218 test_acc:  0.796875\n",
            "iter:  713 train_cost:  0.64691436 train_acc:  0.78125 test_cost:  0.4973948 test_acc:  0.8515625\n",
            "iter:  714 train_cost:  0.4493593 train_acc:  0.859375 test_cost:  0.5265037 test_acc:  0.859375\n",
            "iter:  715 train_cost:  0.6087342 train_acc:  0.7578125 test_cost:  0.51302326 test_acc:  0.8125\n",
            "iter:  716 train_cost:  0.66090906 train_acc:  0.8125 test_cost:  0.37644014 test_acc:  0.8984375\n",
            "iter:  717 train_cost:  0.5449042 train_acc:  0.8203125 test_cost:  0.5100148 test_acc:  0.84375\n",
            "iter:  718 train_cost:  0.68942803 train_acc:  0.78125 test_cost:  0.53880835 test_acc:  0.8359375\n",
            "iter:  719 train_cost:  0.62345135 train_acc:  0.796875 test_cost:  0.59559774 test_acc:  0.8046875\n",
            "iter:  720 train_cost:  0.64416015 train_acc:  0.828125 test_cost:  0.57398766 test_acc:  0.8203125\n",
            "iter:  721 train_cost:  0.6171028 train_acc:  0.8046875 test_cost:  0.5982722 test_acc:  0.8203125\n",
            "iter:  722 train_cost:  0.40329552 train_acc:  0.859375 test_cost:  0.576502 test_acc:  0.8125\n",
            "iter:  723 train_cost:  0.70568264 train_acc:  0.796875 test_cost:  0.5213317 test_acc:  0.859375\n",
            "iter:  724 train_cost:  0.60053724 train_acc:  0.7890625 test_cost:  0.71185577 test_acc:  0.828125\n",
            "iter:  725 train_cost:  0.6872016 train_acc:  0.7890625 test_cost:  0.6389464 test_acc:  0.8125\n",
            "iter:  726 train_cost:  0.39853323 train_acc:  0.875 test_cost:  0.42981398 test_acc:  0.828125\n",
            "iter:  727 train_cost:  0.66962326 train_acc:  0.7890625 test_cost:  0.5195534 test_acc:  0.8125\n",
            "iter:  728 train_cost:  0.4815886 train_acc:  0.859375 test_cost:  0.6235542 test_acc:  0.796875\n",
            "iter:  729 train_cost:  0.6184015 train_acc:  0.8046875 test_cost:  0.65976155 test_acc:  0.8046875\n",
            "iter:  730 train_cost:  0.6475954 train_acc:  0.8125 test_cost:  0.5603182 test_acc:  0.828125\n",
            "iter:  731 train_cost:  0.52476764 train_acc:  0.828125 test_cost:  0.60586286 test_acc:  0.8125\n",
            "iter:  732 train_cost:  0.5539224 train_acc:  0.8359375 test_cost:  0.5485002 test_acc:  0.828125\n",
            "iter:  733 train_cost:  0.5660985 train_acc:  0.8203125 test_cost:  0.52561027 test_acc:  0.828125\n",
            "iter:  734 train_cost:  0.47632945 train_acc:  0.84375 test_cost:  0.560871 test_acc:  0.8046875\n",
            "iter:  735 train_cost:  0.360452 train_acc:  0.8984375 test_cost:  0.49292564 test_acc:  0.859375\n",
            "iter:  736 train_cost:  0.4451372 train_acc:  0.8515625 test_cost:  0.5862156 test_acc:  0.796875\n",
            "iter:  737 train_cost:  0.5871778 train_acc:  0.78125 test_cost:  0.7368655 test_acc:  0.7890625\n",
            "iter:  738 train_cost:  0.5639312 train_acc:  0.8203125 test_cost:  0.6664619 test_acc:  0.8046875\n",
            "iter:  739 train_cost:  0.6584851 train_acc:  0.796875 test_cost:  0.56709874 test_acc:  0.8125\n",
            "iter:  740 train_cost:  0.6022969 train_acc:  0.8046875 test_cost:  0.46323144 test_acc:  0.8515625\n",
            "iter:  741 train_cost:  0.7332276 train_acc:  0.78125 test_cost:  0.55541205 test_acc:  0.8125\n",
            "iter:  742 train_cost:  0.658829 train_acc:  0.7734375 test_cost:  0.7618898 test_acc:  0.78125\n",
            "iter:  743 train_cost:  0.40080345 train_acc:  0.8671875 test_cost:  0.46496052 test_acc:  0.828125\n",
            "iter:  744 train_cost:  0.633404 train_acc:  0.8046875 test_cost:  0.5728735 test_acc:  0.84375\n",
            "iter:  745 train_cost:  0.5929282 train_acc:  0.8359375 test_cost:  0.5399643 test_acc:  0.8203125\n",
            "iter:  746 train_cost:  0.4936749 train_acc:  0.875 test_cost:  0.5093172 test_acc:  0.8515625\n",
            "iter:  747 train_cost:  0.5005388 train_acc:  0.8125 test_cost:  0.43542057 test_acc:  0.890625\n",
            "iter:  748 train_cost:  0.573627 train_acc:  0.8125 test_cost:  0.37118745 test_acc:  0.859375\n",
            "iter:  749 train_cost:  0.5032836 train_acc:  0.828125 test_cost:  0.44941393 test_acc:  0.8359375\n",
            "iter:  750 train_cost:  0.62231076 train_acc:  0.8125 test_cost:  0.5992796 test_acc:  0.8046875\n",
            "iter:  751 train_cost:  0.3696533 train_acc:  0.8515625 test_cost:  0.4499956 test_acc:  0.859375\n",
            "iter:  752 train_cost:  0.5621834 train_acc:  0.828125 test_cost:  0.5960265 test_acc:  0.796875\n",
            "iter:  753 train_cost:  0.4774951 train_acc:  0.8515625 test_cost:  0.6103234 test_acc:  0.8125\n",
            "iter:  754 train_cost:  0.4831438 train_acc:  0.8671875 test_cost:  0.66625214 test_acc:  0.78125\n",
            "iter:  755 train_cost:  0.56976336 train_acc:  0.828125 test_cost:  0.6639669 test_acc:  0.828125\n",
            "iter:  756 train_cost:  0.67075807 train_acc:  0.7734375 test_cost:  0.5329541 test_acc:  0.8359375\n",
            "iter:  757 train_cost:  0.5346196 train_acc:  0.859375 test_cost:  0.5199745 test_acc:  0.8203125\n",
            "iter:  758 train_cost:  0.6563407 train_acc:  0.84375 test_cost:  0.3666582 test_acc:  0.875\n",
            "iter:  759 train_cost:  0.66966194 train_acc:  0.78125 test_cost:  0.54317 test_acc:  0.828125\n",
            "iter:  760 train_cost:  0.5743827 train_acc:  0.8125 test_cost:  0.3655424 test_acc:  0.8828125\n",
            "iter:  761 train_cost:  0.5213375 train_acc:  0.84375 test_cost:  0.4075024 test_acc:  0.875\n",
            "iter:  762 train_cost:  0.50861704 train_acc:  0.8359375 test_cost:  0.6450412 test_acc:  0.765625\n",
            "iter:  763 train_cost:  0.6589209 train_acc:  0.78125 test_cost:  0.5286073 test_acc:  0.8125\n",
            "iter:  764 train_cost:  0.6546115 train_acc:  0.7890625 test_cost:  0.46795294 test_acc:  0.875\n",
            "iter:  765 train_cost:  0.6021385 train_acc:  0.828125 test_cost:  0.50723755 test_acc:  0.8515625\n",
            "iter:  766 train_cost:  0.44993713 train_acc:  0.8515625 test_cost:  0.5725206 test_acc:  0.828125\n",
            "iter:  767 train_cost:  0.5458102 train_acc:  0.8125 test_cost:  0.43542087 test_acc:  0.875\n",
            "iter:  768 train_cost:  0.61167854 train_acc:  0.8125 test_cost:  0.47337466 test_acc:  0.8515625\n",
            "iter:  769 train_cost:  0.6432519 train_acc:  0.8515625 test_cost:  0.4563942 test_acc:  0.859375\n",
            "iter:  770 train_cost:  0.62846303 train_acc:  0.796875 test_cost:  0.39147946 test_acc:  0.875\n",
            "iter:  771 train_cost:  0.6822439 train_acc:  0.8203125 test_cost:  0.39877218 test_acc:  0.890625\n",
            "iter:  772 train_cost:  0.47988874 train_acc:  0.8359375 test_cost:  0.4463107 test_acc:  0.84375\n",
            "iter:  773 train_cost:  0.43235373 train_acc:  0.8984375 test_cost:  0.43559825 test_acc:  0.8203125\n",
            "iter:  774 train_cost:  0.5533509 train_acc:  0.84375 test_cost:  0.42117593 test_acc:  0.828125\n",
            "iter:  775 train_cost:  0.62677 train_acc:  0.75 test_cost:  0.51856303 test_acc:  0.828125\n",
            "iter:  776 train_cost:  0.56414163 train_acc:  0.828125 test_cost:  0.5900591 test_acc:  0.859375\n",
            "iter:  777 train_cost:  0.64049053 train_acc:  0.75 test_cost:  0.6513498 test_acc:  0.8046875\n",
            "iter:  778 train_cost:  0.5874349 train_acc:  0.8125 test_cost:  0.3888656 test_acc:  0.8515625\n",
            "iter:  779 train_cost:  0.51863265 train_acc:  0.8359375 test_cost:  0.6264161 test_acc:  0.7734375\n",
            "iter:  780 train_cost:  0.6646993 train_acc:  0.796875 test_cost:  0.617208 test_acc:  0.8203125\n",
            "iter:  781 train_cost:  0.6186132 train_acc:  0.828125 test_cost:  0.48589158 test_acc:  0.8515625\n",
            "iter:  782 train_cost:  0.49008873 train_acc:  0.8671875 test_cost:  0.5621768 test_acc:  0.84375\n",
            "iter:  783 train_cost:  0.5153197 train_acc:  0.8671875 test_cost:  0.30826795 test_acc:  0.8984375\n",
            "iter:  784 train_cost:  0.4446569 train_acc:  0.8203125 test_cost:  0.4052829 test_acc:  0.875\n",
            "iter:  785 train_cost:  0.48407662 train_acc:  0.859375 test_cost:  0.5014977 test_acc:  0.828125\n",
            "iter:  786 train_cost:  0.6602493 train_acc:  0.8046875 test_cost:  0.6373919 test_acc:  0.8203125\n",
            "iter:  787 train_cost:  0.7808522 train_acc:  0.765625 test_cost:  0.43533444 test_acc:  0.8671875\n",
            "iter:  788 train_cost:  0.5510677 train_acc:  0.84375 test_cost:  0.53992105 test_acc:  0.8203125\n",
            "iter:  789 train_cost:  0.77963495 train_acc:  0.765625 test_cost:  0.50697386 test_acc:  0.84375\n",
            "iter:  790 train_cost:  0.5278777 train_acc:  0.84375 test_cost:  0.5198535 test_acc:  0.84375\n",
            "iter:  791 train_cost:  0.6506537 train_acc:  0.8125 test_cost:  0.5241641 test_acc:  0.828125\n",
            "iter:  792 train_cost:  0.6656402 train_acc:  0.765625 test_cost:  0.40596324 test_acc:  0.8671875\n",
            "iter:  793 train_cost:  0.4905006 train_acc:  0.84375 test_cost:  0.5145775 test_acc:  0.859375\n",
            "iter:  794 train_cost:  0.70582736 train_acc:  0.7890625 test_cost:  0.55541104 test_acc:  0.8515625\n",
            "iter:  795 train_cost:  0.47673085 train_acc:  0.828125 test_cost:  0.5436224 test_acc:  0.828125\n",
            "iter:  796 train_cost:  0.6877544 train_acc:  0.828125 test_cost:  0.3677923 test_acc:  0.890625\n",
            "iter:  797 train_cost:  0.7112763 train_acc:  0.7578125 test_cost:  0.66319966 test_acc:  0.8125\n",
            "iter:  798 train_cost:  0.3913015 train_acc:  0.8828125 test_cost:  0.56921023 test_acc:  0.7734375\n",
            "iter:  799 train_cost:  0.3700886 train_acc:  0.90625 test_cost:  0.5783168 test_acc:  0.8359375\n",
            "iter:  800 train_cost:  0.5452149 train_acc:  0.8359375 test_cost:  0.72639567 test_acc:  0.7578125\n",
            "iter:  801 train_cost:  0.68968594 train_acc:  0.765625 test_cost:  0.49475327 test_acc:  0.859375\n",
            "iter:  802 train_cost:  0.59421235 train_acc:  0.78125 test_cost:  0.5786291 test_acc:  0.828125\n",
            "iter:  803 train_cost:  0.6817306 train_acc:  0.7890625 test_cost:  0.43617836 test_acc:  0.84375\n",
            "iter:  804 train_cost:  0.40493786 train_acc:  0.875 test_cost:  0.6076653 test_acc:  0.796875\n",
            "iter:  805 train_cost:  0.78925985 train_acc:  0.8046875 test_cost:  0.53080183 test_acc:  0.828125\n",
            "iter:  806 train_cost:  0.4014906 train_acc:  0.8671875 test_cost:  0.62221 test_acc:  0.8203125\n",
            "iter:  807 train_cost:  0.47747573 train_acc:  0.84375 test_cost:  0.56956995 test_acc:  0.78125\n",
            "iter:  808 train_cost:  0.42978233 train_acc:  0.8671875 test_cost:  0.5206034 test_acc:  0.84375\n",
            "iter:  809 train_cost:  0.4963434 train_acc:  0.8359375 test_cost:  0.54194766 test_acc:  0.8359375\n",
            "iter:  810 train_cost:  0.36197138 train_acc:  0.8984375 test_cost:  0.5458899 test_acc:  0.8203125\n",
            "iter:  811 train_cost:  0.66887474 train_acc:  0.8203125 test_cost:  0.5294257 test_acc:  0.828125\n",
            "iter:  812 train_cost:  0.6260419 train_acc:  0.828125 test_cost:  0.50864875 test_acc:  0.8203125\n",
            "iter:  813 train_cost:  0.34557253 train_acc:  0.890625 test_cost:  0.4567816 test_acc:  0.875\n",
            "iter:  814 train_cost:  0.554477 train_acc:  0.8125 test_cost:  0.51840067 test_acc:  0.8671875\n",
            "iter:  815 train_cost:  0.61928844 train_acc:  0.8125 test_cost:  0.4607803 test_acc:  0.8515625\n",
            "iter:  816 train_cost:  0.4259456 train_acc:  0.84375 test_cost:  0.48783743 test_acc:  0.859375\n",
            "iter:  817 train_cost:  0.6629091 train_acc:  0.8046875 test_cost:  0.6497748 test_acc:  0.8125\n",
            "iter:  818 train_cost:  0.63176787 train_acc:  0.765625 test_cost:  0.56050617 test_acc:  0.84375\n",
            "iter:  819 train_cost:  0.49327692 train_acc:  0.8359375 test_cost:  0.47419775 test_acc:  0.828125\n",
            "iter:  820 train_cost:  0.5764535 train_acc:  0.7890625 test_cost:  0.3483114 test_acc:  0.890625\n",
            "iter:  821 train_cost:  0.44701928 train_acc:  0.8984375 test_cost:  0.5970977 test_acc:  0.8125\n",
            "iter:  822 train_cost:  0.47706547 train_acc:  0.859375 test_cost:  0.60940826 test_acc:  0.7890625\n",
            "iter:  823 train_cost:  0.56728345 train_acc:  0.8359375 test_cost:  0.43357146 test_acc:  0.859375\n",
            "iter:  824 train_cost:  0.6453273 train_acc:  0.8125 test_cost:  0.38358426 test_acc:  0.875\n",
            "iter:  825 train_cost:  0.48875535 train_acc:  0.8203125 test_cost:  0.5247956 test_acc:  0.8359375\n",
            "iter:  826 train_cost:  0.491995 train_acc:  0.84375 test_cost:  0.48327136 test_acc:  0.8203125\n",
            "iter:  827 train_cost:  0.64472425 train_acc:  0.8046875 test_cost:  0.4755387 test_acc:  0.84375\n",
            "iter:  828 train_cost:  0.37040597 train_acc:  0.890625 test_cost:  0.4448539 test_acc:  0.828125\n",
            "iter:  829 train_cost:  0.5715784 train_acc:  0.8359375 test_cost:  0.53225005 test_acc:  0.84375\n",
            "iter:  830 train_cost:  0.5796397 train_acc:  0.8515625 test_cost:  0.43815726 test_acc:  0.8359375\n",
            "iter:  831 train_cost:  0.5774112 train_acc:  0.7890625 test_cost:  0.4416684 test_acc:  0.8515625\n",
            "iter:  832 train_cost:  0.50142777 train_acc:  0.8515625 test_cost:  0.5596732 test_acc:  0.828125\n",
            "iter:  833 train_cost:  0.5385885 train_acc:  0.84375 test_cost:  0.6139043 test_acc:  0.8125\n",
            "iter:  834 train_cost:  0.56260055 train_acc:  0.828125 test_cost:  0.61820036 test_acc:  0.8046875\n",
            "iter:  835 train_cost:  0.49249762 train_acc:  0.8359375 test_cost:  0.3919521 test_acc:  0.890625\n",
            "iter:  836 train_cost:  0.52705604 train_acc:  0.859375 test_cost:  0.43839377 test_acc:  0.84375\n",
            "iter:  837 train_cost:  0.41628057 train_acc:  0.8359375 test_cost:  0.49508604 test_acc:  0.8515625\n",
            "iter:  838 train_cost:  0.5758904 train_acc:  0.84375 test_cost:  0.5874642 test_acc:  0.84375\n",
            "iter:  839 train_cost:  0.5950633 train_acc:  0.8125 test_cost:  0.5026126 test_acc:  0.8515625\n",
            "iter:  840 train_cost:  0.55909204 train_acc:  0.8203125 test_cost:  0.6194482 test_acc:  0.78125\n",
            "iter:  841 train_cost:  0.39874643 train_acc:  0.890625 test_cost:  0.44513404 test_acc:  0.8671875\n",
            "iter:  842 train_cost:  0.5852032 train_acc:  0.8125 test_cost:  0.4865021 test_acc:  0.84375\n",
            "iter:  843 train_cost:  0.66097665 train_acc:  0.7890625 test_cost:  0.511701 test_acc:  0.8203125\n",
            "iter:  844 train_cost:  0.40057844 train_acc:  0.84375 test_cost:  0.37343502 test_acc:  0.859375\n",
            "iter:  845 train_cost:  0.7334255 train_acc:  0.8046875 test_cost:  0.43772906 test_acc:  0.859375\n",
            "iter:  846 train_cost:  0.51959574 train_acc:  0.8359375 test_cost:  0.32620472 test_acc:  0.9140625\n",
            "iter:  847 train_cost:  0.3625732 train_acc:  0.8984375 test_cost:  0.43740276 test_acc:  0.875\n",
            "iter:  848 train_cost:  0.49592334 train_acc:  0.84375 test_cost:  0.5858481 test_acc:  0.8359375\n",
            "iter:  849 train_cost:  0.46819896 train_acc:  0.8203125 test_cost:  0.6070865 test_acc:  0.8125\n",
            "iter:  850 train_cost:  0.45950764 train_acc:  0.8359375 test_cost:  0.7141646 test_acc:  0.7734375\n",
            "iter:  851 train_cost:  0.6405816 train_acc:  0.828125 test_cost:  0.50697553 test_acc:  0.84375\n",
            "iter:  852 train_cost:  0.5248497 train_acc:  0.8359375 test_cost:  0.6635881 test_acc:  0.796875\n",
            "iter:  853 train_cost:  0.6746272 train_acc:  0.7734375 test_cost:  0.47985175 test_acc:  0.859375\n",
            "iter:  854 train_cost:  0.49708045 train_acc:  0.828125 test_cost:  0.4313385 test_acc:  0.859375\n",
            "iter:  855 train_cost:  0.48935294 train_acc:  0.859375 test_cost:  0.5304971 test_acc:  0.8359375\n",
            "iter:  856 train_cost:  0.64730656 train_acc:  0.8046875 test_cost:  0.44588876 test_acc:  0.8515625\n",
            "iter:  857 train_cost:  0.61895823 train_acc:  0.8203125 test_cost:  0.5589168 test_acc:  0.859375\n",
            "iter:  858 train_cost:  0.60044473 train_acc:  0.796875 test_cost:  0.45800534 test_acc:  0.875\n",
            "iter:  859 train_cost:  0.46964255 train_acc:  0.8515625 test_cost:  0.51211315 test_acc:  0.828125\n",
            "iter:  860 train_cost:  0.582646 train_acc:  0.796875 test_cost:  0.6484353 test_acc:  0.828125\n",
            "iter:  861 train_cost:  0.5127493 train_acc:  0.8359375 test_cost:  0.4105609 test_acc:  0.875\n",
            "iter:  862 train_cost:  0.38741076 train_acc:  0.8828125 test_cost:  0.60374063 test_acc:  0.796875\n",
            "iter:  863 train_cost:  0.55816567 train_acc:  0.84375 test_cost:  0.44956797 test_acc:  0.796875\n",
            "iter:  864 train_cost:  0.5318388 train_acc:  0.8515625 test_cost:  0.48288566 test_acc:  0.875\n",
            "iter:  865 train_cost:  0.3656677 train_acc:  0.8984375 test_cost:  0.51796556 test_acc:  0.84375\n",
            "iter:  866 train_cost:  0.48042542 train_acc:  0.84375 test_cost:  0.44887155 test_acc:  0.859375\n",
            "iter:  867 train_cost:  0.404135 train_acc:  0.90625 test_cost:  0.52497494 test_acc:  0.8671875\n",
            "iter:  868 train_cost:  0.5623831 train_acc:  0.859375 test_cost:  0.6501634 test_acc:  0.796875\n",
            "iter:  869 train_cost:  0.4991563 train_acc:  0.859375 test_cost:  0.46681648 test_acc:  0.828125\n",
            "iter:  870 train_cost:  0.48912442 train_acc:  0.84375 test_cost:  0.47075325 test_acc:  0.84375\n",
            "iter:  871 train_cost:  0.6692748 train_acc:  0.8203125 test_cost:  0.53098595 test_acc:  0.8359375\n",
            "iter:  872 train_cost:  0.48289806 train_acc:  0.828125 test_cost:  0.52020216 test_acc:  0.8046875\n",
            "iter:  873 train_cost:  0.44367146 train_acc:  0.859375 test_cost:  0.40208128 test_acc:  0.8828125\n",
            "iter:  874 train_cost:  0.43713355 train_acc:  0.8515625 test_cost:  0.472539 test_acc:  0.859375\n",
            "iter:  875 train_cost:  0.4706945 train_acc:  0.8203125 test_cost:  0.40864596 test_acc:  0.8515625\n",
            "iter:  876 train_cost:  0.5774577 train_acc:  0.8203125 test_cost:  0.44113335 test_acc:  0.859375\n",
            "iter:  877 train_cost:  0.43026733 train_acc:  0.84375 test_cost:  0.49117905 test_acc:  0.8515625\n",
            "iter:  878 train_cost:  0.4161484 train_acc:  0.84375 test_cost:  0.4133286 test_acc:  0.828125\n",
            "iter:  879 train_cost:  0.68255544 train_acc:  0.8046875 test_cost:  0.61080897 test_acc:  0.796875\n",
            "iter:  880 train_cost:  0.43672335 train_acc:  0.8515625 test_cost:  0.36768985 test_acc:  0.875\n",
            "iter:  881 train_cost:  0.55804026 train_acc:  0.8125 test_cost:  0.48724312 test_acc:  0.859375\n",
            "iter:  882 train_cost:  0.36130333 train_acc:  0.875 test_cost:  0.41092902 test_acc:  0.84375\n",
            "iter:  883 train_cost:  0.44237813 train_acc:  0.8359375 test_cost:  0.46293306 test_acc:  0.828125\n",
            "iter:  884 train_cost:  0.4561736 train_acc:  0.84375 test_cost:  0.44914466 test_acc:  0.84375\n",
            "iter:  885 train_cost:  0.40193158 train_acc:  0.890625 test_cost:  0.5107414 test_acc:  0.8203125\n",
            "iter:  886 train_cost:  0.44303122 train_acc:  0.859375 test_cost:  0.45007655 test_acc:  0.890625\n",
            "iter:  887 train_cost:  0.5471409 train_acc:  0.8515625 test_cost:  0.6587897 test_acc:  0.8046875\n",
            "iter:  888 train_cost:  0.48544198 train_acc:  0.7890625 test_cost:  0.58928645 test_acc:  0.8125\n",
            "iter:  889 train_cost:  0.5599342 train_acc:  0.8515625 test_cost:  0.509905 test_acc:  0.84375\n",
            "iter:  890 train_cost:  0.49450693 train_acc:  0.828125 test_cost:  0.39036074 test_acc:  0.875\n",
            "iter:  891 train_cost:  0.47449368 train_acc:  0.84375 test_cost:  0.4746375 test_acc:  0.84375\n",
            "iter:  892 train_cost:  0.4888002 train_acc:  0.828125 test_cost:  0.6145821 test_acc:  0.828125\n",
            "iter:  893 train_cost:  0.3540729 train_acc:  0.9140625 test_cost:  0.4552169 test_acc:  0.8515625\n",
            "iter:  894 train_cost:  0.34506124 train_acc:  0.921875 test_cost:  0.6836503 test_acc:  0.8203125\n",
            "iter:  895 train_cost:  0.4242058 train_acc:  0.875 test_cost:  0.36605883 test_acc:  0.8828125\n",
            "iter:  896 train_cost:  0.4684426 train_acc:  0.8984375 test_cost:  0.42750585 test_acc:  0.875\n",
            "iter:  897 train_cost:  0.4074027 train_acc:  0.828125 test_cost:  0.47497666 test_acc:  0.859375\n",
            "iter:  898 train_cost:  0.46176994 train_acc:  0.8828125 test_cost:  0.41574025 test_acc:  0.8515625\n",
            "iter:  899 train_cost:  0.6980041 train_acc:  0.8125 test_cost:  0.5120569 test_acc:  0.859375\n",
            "iter:  900 train_cost:  0.63520837 train_acc:  0.796875 test_cost:  0.50743085 test_acc:  0.84375\n",
            "iter:  901 train_cost:  0.32432503 train_acc:  0.8828125 test_cost:  0.51249576 test_acc:  0.828125\n",
            "iter:  902 train_cost:  0.42819458 train_acc:  0.859375 test_cost:  0.5897306 test_acc:  0.84375\n",
            "iter:  903 train_cost:  0.50693095 train_acc:  0.8046875 test_cost:  0.36648458 test_acc:  0.875\n",
            "iter:  904 train_cost:  0.56779444 train_acc:  0.828125 test_cost:  0.4980896 test_acc:  0.8359375\n",
            "iter:  905 train_cost:  0.45171824 train_acc:  0.84375 test_cost:  0.40006995 test_acc:  0.8828125\n",
            "iter:  906 train_cost:  0.35487872 train_acc:  0.8828125 test_cost:  0.58920264 test_acc:  0.7734375\n",
            "iter:  907 train_cost:  0.50416553 train_acc:  0.8671875 test_cost:  0.36997372 test_acc:  0.8828125\n",
            "iter:  908 train_cost:  0.5820646 train_acc:  0.8125 test_cost:  0.4395227 test_acc:  0.8671875\n",
            "iter:  909 train_cost:  0.56728077 train_acc:  0.828125 test_cost:  0.45228615 test_acc:  0.875\n",
            "iter:  910 train_cost:  0.51750004 train_acc:  0.84375 test_cost:  0.35342026 test_acc:  0.90625\n",
            "iter:  911 train_cost:  0.457141 train_acc:  0.84375 test_cost:  0.5455078 test_acc:  0.8203125\n",
            "iter:  912 train_cost:  0.382725 train_acc:  0.8515625 test_cost:  0.3832292 test_acc:  0.875\n",
            "iter:  913 train_cost:  0.49483538 train_acc:  0.875 test_cost:  0.55502725 test_acc:  0.84375\n",
            "iter:  914 train_cost:  0.49131656 train_acc:  0.8515625 test_cost:  0.51771796 test_acc:  0.8671875\n",
            "iter:  915 train_cost:  0.5240585 train_acc:  0.859375 test_cost:  0.53907454 test_acc:  0.828125\n",
            "iter:  916 train_cost:  0.4039323 train_acc:  0.8671875 test_cost:  0.5302613 test_acc:  0.8515625\n",
            "iter:  917 train_cost:  0.33466613 train_acc:  0.890625 test_cost:  0.37768856 test_acc:  0.890625\n",
            "iter:  918 train_cost:  0.4708502 train_acc:  0.875 test_cost:  0.5278877 test_acc:  0.8203125\n",
            "iter:  919 train_cost:  0.47295323 train_acc:  0.8515625 test_cost:  0.5773875 test_acc:  0.828125\n",
            "iter:  920 train_cost:  0.56681246 train_acc:  0.8671875 test_cost:  0.30525616 test_acc:  0.8984375\n",
            "iter:  921 train_cost:  0.54066074 train_acc:  0.8359375 test_cost:  0.6425027 test_acc:  0.8515625\n",
            "iter:  922 train_cost:  0.3334761 train_acc:  0.875 test_cost:  0.4856196 test_acc:  0.859375\n",
            "iter:  923 train_cost:  0.42955044 train_acc:  0.8515625 test_cost:  0.40224883 test_acc:  0.8984375\n",
            "iter:  924 train_cost:  0.3639946 train_acc:  0.9140625 test_cost:  0.35695416 test_acc:  0.8359375\n",
            "iter:  925 train_cost:  0.4610322 train_acc:  0.859375 test_cost:  0.68488353 test_acc:  0.78125\n",
            "iter:  926 train_cost:  0.45031995 train_acc:  0.875 test_cost:  0.4303916 test_acc:  0.8671875\n",
            "iter:  927 train_cost:  0.49623805 train_acc:  0.8515625 test_cost:  0.54647946 test_acc:  0.8359375\n",
            "iter:  928 train_cost:  0.47427398 train_acc:  0.84375 test_cost:  0.44793147 test_acc:  0.859375\n",
            "iter:  929 train_cost:  0.58261853 train_acc:  0.859375 test_cost:  0.6330895 test_acc:  0.8046875\n",
            "iter:  930 train_cost:  0.48518008 train_acc:  0.875 test_cost:  0.4119324 test_acc:  0.890625\n",
            "iter:  931 train_cost:  0.45387495 train_acc:  0.859375 test_cost:  0.55201334 test_acc:  0.875\n",
            "iter:  932 train_cost:  0.5424105 train_acc:  0.875 test_cost:  0.49303424 test_acc:  0.8359375\n",
            "iter:  933 train_cost:  0.51294243 train_acc:  0.84375 test_cost:  0.51355124 test_acc:  0.8671875\n",
            "iter:  934 train_cost:  0.48260677 train_acc:  0.84375 test_cost:  0.38797677 test_acc:  0.875\n",
            "iter:  935 train_cost:  0.53707385 train_acc:  0.84375 test_cost:  0.48949602 test_acc:  0.84375\n",
            "iter:  936 train_cost:  0.5666901 train_acc:  0.8046875 test_cost:  0.43596527 test_acc:  0.8359375\n",
            "iter:  937 train_cost:  0.48388416 train_acc:  0.8359375 test_cost:  0.42379224 test_acc:  0.859375\n",
            "iter:  938 train_cost:  0.44825378 train_acc:  0.859375 test_cost:  0.43439263 test_acc:  0.8671875\n",
            "iter:  939 train_cost:  0.46986142 train_acc:  0.859375 test_cost:  0.4077322 test_acc:  0.8984375\n",
            "iter:  940 train_cost:  0.50036085 train_acc:  0.8828125 test_cost:  0.41181827 test_acc:  0.90625\n",
            "iter:  941 train_cost:  0.45434976 train_acc:  0.8828125 test_cost:  0.40968144 test_acc:  0.84375\n",
            "iter:  942 train_cost:  0.4998788 train_acc:  0.8515625 test_cost:  0.45823014 test_acc:  0.84375\n",
            "iter:  943 train_cost:  0.36730754 train_acc:  0.8828125 test_cost:  0.39426118 test_acc:  0.8671875\n",
            "iter:  944 train_cost:  0.5044885 train_acc:  0.8359375 test_cost:  0.41971952 test_acc:  0.875\n",
            "iter:  945 train_cost:  0.3920417 train_acc:  0.875 test_cost:  0.41874152 test_acc:  0.8515625\n",
            "iter:  946 train_cost:  0.4145746 train_acc:  0.8828125 test_cost:  0.45422637 test_acc:  0.8359375\n",
            "iter:  947 train_cost:  0.4099599 train_acc:  0.8984375 test_cost:  0.50851417 test_acc:  0.8359375\n",
            "iter:  948 train_cost:  0.5006128 train_acc:  0.8671875 test_cost:  0.524541 test_acc:  0.8671875\n",
            "iter:  949 train_cost:  0.49288577 train_acc:  0.8125 test_cost:  0.37913346 test_acc:  0.859375\n",
            "iter:  950 train_cost:  0.49648538 train_acc:  0.8671875 test_cost:  0.6658304 test_acc:  0.7734375\n",
            "iter:  951 train_cost:  0.4732768 train_acc:  0.859375 test_cost:  0.35713363 test_acc:  0.8671875\n",
            "iter:  952 train_cost:  0.4560303 train_acc:  0.8515625 test_cost:  0.43514 test_acc:  0.8359375\n",
            "iter:  953 train_cost:  0.4575604 train_acc:  0.84375 test_cost:  0.521101 test_acc:  0.8515625\n",
            "iter:  954 train_cost:  0.36064976 train_acc:  0.875 test_cost:  0.6042449 test_acc:  0.8203125\n",
            "iter:  955 train_cost:  0.3753116 train_acc:  0.8984375 test_cost:  0.25779182 test_acc:  0.921875\n",
            "iter:  956 train_cost:  0.44176143 train_acc:  0.8828125 test_cost:  0.4320268 test_acc:  0.8828125\n",
            "iter:  957 train_cost:  0.5072566 train_acc:  0.84375 test_cost:  0.4178182 test_acc:  0.875\n",
            "iter:  958 train_cost:  0.45879877 train_acc:  0.8828125 test_cost:  0.47012776 test_acc:  0.8515625\n",
            "iter:  959 train_cost:  0.59438634 train_acc:  0.8203125 test_cost:  0.5269172 test_acc:  0.8671875\n",
            "iter:  960 train_cost:  0.63621575 train_acc:  0.84375 test_cost:  0.5053433 test_acc:  0.84375\n",
            "iter:  961 train_cost:  0.45104593 train_acc:  0.8671875 test_cost:  0.348892 test_acc:  0.890625\n",
            "iter:  962 train_cost:  0.47354865 train_acc:  0.8359375 test_cost:  0.5467958 test_acc:  0.859375\n",
            "iter:  963 train_cost:  0.467884 train_acc:  0.859375 test_cost:  0.3161685 test_acc:  0.921875\n",
            "iter:  964 train_cost:  0.47993326 train_acc:  0.8515625 test_cost:  0.510751 test_acc:  0.8125\n",
            "iter:  965 train_cost:  0.37922794 train_acc:  0.890625 test_cost:  0.5530103 test_acc:  0.828125\n",
            "iter:  966 train_cost:  0.49567282 train_acc:  0.875 test_cost:  0.38957855 test_acc:  0.875\n",
            "iter:  967 train_cost:  0.46886906 train_acc:  0.828125 test_cost:  0.6245682 test_acc:  0.828125\n",
            "iter:  968 train_cost:  0.41048172 train_acc:  0.8515625 test_cost:  0.55127 test_acc:  0.8125\n",
            "iter:  969 train_cost:  0.43891376 train_acc:  0.875 test_cost:  0.4707042 test_acc:  0.84375\n",
            "iter:  970 train_cost:  0.541201 train_acc:  0.84375 test_cost:  0.5018019 test_acc:  0.8671875\n",
            "iter:  971 train_cost:  0.55660117 train_acc:  0.8203125 test_cost:  0.5538444 test_acc:  0.8203125\n",
            "iter:  972 train_cost:  0.42088568 train_acc:  0.875 test_cost:  0.49416745 test_acc:  0.8515625\n",
            "iter:  973 train_cost:  0.62192404 train_acc:  0.8359375 test_cost:  0.4823846 test_acc:  0.8203125\n",
            "iter:  974 train_cost:  0.5936422 train_acc:  0.828125 test_cost:  0.5018627 test_acc:  0.828125\n",
            "iter:  975 train_cost:  0.47722784 train_acc:  0.8203125 test_cost:  0.48365954 test_acc:  0.8359375\n",
            "iter:  976 train_cost:  0.42737138 train_acc:  0.8984375 test_cost:  0.6319884 test_acc:  0.8125\n",
            "iter:  977 train_cost:  0.584189 train_acc:  0.8203125 test_cost:  0.43257427 test_acc:  0.875\n",
            "iter:  978 train_cost:  0.33648604 train_acc:  0.90625 test_cost:  0.46596166 test_acc:  0.84375\n",
            "iter:  979 train_cost:  0.36390677 train_acc:  0.859375 test_cost:  0.471232 test_acc:  0.8515625\n",
            "iter:  980 train_cost:  0.53646654 train_acc:  0.8203125 test_cost:  0.49795094 test_acc:  0.8359375\n",
            "iter:  981 train_cost:  0.64477324 train_acc:  0.8203125 test_cost:  0.45864674 test_acc:  0.828125\n",
            "iter:  982 train_cost:  0.47859612 train_acc:  0.859375 test_cost:  0.56724477 test_acc:  0.8671875\n",
            "iter:  983 train_cost:  0.50675726 train_acc:  0.84375 test_cost:  0.62949646 test_acc:  0.7890625\n",
            "iter:  984 train_cost:  0.396571 train_acc:  0.8984375 test_cost:  0.38236398 test_acc:  0.859375\n",
            "iter:  985 train_cost:  0.55374944 train_acc:  0.8828125 test_cost:  0.49295083 test_acc:  0.84375\n",
            "iter:  986 train_cost:  0.4444008 train_acc:  0.8984375 test_cost:  0.51589 test_acc:  0.828125\n",
            "iter:  987 train_cost:  0.413253 train_acc:  0.8828125 test_cost:  0.42131656 test_acc:  0.890625\n",
            "iter:  988 train_cost:  0.56740475 train_acc:  0.84375 test_cost:  0.5309384 test_acc:  0.859375\n",
            "iter:  989 train_cost:  0.50298566 train_acc:  0.84375 test_cost:  0.52751756 test_acc:  0.8359375\n",
            "iter:  990 train_cost:  0.42813793 train_acc:  0.8671875 test_cost:  0.5778283 test_acc:  0.8125\n",
            "iter:  991 train_cost:  0.450578 train_acc:  0.8359375 test_cost:  0.29035532 test_acc:  0.9140625\n",
            "iter:  992 train_cost:  0.34969062 train_acc:  0.8984375 test_cost:  0.6163151 test_acc:  0.8359375\n",
            "iter:  993 train_cost:  0.55896723 train_acc:  0.84375 test_cost:  0.43946567 test_acc:  0.8046875\n",
            "iter:  994 train_cost:  0.4749906 train_acc:  0.8515625 test_cost:  0.3277167 test_acc:  0.890625\n",
            "iter:  995 train_cost:  0.63721704 train_acc:  0.796875 test_cost:  0.28574875 test_acc:  0.90625\n",
            "iter:  996 train_cost:  0.5580212 train_acc:  0.828125 test_cost:  0.6188501 test_acc:  0.7890625\n",
            "iter:  997 train_cost:  0.7504244 train_acc:  0.7734375 test_cost:  0.41571832 test_acc:  0.875\n",
            "iter:  998 train_cost:  0.43175706 train_acc:  0.8515625 test_cost:  0.54354787 test_acc:  0.8671875\n",
            "iter:  999 train_cost:  0.6273792 train_acc:  0.8125 test_cost:  0.36375287 test_acc:  0.8671875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.167191"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDkPoIsB90MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10.579477999999998 TPU\n",
        "# 10.682025999999999 CPU\n",
        "# 8.640348 GPU \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZtWbWQ92P_",
        "colab_type": "text"
      },
      "source": [
        "# CNN using Tensorflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6k9YPZQ95z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1703
        },
        "outputId": "498b8504-e987-4178-e9e8-6541d22f6239"
      },
      "source": [
        "n_classes=10\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # x input image, W..weights is a tamplate(3d matrix for images),\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    print('con1_before max',conv1.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=1)\n",
        "    print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    print('con2_before max',conv2.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=1)\n",
        "    print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "#\n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "con1_before max [None, 28, 28, 32]\n",
            "con1_after max [None, 28, 28, 32]\n",
            "con2_before max [None, 28, 28, 64]\n",
            "con2_after max [None, 28, 28, 64]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[{{node softmax_cross_entropy_with_logits_sg_4}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2c408dd24289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[node softmax_cross_entropy_with_logits_sg_4 (defined at <ipython-input-35-2c408dd24289>:74) ]]\n\nCaused by op 'softmax_cross_entropy_with_logits_sg_4', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-2c408dd24289>\", line 74, in <module>\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2561, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, axis=dim, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2370, in softmax_cross_entropy_with_logits_v2\n    labels=labels, logits=logits, axis=axis, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2471, in softmax_cross_entropy_with_logits_v2_helper\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7862, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[node softmax_cross_entropy_with_logits_sg_4 (defined at <ipython-input-35-2c408dd24289>:74) ]]\n"
          ]
        }
      ]
    }
  ]
}