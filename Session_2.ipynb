{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmadhajmosa/Machine-Learning-Lab-2020/blob/emir/Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSUExzC-ZBV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWZldyi-Zef",
        "colab_type": "text"
      },
      "source": [
        "# Lab on Machine Learning and Applications in Intelligent Vehicles\n",
        "## Session 1: Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a75NLOuwKXSs",
        "colab_type": "text"
      },
      "source": [
        "#Session 2: 05.06 - 13:00 - 14:30 :\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciUtGM-W_ehP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Intro:\n",
        "\n",
        "Tensorflow is a powerful framework for implementing and deploying large-scale deep learning models. Recently, it has been widely used in both reasearch and production. TF objective is to combine scale and flexibility.\n",
        "\n",
        "In the past session, we will learning the following:\n",
        "\n",
        "1. TF programming stack\n",
        "2. TF programming concepts including computatoin graphs, operations and sessions. \n",
        "3. Implementation of linear regression\n",
        "4. Implementation of feed-forward neural networks\n",
        "\n",
        "## TF stack:\n",
        "\n",
        "TensorFlow is a framework composed of two core building blocks — a library for defining computational graphs and a runtime for executing such graphs on a variety of different hardware\n",
        "\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/layers.png)\n",
        "\n",
        "\n",
        "Before goining into details about the stack, let us talk about computational graphs.\n",
        "\n",
        "### Computational Graphs\n",
        "\n",
        "A directed graph is a data structure consisting of nodes (vertices) and edges. It’s a set of vertices connected pairwise by directed edges.\n",
        "\n",
        "Graphs come in many shapes and sizes and are used to solve many real-life problems, such as representing networks including telephone networks, circuit networks, road networks, and even social networks. \n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*V6aYjD3AxDbEKYahkGqVQw.png)\n",
        "\n",
        "TensorFlow uses directed graphs internally to represent computations, and they call this data flow graphs (or computational graphs).\n",
        "\n",
        "The nodes in TF data flow graph mostly represents operations, variables and placeholders.\n",
        "\n",
        "Take for example the following operation:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "\n",
        "To create a computational graph out of this program, we create nodes for each of the operations in our program, along with the input variables a and b. In fact, a and b could be constants if they don’t change. If one node is used as the input to another operation we draw a directed arrow that goes from one node to another.\n",
        "\n",
        "The computational graph for this program might look like this:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*vPb9E0Yd1QUAD0oFmAgaOw.png)\n",
        "\n",
        "Operations create or manipulate data according to specific rules. In TensorFlow those rules are called Ops, short for operations. Variables on the other hand represent shared, persistent state that can be manipulated by running Ops on those variables.\n",
        "\n",
        "The questions now what are the advantages of representing operations as directed graphs: The main advantage of using directed graphs is the ability to do **parallelism** and what is called **dependency driving scheduling**. \n",
        "For example, consider again the follwoing code:\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*6E3sfit6DCeJ9mOz17g4bA.png)\n",
        "At the most fundamental level, most computer programs are mainly composed of two things — primitive operations and an order in which these operations are executed, often sequentially, line by line. This means we would first multiply a and b and only when this expression was evaluated we would take their sum. Computational graphs on the otherhand, exclusively specify the dependencies across the operations.\n",
        "If we look at our computational graph we see that we could execute the multiplication and addition in parallel. That’s because these two operations do not depend on each other.\n",
        " So we can use the topology of the graph to drive the scheduling of operations and execute them in the most efficient manner, e.g. using multiple GPUs on a single machine or even distribute the execution across multiple machines.\n",
        " Another key advantage is portability. The graph is a language-independent representation of our code. So we can build the graph in Python, save the model (TensorFlow uses protocol buffers), and restore the model in a different language, say C++, if you want to go really fast.\n",
        " \n",
        " \n",
        "\n",
        "--------------------------------\n",
        "# References:\n",
        "\n",
        "https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d\n",
        "\n",
        "https://www.tensorflow.org/guide/extend/architecture\n",
        "\n",
        "https://www.tensorflow.org/guide/low_level_intro\n",
        "\n",
        "  \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-GFJPVDnEwx",
        "colab_type": "text"
      },
      "source": [
        "# placeholder: tensors are feeded externaly for example inputs tensors + output tensors\n",
        "\n",
        "# variables : tensors represent the parameters of the network/graph ie. nn weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7df1t27dZKuC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52bdf072-5fd3-40d9-a422-0457817cf1d6"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlmSCbhtoJBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "c02a9706-cb63-41e4-be11-80eecbe4faa6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_outputs = 4\n",
        "num_samples= 10\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_outputs ]))\n",
        "\n",
        "# model\n",
        "y_p = tf.matmul(x, w_1)\n",
        "\n",
        "# cost\n",
        "\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "iter:  0 cost:  0.9251641\n",
            "iter:  1 cost:  0.9230196\n",
            "iter:  2 cost:  0.92087907\n",
            "iter:  3 cost:  0.9187428\n",
            "iter:  4 cost:  0.91661054\n",
            "iter:  5 cost:  0.9144825\n",
            "iter:  6 cost:  0.91235864\n",
            "iter:  7 cost:  0.91023934\n",
            "iter:  8 cost:  0.90812427\n",
            "iter:  9 cost:  0.9060138\n",
            "predicted  [[ 0.5471682   0.6440942  -0.5967087  -0.67613167]\n",
            " [-0.02052948  0.6660044  -0.52508885 -1.2014067 ]\n",
            " [-0.16581537  0.2078205  -0.5820717  -0.81910884]\n",
            " [ 0.57350653  0.5787266  -0.64671475 -0.60521734]\n",
            " [-0.14361182  0.7289338  -0.3398015  -1.2688292 ]\n",
            " [ 0.69740045  0.55306333 -0.7654652  -0.5365445 ]\n",
            " [-0.24848664  0.25887492 -0.26260936 -0.7398072 ]\n",
            " [-1.1884054  -0.08317816  0.57125366 -0.6488136 ]\n",
            " [ 0.20469603  0.5664025  -0.10122199 -0.56752527]\n",
            " [-0.08939911  0.37532228 -0.3440038  -0.7855734 ]]\n",
            "real  [[0.49760204 0.31353225 0.32040492 0.18017236]\n",
            " [0.53975364 0.13458741 0.75629457 0.74584247]\n",
            " [0.50628167 0.60462236 0.85903408 0.27639893]\n",
            " [0.613447   0.91151207 0.05404809 0.8225853 ]\n",
            " [0.32284177 0.97469127 0.08250948 0.17639807]\n",
            " [0.43602598 0.1753335  0.9531061  0.82751121]\n",
            " [0.48932473 0.63877129 0.96018982 0.77345667]\n",
            " [0.1514769  0.27482945 0.59551825 0.90788982]\n",
            " [0.81213335 0.17482753 0.82352623 0.66691139]\n",
            " [0.98701294 0.79320885 0.08390515 0.30401626]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv2aqi3Fu-AJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "48f869ad-ea16-4353-b818-25f34405fcec"
      },
      "source": [
        "sess = tf.Session() \n",
        "sess.run(init)\n",
        "    \n",
        "for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "print('predicted ', y_p_p)\n",
        "print('real ', y_gr)\n",
        "\n",
        "#sess.close()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  2.3408885\n",
            "iter:  1 cost:  2.337693\n",
            "iter:  2 cost:  2.3345027\n",
            "iter:  3 cost:  2.3313174\n",
            "iter:  4 cost:  2.3281372\n",
            "iter:  5 cost:  2.3249626\n",
            "iter:  6 cost:  2.3217936\n",
            "iter:  7 cost:  2.3186297\n",
            "iter:  8 cost:  2.3154721\n",
            "iter:  9 cost:  2.3123195\n",
            "predicted  [[ 0.24237497 -0.4789864  -2.2616656   0.32732666]\n",
            " [ 0.1757657  -0.2780663  -3.2778995   0.30318576]\n",
            " [-0.05248906 -0.07198967 -1.0122445  -0.7143535 ]\n",
            " [ 0.20904909 -0.47046933 -1.8423275   0.12277   ]\n",
            " [ 0.21928076 -0.23788953 -3.9112666   0.7139541 ]\n",
            " [ 0.19450398 -0.5173181  -1.4469794  -0.09736606]\n",
            " [ 0.01873198 -0.02859414 -1.6016804  -0.07015004]\n",
            " [-0.11140423  0.50781876 -1.6900215   0.4657509 ]\n",
            " [ 0.25245225 -0.2808039  -2.6599216   0.95686954]\n",
            " [ 0.07728175 -0.13389206 -1.9357418   0.05742874]]\n",
            "real  [[0.49760204 0.31353225 0.32040492 0.18017236]\n",
            " [0.53975364 0.13458741 0.75629457 0.74584247]\n",
            " [0.50628167 0.60462236 0.85903408 0.27639893]\n",
            " [0.613447   0.91151207 0.05404809 0.8225853 ]\n",
            " [0.32284177 0.97469127 0.08250948 0.17639807]\n",
            " [0.43602598 0.1753335  0.9531061  0.82751121]\n",
            " [0.48932473 0.63877129 0.96018982 0.77345667]\n",
            " [0.1514769  0.27482945 0.59551825 0.90788982]\n",
            " [0.81213335 0.17482753 0.82352623 0.66691139]\n",
            " [0.98701294 0.79320885 0.08390515 0.30401626]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6qknOSWeKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd206b7a-5ae9-4eab-b116-beec9f20aae1"
      },
      "source": [
        "%tensorflow_version 1.x\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUPSS03avw5D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "bbc606b8-7bb1-413d-b09f-fc6f243717d5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 3\n",
        "num_h1_n = 4\n",
        "num_h2_n = 10\n",
        "num_outputs = 4\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#  O   O   O   O\n",
        "#      O   O   O\n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "#          O   \n",
        "\n",
        "\n",
        "num_samples= 10\n",
        "\n",
        "# Training data\n",
        "x_gr = np.random.rand(num_samples,num_inputs)\n",
        "y_gr = np.random.rand(num_samples,num_outputs)\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "#F(WX+b)\n",
        "# model\n",
        "\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "#placeholders/inputs:outpus --> Variables/Weights --> Model --> cost --> optimizer --> initilize all variables --> start the session\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.pow(y-y_p,2)) # \n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(10):\n",
        "    \n",
        "        sess.run(optimizer, feed_dict={x: x_gr, y: y_gr}) \n",
        "\n",
        "        pr_cost = sess.run(cost, feed_dict={x: x_gr,y: y_gr})\n",
        "    \n",
        "        print('iter: ',i, 'cost: ', pr_cost)\n",
        "    \n",
        "    y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 cost:  2.3913162\n",
            "iter:  1 cost:  2.368434\n",
            "iter:  2 cost:  2.3456938\n",
            "iter:  3 cost:  2.3230968\n",
            "iter:  4 cost:  2.3006427\n",
            "iter:  5 cost:  2.2783332\n",
            "iter:  6 cost:  2.25617\n",
            "iter:  7 cost:  2.2341537\n",
            "iter:  8 cost:  2.2122865\n",
            "iter:  9 cost:  2.1905699\n",
            "predicted  [[ 2.4571824  -1.4728684  -0.5035786   0.36105055]\n",
            " [ 2.3783302  -1.4778717  -0.5346699   0.35490805]\n",
            " [ 2.38185    -1.4839454  -0.4710874   0.32579368]\n",
            " [ 2.5402606  -1.465323   -0.52372813  0.39299434]\n",
            " [ 2.4191535  -1.4801881  -0.48079252  0.34057873]\n",
            " [ 2.238059   -1.4360793  -0.7165757   0.35904354]\n",
            " [ 2.2775388  -1.4477401  -0.7195281   0.37194544]\n",
            " [ 2.3408968  -1.484036   -0.47707683  0.3162409 ]\n",
            " [ 2.1307628  -1.4887321  -0.5778211   0.2486881 ]\n",
            " [ 2.302291   -1.4316609  -0.72863585  0.40350646]]\n",
            "real  [[0.80995619 0.40487275 0.51567326 0.20287189]\n",
            " [0.80964951 0.54433139 0.44038155 0.63512801]\n",
            " [0.62686723 0.66426488 0.14256951 0.60394398]\n",
            " [0.00829485 0.67945396 0.94180642 0.74680466]\n",
            " [0.32816447 0.45940143 0.15390601 0.57218668]\n",
            " [0.3601124  0.58531206 0.23547938 0.61433281]\n",
            " [0.36525227 0.92261253 0.11650207 0.88743648]\n",
            " [0.39578365 0.72077923 0.22510112 0.88386776]\n",
            " [0.42189577 0.25243156 0.0190603  0.96099729]\n",
            " [0.50116232 0.3119491  0.3946565  0.13681191]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwhCMk9VyVj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "f184af28-fdf3-4d3b-915b-63f4d0a468bd"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "display(mnist.train.images.shape) # 28*28 = 784\n",
        "\n",
        "image =mnist.train.images[50].reshape((28,28))\n",
        "#MNIST data input (img shape: 28*28)\n",
        "imshow(image)\n",
        "\n",
        "print(mnist.train.labels[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-4bab6bbff6b9>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(55000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXapqksBX95W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "tic = time.clock()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeRp0T10834",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5ad3841-c02e-421f-b919-21e96de171e2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# training data\n",
        "X_train = mnist.train.images\n",
        "Y_train = mnist.train.labels\n",
        "\n",
        "# testing data\n",
        "X_test = mnist.test.images\n",
        "Y_test = mnist.test.labels\n",
        "\n",
        "# training data validation\n",
        "X_val = mnist.validation.images\n",
        "Y_val = mnist.validation.labels\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_iters = 2000\n",
        "batch_size = 128\n",
        "\n",
        "# Network Parameters\n",
        "\n",
        "num_inputs = 784\n",
        "num_h1_n = 100\n",
        "num_h2_n = 100\n",
        "num_outputs = 10 #digits 0 to 9\n",
        "\n",
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, num_inputs])\n",
        "y = tf.placeholder(tf.float32, [None, num_outputs])\n",
        "\n",
        "# weights \n",
        "w_1 = tf.Variable(tf.random_normal([num_inputs,num_h1_n ]))\n",
        "w_2 = tf.Variable(tf.random_normal([num_h1_n,num_h2_n ]))\n",
        "w_3 = tf.Variable(tf.random_normal([num_h2_n,num_outputs ]))\n",
        "\n",
        "# bias \n",
        "b_1 = tf.Variable(tf.random_normal([num_h1_n]))\n",
        "b_2 = tf.Variable(tf.random_normal([num_h2_n]))\n",
        "b_3 = tf.Variable(tf.random_normal([num_outputs]))\n",
        "\n",
        "# model\n",
        "h1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w_1),b_1)) # model of hidden layer 1\n",
        "h2 = tf.nn.sigmoid(tf.add(tf.matmul(h1, w_2),b_2)) # model of hidden layer 2\n",
        "y_p = tf.add(tf.matmul(h2, w_3),b_3) # model of the output layer\n",
        "\n",
        "# cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "# Evaluate model\n",
        "# equal gives value of 0 or 1\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))# equal gives value of 0 or 1\n",
        "\n",
        "## 3 images, y_p=[[0.1,0.0,0,0.9],[0.9,0.1,0,0.],[0,0.9,0,0.1]] \n",
        "\n",
        "# tf.argmax(y_p, 1) [3,0,1] \n",
        "\n",
        "# 3 images, y=[[0,0.0,0,1],[0,1,0,0],[0,1,0,0]] \n",
        "\n",
        "# tf.argmax(y, 1) [3,1,1]\n",
        "\n",
        "# tf.equal [True,False,True]--[1,0,1]--- 2/3 \n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "tic = time.clock()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        # next_batch randomly sample out input and output data\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    #print('predicted ', y_p_p)\n",
        "    #print('real ', y_gr)\n",
        "\n",
        "\n",
        "\n",
        "toc = time.clock()\n",
        "toc-tic"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:  0 train_cost:  7.776774 train_acc:  0.078125 test_cost:  8.003578 test_acc:  0.0703125\n",
            "iter:  1 train_cost:  7.363817 train_acc:  0.09375 test_cost:  7.689084 test_acc:  0.0859375\n",
            "iter:  2 train_cost:  7.3448086 train_acc:  0.078125 test_cost:  7.3347645 test_acc:  0.109375\n",
            "iter:  3 train_cost:  7.994915 train_acc:  0.03125 test_cost:  7.1498027 test_acc:  0.1015625\n",
            "iter:  4 train_cost:  6.3591423 train_acc:  0.125 test_cost:  6.8996615 test_acc:  0.1015625\n",
            "iter:  5 train_cost:  6.3060822 train_acc:  0.078125 test_cost:  6.540539 test_acc:  0.09375\n",
            "iter:  6 train_cost:  6.0850754 train_acc:  0.09375 test_cost:  6.643146 test_acc:  0.125\n",
            "iter:  7 train_cost:  6.1605635 train_acc:  0.0859375 test_cost:  6.017409 test_acc:  0.0859375\n",
            "iter:  8 train_cost:  6.249713 train_acc:  0.109375 test_cost:  6.141265 test_acc:  0.078125\n",
            "iter:  9 train_cost:  6.2297144 train_acc:  0.0625 test_cost:  5.7736893 test_acc:  0.078125\n",
            "iter:  10 train_cost:  6.2364616 train_acc:  0.0859375 test_cost:  6.0684366 test_acc:  0.0859375\n",
            "iter:  11 train_cost:  5.2009068 train_acc:  0.0546875 test_cost:  5.544656 test_acc:  0.125\n",
            "iter:  12 train_cost:  5.6416755 train_acc:  0.0859375 test_cost:  5.6818867 test_acc:  0.078125\n",
            "iter:  13 train_cost:  5.4182253 train_acc:  0.0859375 test_cost:  6.1450777 test_acc:  0.0703125\n",
            "iter:  14 train_cost:  5.0877934 train_acc:  0.125 test_cost:  5.4505243 test_acc:  0.1015625\n",
            "iter:  15 train_cost:  4.620393 train_acc:  0.1171875 test_cost:  5.1914015 test_acc:  0.125\n",
            "iter:  16 train_cost:  5.165568 train_acc:  0.125 test_cost:  4.9867067 test_acc:  0.0703125\n",
            "iter:  17 train_cost:  4.852827 train_acc:  0.1015625 test_cost:  4.9483776 test_acc:  0.1171875\n",
            "iter:  18 train_cost:  5.0018435 train_acc:  0.140625 test_cost:  4.768748 test_acc:  0.1015625\n",
            "iter:  19 train_cost:  4.954608 train_acc:  0.0859375 test_cost:  5.373048 test_acc:  0.1171875\n",
            "iter:  20 train_cost:  4.390734 train_acc:  0.2109375 test_cost:  4.625742 test_acc:  0.1328125\n",
            "iter:  21 train_cost:  4.3520174 train_acc:  0.1484375 test_cost:  5.184664 test_acc:  0.1171875\n",
            "iter:  22 train_cost:  4.2487397 train_acc:  0.140625 test_cost:  4.5284944 test_acc:  0.1015625\n",
            "iter:  23 train_cost:  4.3029213 train_acc:  0.1484375 test_cost:  4.4848595 test_acc:  0.1171875\n",
            "iter:  24 train_cost:  4.1208816 train_acc:  0.1640625 test_cost:  4.4574637 test_acc:  0.1484375\n",
            "iter:  25 train_cost:  4.154107 train_acc:  0.125 test_cost:  4.416271 test_acc:  0.1015625\n",
            "iter:  26 train_cost:  4.01522 train_acc:  0.1484375 test_cost:  4.416547 test_acc:  0.140625\n",
            "iter:  27 train_cost:  4.0559883 train_acc:  0.1171875 test_cost:  3.9017835 test_acc:  0.1875\n",
            "iter:  28 train_cost:  3.8506436 train_acc:  0.125 test_cost:  3.8940744 test_acc:  0.1953125\n",
            "iter:  29 train_cost:  3.9138527 train_acc:  0.1640625 test_cost:  4.029615 test_acc:  0.1875\n",
            "iter:  30 train_cost:  4.157413 train_acc:  0.0859375 test_cost:  3.5401707 test_acc:  0.2109375\n",
            "iter:  31 train_cost:  4.101461 train_acc:  0.1328125 test_cost:  4.0045147 test_acc:  0.2109375\n",
            "iter:  32 train_cost:  3.933238 train_acc:  0.125 test_cost:  3.5305247 test_acc:  0.1953125\n",
            "iter:  33 train_cost:  3.2704573 train_acc:  0.1875 test_cost:  3.7824154 test_acc:  0.15625\n",
            "iter:  34 train_cost:  3.525855 train_acc:  0.21875 test_cost:  3.4285235 test_acc:  0.2890625\n",
            "iter:  35 train_cost:  4.2028356 train_acc:  0.15625 test_cost:  3.5641048 test_acc:  0.1953125\n",
            "iter:  36 train_cost:  3.77242 train_acc:  0.1875 test_cost:  3.2513907 test_acc:  0.203125\n",
            "iter:  37 train_cost:  3.6839447 train_acc:  0.21875 test_cost:  3.867134 test_acc:  0.171875\n",
            "iter:  38 train_cost:  3.6173632 train_acc:  0.1484375 test_cost:  3.689547 test_acc:  0.171875\n",
            "iter:  39 train_cost:  3.5686476 train_acc:  0.2109375 test_cost:  3.7793686 test_acc:  0.1953125\n",
            "iter:  40 train_cost:  3.5115395 train_acc:  0.1796875 test_cost:  3.4048195 test_acc:  0.171875\n",
            "iter:  41 train_cost:  3.440082 train_acc:  0.25 test_cost:  3.6309721 test_acc:  0.171875\n",
            "iter:  42 train_cost:  3.3174348 train_acc:  0.203125 test_cost:  3.2664063 test_acc:  0.203125\n",
            "iter:  43 train_cost:  3.378086 train_acc:  0.171875 test_cost:  3.4035864 test_acc:  0.21875\n",
            "iter:  44 train_cost:  3.2943969 train_acc:  0.25 test_cost:  3.5116892 test_acc:  0.1796875\n",
            "iter:  45 train_cost:  3.1337044 train_acc:  0.265625 test_cost:  3.4825413 test_acc:  0.1875\n",
            "iter:  46 train_cost:  3.3277798 train_acc:  0.2578125 test_cost:  3.059071 test_acc:  0.2265625\n",
            "iter:  47 train_cost:  3.3148303 train_acc:  0.2109375 test_cost:  3.0417442 test_acc:  0.2265625\n",
            "iter:  48 train_cost:  3.3306825 train_acc:  0.1875 test_cost:  3.4069846 test_acc:  0.1953125\n",
            "iter:  49 train_cost:  2.9998407 train_acc:  0.1953125 test_cost:  3.2281554 test_acc:  0.2421875\n",
            "iter:  50 train_cost:  3.3714592 train_acc:  0.171875 test_cost:  3.3914185 test_acc:  0.25\n",
            "iter:  51 train_cost:  2.922781 train_acc:  0.2421875 test_cost:  3.0746033 test_acc:  0.21875\n",
            "iter:  52 train_cost:  3.0163984 train_acc:  0.2421875 test_cost:  3.0999818 test_acc:  0.2421875\n",
            "iter:  53 train_cost:  2.8000243 train_acc:  0.265625 test_cost:  2.980957 test_acc:  0.2734375\n",
            "iter:  54 train_cost:  2.8084118 train_acc:  0.1953125 test_cost:  3.2866063 test_acc:  0.1953125\n",
            "iter:  55 train_cost:  3.0663726 train_acc:  0.25 test_cost:  2.71128 test_acc:  0.1953125\n",
            "iter:  56 train_cost:  2.9083748 train_acc:  0.265625 test_cost:  3.4369063 test_acc:  0.2109375\n",
            "iter:  57 train_cost:  2.6800358 train_acc:  0.3046875 test_cost:  3.135831 test_acc:  0.2421875\n",
            "iter:  58 train_cost:  3.1332045 train_acc:  0.2265625 test_cost:  2.6674705 test_acc:  0.2734375\n",
            "iter:  59 train_cost:  2.7246313 train_acc:  0.2890625 test_cost:  3.007411 test_acc:  0.28125\n",
            "iter:  60 train_cost:  3.3425925 train_acc:  0.2890625 test_cost:  2.6183743 test_acc:  0.375\n",
            "iter:  61 train_cost:  3.051668 train_acc:  0.2734375 test_cost:  2.6445608 test_acc:  0.28125\n",
            "iter:  62 train_cost:  2.6640003 train_acc:  0.3125 test_cost:  2.6570482 test_acc:  0.3046875\n",
            "iter:  63 train_cost:  3.0099683 train_acc:  0.2578125 test_cost:  2.6510742 test_acc:  0.2578125\n",
            "iter:  64 train_cost:  2.4112978 train_acc:  0.34375 test_cost:  2.3996427 test_acc:  0.328125\n",
            "iter:  65 train_cost:  2.700068 train_acc:  0.328125 test_cost:  2.2376041 test_acc:  0.3203125\n",
            "iter:  66 train_cost:  2.5893025 train_acc:  0.328125 test_cost:  2.7099595 test_acc:  0.3125\n",
            "iter:  67 train_cost:  2.7474136 train_acc:  0.2734375 test_cost:  2.7487476 test_acc:  0.265625\n",
            "iter:  68 train_cost:  2.4749105 train_acc:  0.3359375 test_cost:  2.6349256 test_acc:  0.3046875\n",
            "iter:  69 train_cost:  2.366488 train_acc:  0.3515625 test_cost:  2.46513 test_acc:  0.3125\n",
            "iter:  70 train_cost:  2.5240517 train_acc:  0.28125 test_cost:  2.5590866 test_acc:  0.3046875\n",
            "iter:  71 train_cost:  2.6129606 train_acc:  0.3203125 test_cost:  2.4825993 test_acc:  0.328125\n",
            "iter:  72 train_cost:  2.457824 train_acc:  0.3515625 test_cost:  2.752133 test_acc:  0.3359375\n",
            "iter:  73 train_cost:  2.8150039 train_acc:  0.2734375 test_cost:  2.6885686 test_acc:  0.3046875\n",
            "iter:  74 train_cost:  2.6923785 train_acc:  0.3046875 test_cost:  2.4186728 test_acc:  0.328125\n",
            "iter:  75 train_cost:  2.3450341 train_acc:  0.3125 test_cost:  2.8895206 test_acc:  0.265625\n",
            "iter:  76 train_cost:  2.3203743 train_acc:  0.3515625 test_cost:  2.1696641 test_acc:  0.3515625\n",
            "iter:  77 train_cost:  2.4385998 train_acc:  0.3203125 test_cost:  2.3600793 test_acc:  0.3203125\n",
            "iter:  78 train_cost:  2.4409375 train_acc:  0.3359375 test_cost:  2.4003654 test_acc:  0.3671875\n",
            "iter:  79 train_cost:  2.15679 train_acc:  0.359375 test_cost:  2.2354329 test_acc:  0.3515625\n",
            "iter:  80 train_cost:  2.565226 train_acc:  0.2734375 test_cost:  2.0881417 test_acc:  0.3046875\n",
            "iter:  81 train_cost:  2.2989058 train_acc:  0.3671875 test_cost:  2.29748 test_acc:  0.3828125\n",
            "iter:  82 train_cost:  2.3164482 train_acc:  0.3515625 test_cost:  2.482202 test_acc:  0.34375\n",
            "iter:  83 train_cost:  2.4925 train_acc:  0.34375 test_cost:  2.56619 test_acc:  0.2890625\n",
            "iter:  84 train_cost:  2.2039852 train_acc:  0.40625 test_cost:  2.5200367 test_acc:  0.2578125\n",
            "iter:  85 train_cost:  2.2626808 train_acc:  0.3671875 test_cost:  2.4280856 test_acc:  0.359375\n",
            "iter:  86 train_cost:  2.3057973 train_acc:  0.3828125 test_cost:  2.052731 test_acc:  0.421875\n",
            "iter:  87 train_cost:  2.311082 train_acc:  0.3515625 test_cost:  2.3858423 test_acc:  0.3359375\n",
            "iter:  88 train_cost:  1.9836357 train_acc:  0.4296875 test_cost:  2.060934 test_acc:  0.40625\n",
            "iter:  89 train_cost:  2.3508961 train_acc:  0.3984375 test_cost:  2.0354695 test_acc:  0.3984375\n",
            "iter:  90 train_cost:  1.9860932 train_acc:  0.4453125 test_cost:  2.064281 test_acc:  0.3984375\n",
            "iter:  91 train_cost:  2.1358783 train_acc:  0.390625 test_cost:  2.0667353 test_acc:  0.40625\n",
            "iter:  92 train_cost:  1.9768087 train_acc:  0.4453125 test_cost:  2.2550895 test_acc:  0.3359375\n",
            "iter:  93 train_cost:  2.0371008 train_acc:  0.4765625 test_cost:  2.0702777 test_acc:  0.4296875\n",
            "iter:  94 train_cost:  1.9502804 train_acc:  0.3828125 test_cost:  1.7185154 test_acc:  0.484375\n",
            "iter:  95 train_cost:  1.959373 train_acc:  0.4375 test_cost:  1.9444351 test_acc:  0.4375\n",
            "iter:  96 train_cost:  2.3003697 train_acc:  0.3515625 test_cost:  2.4142592 test_acc:  0.3125\n",
            "iter:  97 train_cost:  2.2192822 train_acc:  0.390625 test_cost:  2.1955013 test_acc:  0.4296875\n",
            "iter:  98 train_cost:  1.9093267 train_acc:  0.3984375 test_cost:  2.1502955 test_acc:  0.3828125\n",
            "iter:  99 train_cost:  2.0946784 train_acc:  0.4375 test_cost:  1.9502475 test_acc:  0.3828125\n",
            "iter:  100 train_cost:  1.7941015 train_acc:  0.484375 test_cost:  2.0533423 test_acc:  0.421875\n",
            "iter:  101 train_cost:  1.6995126 train_acc:  0.484375 test_cost:  2.0631897 test_acc:  0.421875\n",
            "iter:  102 train_cost:  2.2048554 train_acc:  0.390625 test_cost:  2.1507635 test_acc:  0.4609375\n",
            "iter:  103 train_cost:  2.0420873 train_acc:  0.3984375 test_cost:  2.34209 test_acc:  0.3984375\n",
            "iter:  104 train_cost:  2.2959657 train_acc:  0.390625 test_cost:  1.8600714 test_acc:  0.421875\n",
            "iter:  105 train_cost:  1.9409928 train_acc:  0.46875 test_cost:  2.2749503 test_acc:  0.390625\n",
            "iter:  106 train_cost:  1.6011027 train_acc:  0.546875 test_cost:  1.8255465 test_acc:  0.40625\n",
            "iter:  107 train_cost:  2.2031634 train_acc:  0.421875 test_cost:  1.8946912 test_acc:  0.3828125\n",
            "iter:  108 train_cost:  1.8836656 train_acc:  0.4765625 test_cost:  1.9832673 test_acc:  0.4453125\n",
            "iter:  109 train_cost:  1.8715637 train_acc:  0.46875 test_cost:  1.9812486 test_acc:  0.4453125\n",
            "iter:  110 train_cost:  1.96511 train_acc:  0.4375 test_cost:  1.7561729 test_acc:  0.5\n",
            "iter:  111 train_cost:  2.0276098 train_acc:  0.453125 test_cost:  2.3100805 test_acc:  0.3828125\n",
            "iter:  112 train_cost:  1.8101053 train_acc:  0.46875 test_cost:  1.6782736 test_acc:  0.53125\n",
            "iter:  113 train_cost:  1.7538621 train_acc:  0.4296875 test_cost:  1.8722286 test_acc:  0.4609375\n",
            "iter:  114 train_cost:  1.9353769 train_acc:  0.4765625 test_cost:  1.9976178 test_acc:  0.3671875\n",
            "iter:  115 train_cost:  1.811386 train_acc:  0.4765625 test_cost:  1.423699 test_acc:  0.53125\n",
            "iter:  116 train_cost:  1.582545 train_acc:  0.5078125 test_cost:  2.013339 test_acc:  0.4765625\n",
            "iter:  117 train_cost:  1.9060174 train_acc:  0.46875 test_cost:  2.0422592 test_acc:  0.4453125\n",
            "iter:  118 train_cost:  1.5828469 train_acc:  0.4609375 test_cost:  1.5092504 test_acc:  0.5546875\n",
            "iter:  119 train_cost:  1.7535465 train_acc:  0.4609375 test_cost:  1.5722992 test_acc:  0.53125\n",
            "iter:  120 train_cost:  1.7598133 train_acc:  0.4609375 test_cost:  1.6201134 test_acc:  0.4921875\n",
            "iter:  121 train_cost:  1.7470223 train_acc:  0.4765625 test_cost:  2.0593076 test_acc:  0.4296875\n",
            "iter:  122 train_cost:  1.9748256 train_acc:  0.390625 test_cost:  1.7731838 test_acc:  0.453125\n",
            "iter:  123 train_cost:  1.7653089 train_acc:  0.515625 test_cost:  1.7992007 test_acc:  0.4609375\n",
            "iter:  124 train_cost:  1.7050067 train_acc:  0.5 test_cost:  1.6630656 test_acc:  0.4609375\n",
            "iter:  125 train_cost:  1.6188527 train_acc:  0.5234375 test_cost:  1.6812389 test_acc:  0.4609375\n",
            "iter:  126 train_cost:  1.8873174 train_acc:  0.46875 test_cost:  1.734594 test_acc:  0.5078125\n",
            "iter:  127 train_cost:  1.9890645 train_acc:  0.453125 test_cost:  1.5901394 test_acc:  0.4921875\n",
            "iter:  128 train_cost:  1.8650541 train_acc:  0.4453125 test_cost:  1.2996395 test_acc:  0.5859375\n",
            "iter:  129 train_cost:  1.858488 train_acc:  0.4453125 test_cost:  1.7285805 test_acc:  0.40625\n",
            "iter:  130 train_cost:  1.5553775 train_acc:  0.5234375 test_cost:  1.6385937 test_acc:  0.5625\n",
            "iter:  131 train_cost:  1.8965816 train_acc:  0.4453125 test_cost:  1.7101254 test_acc:  0.4453125\n",
            "iter:  132 train_cost:  2.0342798 train_acc:  0.4765625 test_cost:  1.7930225 test_acc:  0.5\n",
            "iter:  133 train_cost:  1.6715038 train_acc:  0.4921875 test_cost:  1.5301836 test_acc:  0.5078125\n",
            "iter:  134 train_cost:  1.5884898 train_acc:  0.53125 test_cost:  1.5743256 test_acc:  0.5390625\n",
            "iter:  135 train_cost:  1.5715423 train_acc:  0.5 test_cost:  1.8372701 test_acc:  0.4921875\n",
            "iter:  136 train_cost:  1.7582326 train_acc:  0.484375 test_cost:  1.5401185 test_acc:  0.5859375\n",
            "iter:  137 train_cost:  1.5512925 train_acc:  0.4921875 test_cost:  1.8146102 test_acc:  0.4453125\n",
            "iter:  138 train_cost:  2.110416 train_acc:  0.375 test_cost:  1.561862 test_acc:  0.4921875\n",
            "iter:  139 train_cost:  1.8991888 train_acc:  0.46875 test_cost:  1.7370158 test_acc:  0.5546875\n",
            "iter:  140 train_cost:  1.6836824 train_acc:  0.484375 test_cost:  1.3529272 test_acc:  0.6171875\n",
            "iter:  141 train_cost:  1.6882812 train_acc:  0.4921875 test_cost:  1.5698855 test_acc:  0.53125\n",
            "iter:  142 train_cost:  1.4742332 train_acc:  0.515625 test_cost:  1.3449655 test_acc:  0.6171875\n",
            "iter:  143 train_cost:  1.266841 train_acc:  0.6015625 test_cost:  1.537319 test_acc:  0.515625\n",
            "iter:  144 train_cost:  1.3726275 train_acc:  0.5546875 test_cost:  1.4605877 test_acc:  0.6015625\n",
            "iter:  145 train_cost:  1.4692625 train_acc:  0.5546875 test_cost:  1.6078314 test_acc:  0.546875\n",
            "iter:  146 train_cost:  1.2534568 train_acc:  0.609375 test_cost:  1.6687937 test_acc:  0.5078125\n",
            "iter:  147 train_cost:  1.7492056 train_acc:  0.5234375 test_cost:  1.3913089 test_acc:  0.59375\n",
            "iter:  148 train_cost:  1.3684858 train_acc:  0.5703125 test_cost:  1.6244352 test_acc:  0.515625\n",
            "iter:  149 train_cost:  1.4504552 train_acc:  0.5703125 test_cost:  1.744842 test_acc:  0.4765625\n",
            "iter:  150 train_cost:  1.4223046 train_acc:  0.5625 test_cost:  1.5020148 test_acc:  0.578125\n",
            "iter:  151 train_cost:  1.6119611 train_acc:  0.5 test_cost:  1.4723473 test_acc:  0.4765625\n",
            "iter:  152 train_cost:  1.3698977 train_acc:  0.5859375 test_cost:  1.4003212 test_acc:  0.546875\n",
            "iter:  153 train_cost:  1.3989531 train_acc:  0.5390625 test_cost:  1.565819 test_acc:  0.5625\n",
            "iter:  154 train_cost:  1.5556111 train_acc:  0.5390625 test_cost:  1.5404558 test_acc:  0.5390625\n",
            "iter:  155 train_cost:  1.4628878 train_acc:  0.5859375 test_cost:  1.4868233 test_acc:  0.5625\n",
            "iter:  156 train_cost:  1.4319491 train_acc:  0.5546875 test_cost:  1.4475839 test_acc:  0.546875\n",
            "iter:  157 train_cost:  1.4592854 train_acc:  0.546875 test_cost:  1.434799 test_acc:  0.609375\n",
            "iter:  158 train_cost:  1.3966159 train_acc:  0.5625 test_cost:  1.3428087 test_acc:  0.5703125\n",
            "iter:  159 train_cost:  1.5175791 train_acc:  0.5546875 test_cost:  1.5220202 test_acc:  0.5546875\n",
            "iter:  160 train_cost:  1.7252676 train_acc:  0.4609375 test_cost:  1.4071784 test_acc:  0.515625\n",
            "iter:  161 train_cost:  1.6094598 train_acc:  0.5234375 test_cost:  1.5614152 test_acc:  0.578125\n",
            "iter:  162 train_cost:  1.2310053 train_acc:  0.6484375 test_cost:  1.8606565 test_acc:  0.5078125\n",
            "iter:  163 train_cost:  1.6660097 train_acc:  0.5234375 test_cost:  1.5988193 test_acc:  0.5390625\n",
            "iter:  164 train_cost:  1.4723363 train_acc:  0.5703125 test_cost:  1.1209034 test_acc:  0.6484375\n",
            "iter:  165 train_cost:  1.3322543 train_acc:  0.5859375 test_cost:  1.6018882 test_acc:  0.5625\n",
            "iter:  166 train_cost:  1.140595 train_acc:  0.625 test_cost:  1.3547492 test_acc:  0.5703125\n",
            "iter:  167 train_cost:  1.2747588 train_acc:  0.515625 test_cost:  1.3558708 test_acc:  0.5546875\n",
            "iter:  168 train_cost:  1.4522114 train_acc:  0.5390625 test_cost:  1.4348955 test_acc:  0.53125\n",
            "iter:  169 train_cost:  1.4632151 train_acc:  0.515625 test_cost:  1.4428477 test_acc:  0.5859375\n",
            "iter:  170 train_cost:  1.2653452 train_acc:  0.609375 test_cost:  1.3138969 test_acc:  0.5703125\n",
            "iter:  171 train_cost:  1.209938 train_acc:  0.59375 test_cost:  1.3009083 test_acc:  0.5625\n",
            "iter:  172 train_cost:  1.3797891 train_acc:  0.640625 test_cost:  1.4233837 test_acc:  0.609375\n",
            "iter:  173 train_cost:  1.5191917 train_acc:  0.5546875 test_cost:  1.5228019 test_acc:  0.5625\n",
            "iter:  174 train_cost:  1.4808633 train_acc:  0.5625 test_cost:  1.4470911 test_acc:  0.5703125\n",
            "iter:  175 train_cost:  1.7233715 train_acc:  0.46875 test_cost:  1.4737309 test_acc:  0.515625\n",
            "iter:  176 train_cost:  1.5372008 train_acc:  0.5390625 test_cost:  1.2183357 test_acc:  0.609375\n",
            "iter:  177 train_cost:  1.3613827 train_acc:  0.5859375 test_cost:  1.22242 test_acc:  0.6328125\n",
            "iter:  178 train_cost:  1.2376112 train_acc:  0.640625 test_cost:  1.2726424 test_acc:  0.59375\n",
            "iter:  179 train_cost:  1.5120188 train_acc:  0.6015625 test_cost:  1.4006728 test_acc:  0.5625\n",
            "iter:  180 train_cost:  1.5815094 train_acc:  0.5234375 test_cost:  1.1196923 test_acc:  0.6796875\n",
            "iter:  181 train_cost:  1.3381745 train_acc:  0.5859375 test_cost:  1.3831818 test_acc:  0.59375\n",
            "iter:  182 train_cost:  1.3933761 train_acc:  0.6171875 test_cost:  1.1697654 test_acc:  0.609375\n",
            "iter:  183 train_cost:  1.2950493 train_acc:  0.6171875 test_cost:  1.2680279 test_acc:  0.5625\n",
            "iter:  184 train_cost:  1.3127723 train_acc:  0.6640625 test_cost:  1.4219766 test_acc:  0.59375\n",
            "iter:  185 train_cost:  1.2745987 train_acc:  0.65625 test_cost:  1.4937522 test_acc:  0.546875\n",
            "iter:  186 train_cost:  1.5215333 train_acc:  0.5546875 test_cost:  0.9782067 test_acc:  0.671875\n",
            "iter:  187 train_cost:  1.2492986 train_acc:  0.6328125 test_cost:  1.4822571 test_acc:  0.59375\n",
            "iter:  188 train_cost:  1.1456281 train_acc:  0.609375 test_cost:  1.3792417 test_acc:  0.6171875\n",
            "iter:  189 train_cost:  1.2419367 train_acc:  0.640625 test_cost:  1.3741549 test_acc:  0.59375\n",
            "iter:  190 train_cost:  1.2283033 train_acc:  0.6171875 test_cost:  1.1899304 test_acc:  0.6640625\n",
            "iter:  191 train_cost:  1.4120078 train_acc:  0.5546875 test_cost:  1.0766932 test_acc:  0.6796875\n",
            "iter:  192 train_cost:  1.4072576 train_acc:  0.578125 test_cost:  1.4395001 test_acc:  0.5859375\n",
            "iter:  193 train_cost:  1.2949526 train_acc:  0.6015625 test_cost:  1.2710626 test_acc:  0.5703125\n",
            "iter:  194 train_cost:  1.3977236 train_acc:  0.6015625 test_cost:  1.3912395 test_acc:  0.6015625\n",
            "iter:  195 train_cost:  0.97677433 train_acc:  0.6953125 test_cost:  1.4235579 test_acc:  0.578125\n",
            "iter:  196 train_cost:  1.1779548 train_acc:  0.6640625 test_cost:  1.3711267 test_acc:  0.6171875\n",
            "iter:  197 train_cost:  1.1579094 train_acc:  0.6171875 test_cost:  1.2827631 test_acc:  0.5625\n",
            "iter:  198 train_cost:  1.2491481 train_acc:  0.640625 test_cost:  0.97251654 test_acc:  0.6484375\n",
            "iter:  199 train_cost:  1.1100559 train_acc:  0.6328125 test_cost:  1.1666656 test_acc:  0.59375\n",
            "iter:  200 train_cost:  1.2111771 train_acc:  0.6171875 test_cost:  1.1781421 test_acc:  0.609375\n",
            "iter:  201 train_cost:  1.3092213 train_acc:  0.59375 test_cost:  1.3322071 test_acc:  0.59375\n",
            "iter:  202 train_cost:  1.2238128 train_acc:  0.6328125 test_cost:  1.462661 test_acc:  0.5859375\n",
            "iter:  203 train_cost:  1.2387885 train_acc:  0.6015625 test_cost:  1.2835932 test_acc:  0.6015625\n",
            "iter:  204 train_cost:  1.427805 train_acc:  0.5703125 test_cost:  1.2650256 test_acc:  0.6328125\n",
            "iter:  205 train_cost:  1.3271185 train_acc:  0.5859375 test_cost:  1.0476543 test_acc:  0.6640625\n",
            "iter:  206 train_cost:  1.2960776 train_acc:  0.6171875 test_cost:  1.3223231 test_acc:  0.59375\n",
            "iter:  207 train_cost:  1.2870972 train_acc:  0.65625 test_cost:  1.3085277 test_acc:  0.6015625\n",
            "iter:  208 train_cost:  1.1669779 train_acc:  0.6015625 test_cost:  1.2275972 test_acc:  0.65625\n",
            "iter:  209 train_cost:  1.0547744 train_acc:  0.6875 test_cost:  1.2321583 test_acc:  0.578125\n",
            "iter:  210 train_cost:  1.3385303 train_acc:  0.59375 test_cost:  1.2069572 test_acc:  0.625\n",
            "iter:  211 train_cost:  1.2653037 train_acc:  0.6171875 test_cost:  1.1555662 test_acc:  0.625\n",
            "iter:  212 train_cost:  1.1490077 train_acc:  0.59375 test_cost:  1.4051526 test_acc:  0.609375\n",
            "iter:  213 train_cost:  1.292247 train_acc:  0.578125 test_cost:  1.2446902 test_acc:  0.671875\n",
            "iter:  214 train_cost:  1.1146673 train_acc:  0.65625 test_cost:  1.0814254 test_acc:  0.6328125\n",
            "iter:  215 train_cost:  1.3466825 train_acc:  0.5859375 test_cost:  1.3110571 test_acc:  0.6171875\n",
            "iter:  216 train_cost:  1.4170223 train_acc:  0.59375 test_cost:  1.3373722 test_acc:  0.5703125\n",
            "iter:  217 train_cost:  1.1065729 train_acc:  0.6484375 test_cost:  1.2031085 test_acc:  0.6640625\n",
            "iter:  218 train_cost:  1.1791346 train_acc:  0.6484375 test_cost:  1.0339329 test_acc:  0.671875\n",
            "iter:  219 train_cost:  1.313822 train_acc:  0.640625 test_cost:  1.4000072 test_acc:  0.515625\n",
            "iter:  220 train_cost:  1.3116633 train_acc:  0.6171875 test_cost:  1.0968463 test_acc:  0.6484375\n",
            "iter:  221 train_cost:  0.8717912 train_acc:  0.7109375 test_cost:  1.0302529 test_acc:  0.6953125\n",
            "iter:  222 train_cost:  1.498197 train_acc:  0.5703125 test_cost:  1.3187131 test_acc:  0.625\n",
            "iter:  223 train_cost:  1.0673039 train_acc:  0.6875 test_cost:  1.1056669 test_acc:  0.65625\n",
            "iter:  224 train_cost:  1.2374332 train_acc:  0.6171875 test_cost:  1.080081 test_acc:  0.6640625\n",
            "iter:  225 train_cost:  1.3077844 train_acc:  0.609375 test_cost:  0.96409035 test_acc:  0.6640625\n",
            "iter:  226 train_cost:  1.1072009 train_acc:  0.6640625 test_cost:  1.3561754 test_acc:  0.6171875\n",
            "iter:  227 train_cost:  1.0765984 train_acc:  0.6328125 test_cost:  1.2372322 test_acc:  0.625\n",
            "iter:  228 train_cost:  1.2043548 train_acc:  0.5546875 test_cost:  1.1966802 test_acc:  0.6171875\n",
            "iter:  229 train_cost:  0.9494597 train_acc:  0.6640625 test_cost:  1.2319884 test_acc:  0.6015625\n",
            "iter:  230 train_cost:  1.5720677 train_acc:  0.515625 test_cost:  1.1191692 test_acc:  0.6328125\n",
            "iter:  231 train_cost:  1.2655803 train_acc:  0.5859375 test_cost:  0.944723 test_acc:  0.6953125\n",
            "iter:  232 train_cost:  1.227185 train_acc:  0.65625 test_cost:  1.0389464 test_acc:  0.65625\n",
            "iter:  233 train_cost:  1.1252232 train_acc:  0.640625 test_cost:  1.1515809 test_acc:  0.6171875\n",
            "iter:  234 train_cost:  1.0916693 train_acc:  0.640625 test_cost:  0.9455405 test_acc:  0.671875\n",
            "iter:  235 train_cost:  0.9830662 train_acc:  0.640625 test_cost:  0.81055135 test_acc:  0.703125\n",
            "iter:  236 train_cost:  1.0961332 train_acc:  0.6328125 test_cost:  1.046357 test_acc:  0.6953125\n",
            "iter:  237 train_cost:  1.1986232 train_acc:  0.6171875 test_cost:  1.2337732 test_acc:  0.6484375\n",
            "iter:  238 train_cost:  1.047385 train_acc:  0.6875 test_cost:  1.0993128 test_acc:  0.65625\n",
            "iter:  239 train_cost:  1.2944492 train_acc:  0.6328125 test_cost:  1.0038381 test_acc:  0.6796875\n",
            "iter:  240 train_cost:  1.3484275 train_acc:  0.546875 test_cost:  1.1512555 test_acc:  0.640625\n",
            "iter:  241 train_cost:  1.1795336 train_acc:  0.640625 test_cost:  0.8292406 test_acc:  0.7265625\n",
            "iter:  242 train_cost:  1.2699848 train_acc:  0.609375 test_cost:  1.2288052 test_acc:  0.5859375\n",
            "iter:  243 train_cost:  1.2328229 train_acc:  0.6484375 test_cost:  1.0933663 test_acc:  0.6484375\n",
            "iter:  244 train_cost:  1.1739643 train_acc:  0.625 test_cost:  1.1349297 test_acc:  0.6484375\n",
            "iter:  245 train_cost:  1.0142348 train_acc:  0.625 test_cost:  1.1160917 test_acc:  0.6640625\n",
            "iter:  246 train_cost:  1.3690069 train_acc:  0.59375 test_cost:  1.0653737 test_acc:  0.6484375\n",
            "iter:  247 train_cost:  1.2748723 train_acc:  0.640625 test_cost:  1.2107935 test_acc:  0.609375\n",
            "iter:  248 train_cost:  1.0661017 train_acc:  0.6171875 test_cost:  1.0571193 test_acc:  0.640625\n",
            "iter:  249 train_cost:  1.2134683 train_acc:  0.59375 test_cost:  1.2740595 test_acc:  0.6484375\n",
            "iter:  250 train_cost:  1.0565634 train_acc:  0.6796875 test_cost:  0.92363846 test_acc:  0.703125\n",
            "iter:  251 train_cost:  1.220184 train_acc:  0.6328125 test_cost:  1.195972 test_acc:  0.6328125\n",
            "iter:  252 train_cost:  1.055765 train_acc:  0.6484375 test_cost:  0.93400997 test_acc:  0.671875\n",
            "iter:  253 train_cost:  1.0278448 train_acc:  0.6640625 test_cost:  1.1164453 test_acc:  0.6796875\n",
            "iter:  254 train_cost:  0.8276037 train_acc:  0.703125 test_cost:  0.9795027 test_acc:  0.6640625\n",
            "iter:  255 train_cost:  1.0507482 train_acc:  0.6796875 test_cost:  1.0626869 test_acc:  0.6484375\n",
            "iter:  256 train_cost:  1.0215722 train_acc:  0.6640625 test_cost:  0.9316796 test_acc:  0.7421875\n",
            "iter:  257 train_cost:  1.0026045 train_acc:  0.6875 test_cost:  1.2784793 test_acc:  0.6171875\n",
            "iter:  258 train_cost:  1.2011349 train_acc:  0.640625 test_cost:  1.1409593 test_acc:  0.6015625\n",
            "iter:  259 train_cost:  1.1773853 train_acc:  0.640625 test_cost:  0.95662785 test_acc:  0.6953125\n",
            "iter:  260 train_cost:  0.91055596 train_acc:  0.71875 test_cost:  1.0576138 test_acc:  0.6640625\n",
            "iter:  261 train_cost:  1.2507929 train_acc:  0.6171875 test_cost:  1.0141797 test_acc:  0.6640625\n",
            "iter:  262 train_cost:  1.1672305 train_acc:  0.671875 test_cost:  0.892032 test_acc:  0.6953125\n",
            "iter:  263 train_cost:  1.0816131 train_acc:  0.6953125 test_cost:  0.88759243 test_acc:  0.7265625\n",
            "iter:  264 train_cost:  1.0629228 train_acc:  0.6953125 test_cost:  1.1025789 test_acc:  0.65625\n",
            "iter:  265 train_cost:  1.1380519 train_acc:  0.6796875 test_cost:  1.3042191 test_acc:  0.6171875\n",
            "iter:  266 train_cost:  0.90949 train_acc:  0.7734375 test_cost:  1.1423955 test_acc:  0.6796875\n",
            "iter:  267 train_cost:  0.89890826 train_acc:  0.65625 test_cost:  1.1787504 test_acc:  0.671875\n",
            "iter:  268 train_cost:  1.1089171 train_acc:  0.6484375 test_cost:  1.0737207 test_acc:  0.671875\n",
            "iter:  269 train_cost:  0.8725851 train_acc:  0.75 test_cost:  1.1211481 test_acc:  0.6875\n",
            "iter:  270 train_cost:  1.4081867 train_acc:  0.6171875 test_cost:  1.0165067 test_acc:  0.703125\n",
            "iter:  271 train_cost:  1.09992 train_acc:  0.6875 test_cost:  0.98616993 test_acc:  0.6875\n",
            "iter:  272 train_cost:  1.2986321 train_acc:  0.59375 test_cost:  1.0020759 test_acc:  0.6796875\n",
            "iter:  273 train_cost:  1.0856087 train_acc:  0.6484375 test_cost:  0.9876536 test_acc:  0.671875\n",
            "iter:  274 train_cost:  0.8812812 train_acc:  0.7265625 test_cost:  1.0750152 test_acc:  0.6875\n",
            "iter:  275 train_cost:  0.83470225 train_acc:  0.7265625 test_cost:  1.0755516 test_acc:  0.640625\n",
            "iter:  276 train_cost:  1.385735 train_acc:  0.59375 test_cost:  1.213759 test_acc:  0.6015625\n",
            "iter:  277 train_cost:  0.89249164 train_acc:  0.7421875 test_cost:  0.8983769 test_acc:  0.6953125\n",
            "iter:  278 train_cost:  0.99979836 train_acc:  0.6796875 test_cost:  1.0830249 test_acc:  0.6640625\n",
            "iter:  279 train_cost:  0.91448605 train_acc:  0.7109375 test_cost:  1.1297191 test_acc:  0.6640625\n",
            "iter:  280 train_cost:  1.0563798 train_acc:  0.65625 test_cost:  1.032142 test_acc:  0.6796875\n",
            "iter:  281 train_cost:  1.1105887 train_acc:  0.6640625 test_cost:  0.9568272 test_acc:  0.7109375\n",
            "iter:  282 train_cost:  1.0094899 train_acc:  0.65625 test_cost:  0.97061706 test_acc:  0.6953125\n",
            "iter:  283 train_cost:  0.92651427 train_acc:  0.65625 test_cost:  1.035661 test_acc:  0.703125\n",
            "iter:  284 train_cost:  0.96330833 train_acc:  0.703125 test_cost:  1.1202796 test_acc:  0.6640625\n",
            "iter:  285 train_cost:  0.8775387 train_acc:  0.7265625 test_cost:  0.9485855 test_acc:  0.671875\n",
            "iter:  286 train_cost:  1.1303792 train_acc:  0.6328125 test_cost:  0.9748513 test_acc:  0.703125\n",
            "iter:  287 train_cost:  1.2951037 train_acc:  0.6484375 test_cost:  1.119653 test_acc:  0.6484375\n",
            "iter:  288 train_cost:  1.0831093 train_acc:  0.6875 test_cost:  1.0443594 test_acc:  0.65625\n",
            "iter:  289 train_cost:  0.97947073 train_acc:  0.6796875 test_cost:  0.93199855 test_acc:  0.71875\n",
            "iter:  290 train_cost:  0.752112 train_acc:  0.6953125 test_cost:  1.0511543 test_acc:  0.7109375\n",
            "iter:  291 train_cost:  1.0169026 train_acc:  0.71875 test_cost:  0.9808873 test_acc:  0.671875\n",
            "iter:  292 train_cost:  0.9331466 train_acc:  0.6953125 test_cost:  1.0451438 test_acc:  0.6875\n",
            "iter:  293 train_cost:  0.9954903 train_acc:  0.671875 test_cost:  1.1000525 test_acc:  0.671875\n",
            "iter:  294 train_cost:  1.0118173 train_acc:  0.65625 test_cost:  0.99835 test_acc:  0.6953125\n",
            "iter:  295 train_cost:  1.1403794 train_acc:  0.6484375 test_cost:  0.993216 test_acc:  0.6328125\n",
            "iter:  296 train_cost:  1.2308602 train_acc:  0.6328125 test_cost:  1.1677012 test_acc:  0.6484375\n",
            "iter:  297 train_cost:  1.0011283 train_acc:  0.6796875 test_cost:  0.71407294 test_acc:  0.7890625\n",
            "iter:  298 train_cost:  0.91001266 train_acc:  0.734375 test_cost:  1.0546203 test_acc:  0.6875\n",
            "iter:  299 train_cost:  1.1385784 train_acc:  0.6640625 test_cost:  0.86550957 test_acc:  0.6875\n",
            "iter:  300 train_cost:  0.91671515 train_acc:  0.703125 test_cost:  0.7967242 test_acc:  0.734375\n",
            "iter:  301 train_cost:  0.98110634 train_acc:  0.7265625 test_cost:  0.81252 test_acc:  0.7578125\n",
            "iter:  302 train_cost:  0.97503984 train_acc:  0.6875 test_cost:  1.0195396 test_acc:  0.6796875\n",
            "iter:  303 train_cost:  0.88836145 train_acc:  0.703125 test_cost:  0.8865497 test_acc:  0.703125\n",
            "iter:  304 train_cost:  0.93598646 train_acc:  0.7265625 test_cost:  1.0945432 test_acc:  0.6796875\n",
            "iter:  305 train_cost:  1.1669631 train_acc:  0.6640625 test_cost:  0.85438794 test_acc:  0.7265625\n",
            "iter:  306 train_cost:  0.9095941 train_acc:  0.6875 test_cost:  0.89860153 test_acc:  0.671875\n",
            "iter:  307 train_cost:  1.024545 train_acc:  0.671875 test_cost:  0.8491665 test_acc:  0.75\n",
            "iter:  308 train_cost:  0.96025157 train_acc:  0.7109375 test_cost:  0.9816822 test_acc:  0.703125\n",
            "iter:  309 train_cost:  0.9734846 train_acc:  0.6796875 test_cost:  0.905465 test_acc:  0.734375\n",
            "iter:  310 train_cost:  1.1687291 train_acc:  0.6015625 test_cost:  1.0912789 test_acc:  0.6796875\n",
            "iter:  311 train_cost:  0.66678166 train_acc:  0.734375 test_cost:  0.8472191 test_acc:  0.765625\n",
            "iter:  312 train_cost:  1.1937706 train_acc:  0.6953125 test_cost:  1.0195217 test_acc:  0.6484375\n",
            "iter:  313 train_cost:  0.92970747 train_acc:  0.7421875 test_cost:  1.0874571 test_acc:  0.6640625\n",
            "iter:  314 train_cost:  0.9034231 train_acc:  0.6953125 test_cost:  1.0684193 test_acc:  0.65625\n",
            "iter:  315 train_cost:  1.0816108 train_acc:  0.6171875 test_cost:  0.8884851 test_acc:  0.734375\n",
            "iter:  316 train_cost:  0.80855656 train_acc:  0.71875 test_cost:  0.8917093 test_acc:  0.734375\n",
            "iter:  317 train_cost:  0.96079326 train_acc:  0.671875 test_cost:  0.8469558 test_acc:  0.71875\n",
            "iter:  318 train_cost:  0.990415 train_acc:  0.78125 test_cost:  1.0251508 test_acc:  0.7109375\n",
            "iter:  319 train_cost:  0.99071026 train_acc:  0.71875 test_cost:  0.90659106 test_acc:  0.71875\n",
            "iter:  320 train_cost:  0.92309666 train_acc:  0.71875 test_cost:  0.7718517 test_acc:  0.7734375\n",
            "iter:  321 train_cost:  1.0672274 train_acc:  0.6875 test_cost:  1.037736 test_acc:  0.734375\n",
            "iter:  322 train_cost:  0.7445587 train_acc:  0.78125 test_cost:  0.9845698 test_acc:  0.671875\n",
            "iter:  323 train_cost:  1.0244073 train_acc:  0.6796875 test_cost:  0.7739728 test_acc:  0.7734375\n",
            "iter:  324 train_cost:  1.0552591 train_acc:  0.703125 test_cost:  0.6937803 test_acc:  0.75\n",
            "iter:  325 train_cost:  0.66985637 train_acc:  0.8203125 test_cost:  0.8980185 test_acc:  0.765625\n",
            "iter:  326 train_cost:  0.8004539 train_acc:  0.734375 test_cost:  0.6802623 test_acc:  0.8125\n",
            "iter:  327 train_cost:  1.0346026 train_acc:  0.7109375 test_cost:  0.80665356 test_acc:  0.7421875\n",
            "iter:  328 train_cost:  0.98481065 train_acc:  0.65625 test_cost:  0.8657125 test_acc:  0.6953125\n",
            "iter:  329 train_cost:  1.0032912 train_acc:  0.734375 test_cost:  1.040642 test_acc:  0.6875\n",
            "iter:  330 train_cost:  0.9904919 train_acc:  0.6953125 test_cost:  0.5820163 test_acc:  0.8046875\n",
            "iter:  331 train_cost:  0.9063896 train_acc:  0.6875 test_cost:  0.819535 test_acc:  0.6796875\n",
            "iter:  332 train_cost:  0.84910995 train_acc:  0.765625 test_cost:  0.89629614 test_acc:  0.734375\n",
            "iter:  333 train_cost:  0.7737676 train_acc:  0.7578125 test_cost:  0.9834978 test_acc:  0.703125\n",
            "iter:  334 train_cost:  1.1202991 train_acc:  0.6171875 test_cost:  1.0169129 test_acc:  0.6953125\n",
            "iter:  335 train_cost:  1.0923736 train_acc:  0.6796875 test_cost:  1.1641719 test_acc:  0.6640625\n",
            "iter:  336 train_cost:  1.2532263 train_acc:  0.6953125 test_cost:  0.74496156 test_acc:  0.7421875\n",
            "iter:  337 train_cost:  1.0287906 train_acc:  0.6796875 test_cost:  0.78669214 test_acc:  0.7578125\n",
            "iter:  338 train_cost:  0.8646903 train_acc:  0.7109375 test_cost:  0.8866614 test_acc:  0.6875\n",
            "iter:  339 train_cost:  0.8998566 train_acc:  0.65625 test_cost:  1.066106 test_acc:  0.6953125\n",
            "iter:  340 train_cost:  0.8744555 train_acc:  0.75 test_cost:  0.8663523 test_acc:  0.71875\n",
            "iter:  341 train_cost:  1.060072 train_acc:  0.6875 test_cost:  0.7631854 test_acc:  0.75\n",
            "iter:  342 train_cost:  0.8442364 train_acc:  0.734375 test_cost:  0.94853306 test_acc:  0.71875\n",
            "iter:  343 train_cost:  1.119302 train_acc:  0.65625 test_cost:  0.8580585 test_acc:  0.78125\n",
            "iter:  344 train_cost:  0.97833556 train_acc:  0.6640625 test_cost:  1.0519629 test_acc:  0.7265625\n",
            "iter:  345 train_cost:  0.89340127 train_acc:  0.7109375 test_cost:  0.92277044 test_acc:  0.7109375\n",
            "iter:  346 train_cost:  0.7309429 train_acc:  0.7265625 test_cost:  0.74685305 test_acc:  0.7890625\n",
            "iter:  347 train_cost:  0.97068894 train_acc:  0.6640625 test_cost:  0.92920256 test_acc:  0.7109375\n",
            "iter:  348 train_cost:  0.7887481 train_acc:  0.765625 test_cost:  0.83080965 test_acc:  0.734375\n",
            "iter:  349 train_cost:  0.9294827 train_acc:  0.7265625 test_cost:  0.9928546 test_acc:  0.6796875\n",
            "iter:  350 train_cost:  0.9146928 train_acc:  0.765625 test_cost:  0.79762435 test_acc:  0.7265625\n",
            "iter:  351 train_cost:  0.8673363 train_acc:  0.7109375 test_cost:  0.7713584 test_acc:  0.734375\n",
            "iter:  352 train_cost:  0.74106777 train_acc:  0.8046875 test_cost:  0.84707713 test_acc:  0.6953125\n",
            "iter:  353 train_cost:  0.7295747 train_acc:  0.7265625 test_cost:  0.8323019 test_acc:  0.7265625\n",
            "iter:  354 train_cost:  0.9068728 train_acc:  0.7109375 test_cost:  0.8061317 test_acc:  0.7578125\n",
            "iter:  355 train_cost:  0.8119163 train_acc:  0.734375 test_cost:  0.98123 test_acc:  0.671875\n",
            "iter:  356 train_cost:  0.8046478 train_acc:  0.703125 test_cost:  0.8003086 test_acc:  0.7578125\n",
            "iter:  357 train_cost:  0.8496623 train_acc:  0.796875 test_cost:  0.9116783 test_acc:  0.71875\n",
            "iter:  358 train_cost:  0.7731106 train_acc:  0.7578125 test_cost:  0.8176105 test_acc:  0.7578125\n",
            "iter:  359 train_cost:  0.88169336 train_acc:  0.6953125 test_cost:  0.7650393 test_acc:  0.75\n",
            "iter:  360 train_cost:  1.0168357 train_acc:  0.6796875 test_cost:  0.8846385 test_acc:  0.7265625\n",
            "iter:  361 train_cost:  0.9466068 train_acc:  0.6953125 test_cost:  0.6190131 test_acc:  0.7890625\n",
            "iter:  362 train_cost:  0.8716048 train_acc:  0.6953125 test_cost:  0.8942502 test_acc:  0.734375\n",
            "iter:  363 train_cost:  0.8585185 train_acc:  0.75 test_cost:  0.8009889 test_acc:  0.75\n",
            "iter:  364 train_cost:  0.9805727 train_acc:  0.734375 test_cost:  1.0098014 test_acc:  0.6796875\n",
            "iter:  365 train_cost:  1.2061784 train_acc:  0.609375 test_cost:  0.8341597 test_acc:  0.7421875\n",
            "iter:  366 train_cost:  0.8258052 train_acc:  0.71875 test_cost:  0.72936594 test_acc:  0.828125\n",
            "iter:  367 train_cost:  0.856576 train_acc:  0.71875 test_cost:  0.7640301 test_acc:  0.7421875\n",
            "iter:  368 train_cost:  0.69387686 train_acc:  0.8046875 test_cost:  0.8201946 test_acc:  0.7421875\n",
            "iter:  369 train_cost:  0.8097669 train_acc:  0.734375 test_cost:  0.8747262 test_acc:  0.7265625\n",
            "iter:  370 train_cost:  0.75475 train_acc:  0.734375 test_cost:  0.7926588 test_acc:  0.75\n",
            "iter:  371 train_cost:  1.1265669 train_acc:  0.65625 test_cost:  0.8842722 test_acc:  0.71875\n",
            "iter:  372 train_cost:  0.73401487 train_acc:  0.7734375 test_cost:  0.90253365 test_acc:  0.75\n",
            "iter:  373 train_cost:  0.81953615 train_acc:  0.71875 test_cost:  1.013671 test_acc:  0.640625\n",
            "iter:  374 train_cost:  0.67938936 train_acc:  0.7421875 test_cost:  0.78783935 test_acc:  0.796875\n",
            "iter:  375 train_cost:  0.87460583 train_acc:  0.7265625 test_cost:  0.86705357 test_acc:  0.7109375\n",
            "iter:  376 train_cost:  0.81383467 train_acc:  0.7421875 test_cost:  0.8042363 test_acc:  0.75\n",
            "iter:  377 train_cost:  1.1029427 train_acc:  0.6796875 test_cost:  1.2407894 test_acc:  0.6171875\n",
            "iter:  378 train_cost:  0.7871279 train_acc:  0.75 test_cost:  0.8051716 test_acc:  0.7421875\n",
            "iter:  379 train_cost:  0.69877684 train_acc:  0.796875 test_cost:  0.8360035 test_acc:  0.765625\n",
            "iter:  380 train_cost:  0.7896137 train_acc:  0.7265625 test_cost:  0.8294367 test_acc:  0.7421875\n",
            "iter:  381 train_cost:  0.9786315 train_acc:  0.71875 test_cost:  1.0785731 test_acc:  0.6953125\n",
            "iter:  382 train_cost:  0.84811056 train_acc:  0.75 test_cost:  0.64850533 test_acc:  0.765625\n",
            "iter:  383 train_cost:  0.9042659 train_acc:  0.765625 test_cost:  0.7652626 test_acc:  0.765625\n",
            "iter:  384 train_cost:  0.57030624 train_acc:  0.828125 test_cost:  0.98219407 test_acc:  0.7421875\n",
            "iter:  385 train_cost:  0.77782637 train_acc:  0.734375 test_cost:  0.9008305 test_acc:  0.71875\n",
            "iter:  386 train_cost:  1.199498 train_acc:  0.6328125 test_cost:  0.8461641 test_acc:  0.6953125\n",
            "iter:  387 train_cost:  0.8801816 train_acc:  0.7578125 test_cost:  0.7990831 test_acc:  0.734375\n",
            "iter:  388 train_cost:  0.7157664 train_acc:  0.75 test_cost:  1.072383 test_acc:  0.6875\n",
            "iter:  389 train_cost:  0.8307694 train_acc:  0.6875 test_cost:  0.86539304 test_acc:  0.734375\n",
            "iter:  390 train_cost:  0.6652012 train_acc:  0.78125 test_cost:  0.80917424 test_acc:  0.71875\n",
            "iter:  391 train_cost:  0.78557914 train_acc:  0.734375 test_cost:  0.7116785 test_acc:  0.78125\n",
            "iter:  392 train_cost:  0.8589705 train_acc:  0.6953125 test_cost:  0.8036508 test_acc:  0.7734375\n",
            "iter:  393 train_cost:  0.82094705 train_acc:  0.7421875 test_cost:  0.8456559 test_acc:  0.71875\n",
            "iter:  394 train_cost:  0.94648063 train_acc:  0.71875 test_cost:  0.7932307 test_acc:  0.7265625\n",
            "iter:  395 train_cost:  0.8602904 train_acc:  0.7265625 test_cost:  0.9360779 test_acc:  0.7578125\n",
            "iter:  396 train_cost:  0.8321836 train_acc:  0.7265625 test_cost:  0.74063706 test_acc:  0.765625\n",
            "iter:  397 train_cost:  0.76449394 train_acc:  0.796875 test_cost:  0.8344249 test_acc:  0.71875\n",
            "iter:  398 train_cost:  0.80819184 train_acc:  0.71875 test_cost:  0.6348444 test_acc:  0.7890625\n",
            "iter:  399 train_cost:  0.79520893 train_acc:  0.6875 test_cost:  0.97766525 test_acc:  0.71875\n",
            "iter:  400 train_cost:  0.70744944 train_acc:  0.765625 test_cost:  0.79430044 test_acc:  0.8125\n",
            "iter:  401 train_cost:  0.9523744 train_acc:  0.6953125 test_cost:  0.8950735 test_acc:  0.671875\n",
            "iter:  402 train_cost:  0.7644413 train_acc:  0.8046875 test_cost:  0.88390183 test_acc:  0.765625\n",
            "iter:  403 train_cost:  0.7435775 train_acc:  0.8125 test_cost:  0.8779575 test_acc:  0.765625\n",
            "iter:  404 train_cost:  1.028378 train_acc:  0.734375 test_cost:  0.704912 test_acc:  0.75\n",
            "iter:  405 train_cost:  0.8298318 train_acc:  0.7421875 test_cost:  0.9429584 test_acc:  0.6953125\n",
            "iter:  406 train_cost:  1.076828 train_acc:  0.6953125 test_cost:  0.91037667 test_acc:  0.71875\n",
            "iter:  407 train_cost:  0.63386816 train_acc:  0.796875 test_cost:  0.6133567 test_acc:  0.7890625\n",
            "iter:  408 train_cost:  0.74518406 train_acc:  0.734375 test_cost:  0.63641036 test_acc:  0.8125\n",
            "iter:  409 train_cost:  0.77033025 train_acc:  0.7578125 test_cost:  0.8219782 test_acc:  0.7265625\n",
            "iter:  410 train_cost:  0.80007565 train_acc:  0.75 test_cost:  0.8059319 test_acc:  0.7578125\n",
            "iter:  411 train_cost:  0.822604 train_acc:  0.7734375 test_cost:  0.71485204 test_acc:  0.78125\n",
            "iter:  412 train_cost:  1.0658013 train_acc:  0.671875 test_cost:  0.8895172 test_acc:  0.71875\n",
            "iter:  413 train_cost:  1.3040681 train_acc:  0.6328125 test_cost:  0.824822 test_acc:  0.7421875\n",
            "iter:  414 train_cost:  1.0181485 train_acc:  0.7421875 test_cost:  0.95781195 test_acc:  0.78125\n",
            "iter:  415 train_cost:  0.89003503 train_acc:  0.6875 test_cost:  0.67347455 test_acc:  0.75\n",
            "iter:  416 train_cost:  0.9779781 train_acc:  0.71875 test_cost:  0.84709454 test_acc:  0.7109375\n",
            "iter:  417 train_cost:  0.8368889 train_acc:  0.7421875 test_cost:  0.71900916 test_acc:  0.7265625\n",
            "iter:  418 train_cost:  0.7362411 train_acc:  0.7578125 test_cost:  0.9507652 test_acc:  0.6875\n",
            "iter:  419 train_cost:  0.8963227 train_acc:  0.7265625 test_cost:  0.6800368 test_acc:  0.7421875\n",
            "iter:  420 train_cost:  0.8346529 train_acc:  0.71875 test_cost:  0.86308795 test_acc:  0.75\n",
            "iter:  421 train_cost:  0.88747424 train_acc:  0.703125 test_cost:  0.9156977 test_acc:  0.734375\n",
            "iter:  422 train_cost:  0.812904 train_acc:  0.7265625 test_cost:  0.71866274 test_acc:  0.78125\n",
            "iter:  423 train_cost:  0.8850114 train_acc:  0.765625 test_cost:  0.55449826 test_acc:  0.8359375\n",
            "iter:  424 train_cost:  0.7294661 train_acc:  0.7421875 test_cost:  0.9388087 test_acc:  0.75\n",
            "iter:  425 train_cost:  0.88722277 train_acc:  0.734375 test_cost:  0.7897518 test_acc:  0.765625\n",
            "iter:  426 train_cost:  0.9173631 train_acc:  0.7421875 test_cost:  0.59662545 test_acc:  0.828125\n",
            "iter:  427 train_cost:  0.732134 train_acc:  0.7578125 test_cost:  0.83458126 test_acc:  0.75\n",
            "iter:  428 train_cost:  0.6622021 train_acc:  0.78125 test_cost:  0.6976013 test_acc:  0.78125\n",
            "iter:  429 train_cost:  0.6732055 train_acc:  0.8203125 test_cost:  0.67121196 test_acc:  0.78125\n",
            "iter:  430 train_cost:  0.8654971 train_acc:  0.7421875 test_cost:  0.8729868 test_acc:  0.71875\n",
            "iter:  431 train_cost:  0.8674125 train_acc:  0.6875 test_cost:  0.7539747 test_acc:  0.7578125\n",
            "iter:  432 train_cost:  0.83600634 train_acc:  0.7265625 test_cost:  0.69624543 test_acc:  0.7734375\n",
            "iter:  433 train_cost:  0.6564765 train_acc:  0.78125 test_cost:  0.9386882 test_acc:  0.7421875\n",
            "iter:  434 train_cost:  0.60606384 train_acc:  0.828125 test_cost:  0.7505231 test_acc:  0.75\n",
            "iter:  435 train_cost:  0.8796363 train_acc:  0.71875 test_cost:  0.7676394 test_acc:  0.796875\n",
            "iter:  436 train_cost:  0.6654154 train_acc:  0.7890625 test_cost:  0.778299 test_acc:  0.7578125\n",
            "iter:  437 train_cost:  0.87423354 train_acc:  0.7265625 test_cost:  0.66635644 test_acc:  0.78125\n",
            "iter:  438 train_cost:  0.8052583 train_acc:  0.7421875 test_cost:  0.7632967 test_acc:  0.75\n",
            "iter:  439 train_cost:  0.6760602 train_acc:  0.8125 test_cost:  0.7921791 test_acc:  0.71875\n",
            "iter:  440 train_cost:  0.5745404 train_acc:  0.8203125 test_cost:  0.8771885 test_acc:  0.78125\n",
            "iter:  441 train_cost:  0.9067295 train_acc:  0.765625 test_cost:  0.7731403 test_acc:  0.7578125\n",
            "iter:  442 train_cost:  0.851154 train_acc:  0.7578125 test_cost:  0.7506496 test_acc:  0.7734375\n",
            "iter:  443 train_cost:  0.93636465 train_acc:  0.7265625 test_cost:  0.7364837 test_acc:  0.796875\n",
            "iter:  444 train_cost:  0.79845107 train_acc:  0.7890625 test_cost:  1.0553095 test_acc:  0.71875\n",
            "iter:  445 train_cost:  0.7771276 train_acc:  0.78125 test_cost:  0.7418631 test_acc:  0.7578125\n",
            "iter:  446 train_cost:  0.74420404 train_acc:  0.75 test_cost:  0.7556492 test_acc:  0.734375\n",
            "iter:  447 train_cost:  0.9195357 train_acc:  0.7578125 test_cost:  1.0248194 test_acc:  0.734375\n",
            "iter:  448 train_cost:  0.63824344 train_acc:  0.8046875 test_cost:  0.6806685 test_acc:  0.8125\n",
            "iter:  449 train_cost:  0.59432995 train_acc:  0.8046875 test_cost:  0.65943694 test_acc:  0.796875\n",
            "iter:  450 train_cost:  0.5758251 train_acc:  0.78125 test_cost:  0.9116002 test_acc:  0.6796875\n",
            "iter:  451 train_cost:  0.76426625 train_acc:  0.7578125 test_cost:  0.6806875 test_acc:  0.7734375\n",
            "iter:  452 train_cost:  0.8077332 train_acc:  0.7578125 test_cost:  0.7318171 test_acc:  0.78125\n",
            "iter:  453 train_cost:  1.0054497 train_acc:  0.671875 test_cost:  0.83781105 test_acc:  0.7109375\n",
            "iter:  454 train_cost:  0.8438853 train_acc:  0.7265625 test_cost:  0.8496174 test_acc:  0.7421875\n",
            "iter:  455 train_cost:  0.7767003 train_acc:  0.765625 test_cost:  0.58705485 test_acc:  0.8203125\n",
            "iter:  456 train_cost:  0.5138694 train_acc:  0.828125 test_cost:  0.561834 test_acc:  0.8359375\n",
            "iter:  457 train_cost:  0.79586315 train_acc:  0.8203125 test_cost:  0.7746901 test_acc:  0.8125\n",
            "iter:  458 train_cost:  0.80114645 train_acc:  0.7265625 test_cost:  0.7216224 test_acc:  0.796875\n",
            "iter:  459 train_cost:  0.523415 train_acc:  0.8515625 test_cost:  0.6975484 test_acc:  0.78125\n",
            "iter:  460 train_cost:  0.73259276 train_acc:  0.7421875 test_cost:  0.9568978 test_acc:  0.75\n",
            "iter:  461 train_cost:  0.89629054 train_acc:  0.6875 test_cost:  0.623808 test_acc:  0.8125\n",
            "iter:  462 train_cost:  0.82343173 train_acc:  0.765625 test_cost:  0.8268078 test_acc:  0.7421875\n",
            "iter:  463 train_cost:  0.6890341 train_acc:  0.8359375 test_cost:  0.69051707 test_acc:  0.7578125\n",
            "iter:  464 train_cost:  0.6895282 train_acc:  0.765625 test_cost:  0.55120844 test_acc:  0.8125\n",
            "iter:  465 train_cost:  0.6780734 train_acc:  0.78125 test_cost:  0.5966127 test_acc:  0.8125\n",
            "iter:  466 train_cost:  1.0667768 train_acc:  0.6796875 test_cost:  1.071842 test_acc:  0.7109375\n",
            "iter:  467 train_cost:  0.7575791 train_acc:  0.7890625 test_cost:  0.7452429 test_acc:  0.7578125\n",
            "iter:  468 train_cost:  0.63371056 train_acc:  0.8359375 test_cost:  0.8802557 test_acc:  0.7890625\n",
            "iter:  469 train_cost:  0.6531055 train_acc:  0.8046875 test_cost:  0.65124494 test_acc:  0.78125\n",
            "iter:  470 train_cost:  0.53795165 train_acc:  0.78125 test_cost:  0.77907276 test_acc:  0.765625\n",
            "iter:  471 train_cost:  0.70641243 train_acc:  0.7734375 test_cost:  0.5939777 test_acc:  0.8046875\n",
            "iter:  472 train_cost:  0.7502135 train_acc:  0.7578125 test_cost:  0.73399484 test_acc:  0.75\n",
            "iter:  473 train_cost:  0.739852 train_acc:  0.7890625 test_cost:  1.0216057 test_acc:  0.703125\n",
            "iter:  474 train_cost:  0.6208491 train_acc:  0.796875 test_cost:  0.8163186 test_acc:  0.765625\n",
            "iter:  475 train_cost:  0.66146624 train_acc:  0.78125 test_cost:  0.65945816 test_acc:  0.796875\n",
            "iter:  476 train_cost:  0.8913467 train_acc:  0.75 test_cost:  0.742974 test_acc:  0.75\n",
            "iter:  477 train_cost:  0.9317138 train_acc:  0.7421875 test_cost:  0.8079291 test_acc:  0.78125\n",
            "iter:  478 train_cost:  0.80968094 train_acc:  0.7109375 test_cost:  0.6915849 test_acc:  0.796875\n",
            "iter:  479 train_cost:  0.71803427 train_acc:  0.7578125 test_cost:  0.7695962 test_acc:  0.78125\n",
            "iter:  480 train_cost:  0.7315339 train_acc:  0.7890625 test_cost:  0.78002965 test_acc:  0.7734375\n",
            "iter:  481 train_cost:  0.9361476 train_acc:  0.71875 test_cost:  0.7035953 test_acc:  0.796875\n",
            "iter:  482 train_cost:  0.6292287 train_acc:  0.7890625 test_cost:  0.5748639 test_acc:  0.8359375\n",
            "iter:  483 train_cost:  0.6810825 train_acc:  0.765625 test_cost:  0.80918896 test_acc:  0.7421875\n",
            "iter:  484 train_cost:  0.8207076 train_acc:  0.796875 test_cost:  0.89163995 test_acc:  0.734375\n",
            "iter:  485 train_cost:  0.8393315 train_acc:  0.7734375 test_cost:  0.75564325 test_acc:  0.796875\n",
            "iter:  486 train_cost:  0.73953867 train_acc:  0.7734375 test_cost:  0.93198526 test_acc:  0.7265625\n",
            "iter:  487 train_cost:  0.85812056 train_acc:  0.7109375 test_cost:  0.7634356 test_acc:  0.78125\n",
            "iter:  488 train_cost:  0.7183757 train_acc:  0.8125 test_cost:  0.6836124 test_acc:  0.8046875\n",
            "iter:  489 train_cost:  0.7928062 train_acc:  0.8046875 test_cost:  0.68874216 test_acc:  0.7890625\n",
            "iter:  490 train_cost:  0.79061675 train_acc:  0.7265625 test_cost:  0.66169417 test_acc:  0.8203125\n",
            "iter:  491 train_cost:  0.87385345 train_acc:  0.6875 test_cost:  0.63665015 test_acc:  0.796875\n",
            "iter:  492 train_cost:  0.80649275 train_acc:  0.7890625 test_cost:  0.78261477 test_acc:  0.765625\n",
            "iter:  493 train_cost:  0.76919544 train_acc:  0.78125 test_cost:  0.68414557 test_acc:  0.8203125\n",
            "iter:  494 train_cost:  0.6175682 train_acc:  0.78125 test_cost:  0.7979213 test_acc:  0.7265625\n",
            "iter:  495 train_cost:  0.58555603 train_acc:  0.8203125 test_cost:  0.66262305 test_acc:  0.7578125\n",
            "iter:  496 train_cost:  0.60667497 train_acc:  0.78125 test_cost:  0.6455674 test_acc:  0.7578125\n",
            "iter:  497 train_cost:  0.47184438 train_acc:  0.8671875 test_cost:  0.72154105 test_acc:  0.7734375\n",
            "iter:  498 train_cost:  0.59934306 train_acc:  0.8203125 test_cost:  0.6376381 test_acc:  0.796875\n",
            "iter:  499 train_cost:  0.5728934 train_acc:  0.7890625 test_cost:  0.87133193 test_acc:  0.75\n",
            "iter:  500 train_cost:  0.69807225 train_acc:  0.7890625 test_cost:  0.57238054 test_acc:  0.7890625\n",
            "iter:  501 train_cost:  0.6220292 train_acc:  0.78125 test_cost:  0.4680022 test_acc:  0.828125\n",
            "iter:  502 train_cost:  0.79544085 train_acc:  0.7421875 test_cost:  0.7156415 test_acc:  0.78125\n",
            "iter:  503 train_cost:  0.74836713 train_acc:  0.7578125 test_cost:  0.5859721 test_acc:  0.8046875\n",
            "iter:  504 train_cost:  0.6248825 train_acc:  0.828125 test_cost:  0.5819473 test_acc:  0.78125\n",
            "iter:  505 train_cost:  0.5809987 train_acc:  0.8359375 test_cost:  0.84709346 test_acc:  0.7109375\n",
            "iter:  506 train_cost:  0.7150261 train_acc:  0.75 test_cost:  0.86748886 test_acc:  0.7578125\n",
            "iter:  507 train_cost:  0.745256 train_acc:  0.796875 test_cost:  0.57768923 test_acc:  0.84375\n",
            "iter:  508 train_cost:  0.64529616 train_acc:  0.7890625 test_cost:  0.62304044 test_acc:  0.7578125\n",
            "iter:  509 train_cost:  0.6146266 train_acc:  0.8203125 test_cost:  0.7136368 test_acc:  0.765625\n",
            "iter:  510 train_cost:  0.65679884 train_acc:  0.7890625 test_cost:  0.6337825 test_acc:  0.7890625\n",
            "iter:  511 train_cost:  0.7789159 train_acc:  0.71875 test_cost:  0.5143783 test_acc:  0.8359375\n",
            "iter:  512 train_cost:  0.5640878 train_acc:  0.796875 test_cost:  0.5607379 test_acc:  0.8046875\n",
            "iter:  513 train_cost:  0.71670383 train_acc:  0.7578125 test_cost:  0.6241018 test_acc:  0.8359375\n",
            "iter:  514 train_cost:  0.74641746 train_acc:  0.7890625 test_cost:  0.7401391 test_acc:  0.8203125\n",
            "iter:  515 train_cost:  0.80426013 train_acc:  0.71875 test_cost:  1.0556823 test_acc:  0.7109375\n",
            "iter:  516 train_cost:  0.6092787 train_acc:  0.8125 test_cost:  0.8984914 test_acc:  0.75\n",
            "iter:  517 train_cost:  0.767366 train_acc:  0.765625 test_cost:  0.58139753 test_acc:  0.8359375\n",
            "iter:  518 train_cost:  0.53358966 train_acc:  0.8359375 test_cost:  0.53855646 test_acc:  0.8515625\n",
            "iter:  519 train_cost:  0.68341374 train_acc:  0.765625 test_cost:  0.5554864 test_acc:  0.7890625\n",
            "iter:  520 train_cost:  0.35647088 train_acc:  0.890625 test_cost:  0.70231694 test_acc:  0.78125\n",
            "iter:  521 train_cost:  0.6955005 train_acc:  0.7890625 test_cost:  0.7321991 test_acc:  0.7734375\n",
            "iter:  522 train_cost:  0.9209636 train_acc:  0.71875 test_cost:  0.8135893 test_acc:  0.78125\n",
            "iter:  523 train_cost:  0.5412518 train_acc:  0.84375 test_cost:  0.60063523 test_acc:  0.8359375\n",
            "iter:  524 train_cost:  0.8067666 train_acc:  0.75 test_cost:  0.71083206 test_acc:  0.78125\n",
            "iter:  525 train_cost:  0.82839024 train_acc:  0.78125 test_cost:  0.89142644 test_acc:  0.7421875\n",
            "iter:  526 train_cost:  0.57657653 train_acc:  0.796875 test_cost:  0.8426802 test_acc:  0.7421875\n",
            "iter:  527 train_cost:  0.5325563 train_acc:  0.859375 test_cost:  0.80832636 test_acc:  0.7890625\n",
            "iter:  528 train_cost:  0.9119935 train_acc:  0.7734375 test_cost:  0.7741147 test_acc:  0.71875\n",
            "iter:  529 train_cost:  0.88411415 train_acc:  0.734375 test_cost:  0.76491594 test_acc:  0.7421875\n",
            "iter:  530 train_cost:  0.6007555 train_acc:  0.8125 test_cost:  0.7090529 test_acc:  0.8046875\n",
            "iter:  531 train_cost:  0.80485713 train_acc:  0.765625 test_cost:  0.61217624 test_acc:  0.8203125\n",
            "iter:  532 train_cost:  0.7483394 train_acc:  0.765625 test_cost:  0.48579246 test_acc:  0.8203125\n",
            "iter:  533 train_cost:  0.6865495 train_acc:  0.75 test_cost:  0.7086198 test_acc:  0.7578125\n",
            "iter:  534 train_cost:  0.73398966 train_acc:  0.765625 test_cost:  0.55337095 test_acc:  0.8203125\n",
            "iter:  535 train_cost:  0.7215277 train_acc:  0.78125 test_cost:  0.6631267 test_acc:  0.8046875\n",
            "iter:  536 train_cost:  0.78094804 train_acc:  0.7734375 test_cost:  0.8278303 test_acc:  0.75\n",
            "iter:  537 train_cost:  0.6616597 train_acc:  0.828125 test_cost:  0.74928045 test_acc:  0.7734375\n",
            "iter:  538 train_cost:  0.74868697 train_acc:  0.7890625 test_cost:  0.70637274 test_acc:  0.765625\n",
            "iter:  539 train_cost:  0.87096906 train_acc:  0.71875 test_cost:  0.72383904 test_acc:  0.734375\n",
            "iter:  540 train_cost:  0.680332 train_acc:  0.8125 test_cost:  0.8361083 test_acc:  0.7734375\n",
            "iter:  541 train_cost:  0.7005397 train_acc:  0.7734375 test_cost:  0.62543666 test_acc:  0.8125\n",
            "iter:  542 train_cost:  0.6080464 train_acc:  0.796875 test_cost:  0.79557216 test_acc:  0.7578125\n",
            "iter:  543 train_cost:  0.67487335 train_acc:  0.7578125 test_cost:  0.51084065 test_acc:  0.8359375\n",
            "iter:  544 train_cost:  0.65553194 train_acc:  0.7890625 test_cost:  0.57710224 test_acc:  0.8359375\n",
            "iter:  545 train_cost:  0.678005 train_acc:  0.7734375 test_cost:  0.6296587 test_acc:  0.828125\n",
            "iter:  546 train_cost:  0.7720429 train_acc:  0.7578125 test_cost:  0.66276413 test_acc:  0.8046875\n",
            "iter:  547 train_cost:  0.8376421 train_acc:  0.7890625 test_cost:  0.6324456 test_acc:  0.8203125\n",
            "iter:  548 train_cost:  0.83110833 train_acc:  0.78125 test_cost:  0.6257295 test_acc:  0.7890625\n",
            "iter:  549 train_cost:  0.82525194 train_acc:  0.7109375 test_cost:  0.6863439 test_acc:  0.8046875\n",
            "iter:  550 train_cost:  0.6769235 train_acc:  0.796875 test_cost:  0.5954654 test_acc:  0.7890625\n",
            "iter:  551 train_cost:  0.4513664 train_acc:  0.8125 test_cost:  0.73282117 test_acc:  0.78125\n",
            "iter:  552 train_cost:  0.6963366 train_acc:  0.796875 test_cost:  0.6977359 test_acc:  0.765625\n",
            "iter:  553 train_cost:  0.8408891 train_acc:  0.75 test_cost:  0.6080698 test_acc:  0.7890625\n",
            "iter:  554 train_cost:  0.73618495 train_acc:  0.7890625 test_cost:  1.1084694 test_acc:  0.671875\n",
            "iter:  555 train_cost:  0.632584 train_acc:  0.8203125 test_cost:  0.63854593 test_acc:  0.84375\n",
            "iter:  556 train_cost:  0.7058983 train_acc:  0.7734375 test_cost:  0.63736576 test_acc:  0.796875\n",
            "iter:  557 train_cost:  0.57859993 train_acc:  0.84375 test_cost:  0.61954725 test_acc:  0.8125\n",
            "iter:  558 train_cost:  0.61405283 train_acc:  0.75 test_cost:  0.58613193 test_acc:  0.8046875\n",
            "iter:  559 train_cost:  0.522629 train_acc:  0.828125 test_cost:  0.687076 test_acc:  0.7734375\n",
            "iter:  560 train_cost:  0.93635315 train_acc:  0.7578125 test_cost:  0.75468624 test_acc:  0.78125\n",
            "iter:  561 train_cost:  0.84561116 train_acc:  0.765625 test_cost:  0.7366227 test_acc:  0.7734375\n",
            "iter:  562 train_cost:  0.52483034 train_acc:  0.828125 test_cost:  0.78722024 test_acc:  0.7734375\n",
            "iter:  563 train_cost:  0.6757089 train_acc:  0.7734375 test_cost:  0.6668382 test_acc:  0.7734375\n",
            "iter:  564 train_cost:  0.55516624 train_acc:  0.8125 test_cost:  0.7463823 test_acc:  0.78125\n",
            "iter:  565 train_cost:  0.59394896 train_acc:  0.84375 test_cost:  0.5134727 test_acc:  0.828125\n",
            "iter:  566 train_cost:  0.74306434 train_acc:  0.78125 test_cost:  0.8021573 test_acc:  0.796875\n",
            "iter:  567 train_cost:  0.74319774 train_acc:  0.7890625 test_cost:  0.58829415 test_acc:  0.828125\n",
            "iter:  568 train_cost:  0.7080288 train_acc:  0.84375 test_cost:  0.52216 test_acc:  0.859375\n",
            "iter:  569 train_cost:  0.6883703 train_acc:  0.734375 test_cost:  0.8568527 test_acc:  0.7734375\n",
            "iter:  570 train_cost:  0.71254003 train_acc:  0.8125 test_cost:  0.6772002 test_acc:  0.7734375\n",
            "iter:  571 train_cost:  0.83795625 train_acc:  0.765625 test_cost:  0.63260037 test_acc:  0.7890625\n",
            "iter:  572 train_cost:  0.63983256 train_acc:  0.765625 test_cost:  0.6169968 test_acc:  0.796875\n",
            "iter:  573 train_cost:  0.73194754 train_acc:  0.765625 test_cost:  0.5759356 test_acc:  0.8125\n",
            "iter:  574 train_cost:  0.83366215 train_acc:  0.765625 test_cost:  0.68002474 test_acc:  0.8125\n",
            "iter:  575 train_cost:  0.58297026 train_acc:  0.7890625 test_cost:  0.6838776 test_acc:  0.765625\n",
            "iter:  576 train_cost:  0.64049554 train_acc:  0.7890625 test_cost:  0.75922763 test_acc:  0.765625\n",
            "iter:  577 train_cost:  0.7372023 train_acc:  0.796875 test_cost:  0.75998425 test_acc:  0.75\n",
            "iter:  578 train_cost:  0.6457259 train_acc:  0.7734375 test_cost:  0.6428698 test_acc:  0.8203125\n",
            "iter:  579 train_cost:  0.54215837 train_acc:  0.8203125 test_cost:  0.58215135 test_acc:  0.8125\n",
            "iter:  580 train_cost:  0.6676032 train_acc:  0.765625 test_cost:  0.45516515 test_acc:  0.84375\n",
            "iter:  581 train_cost:  0.6126349 train_acc:  0.8203125 test_cost:  0.7282656 test_acc:  0.796875\n",
            "iter:  582 train_cost:  0.5992504 train_acc:  0.828125 test_cost:  0.6028372 test_acc:  0.7890625\n",
            "iter:  583 train_cost:  0.6283786 train_acc:  0.796875 test_cost:  0.7852687 test_acc:  0.75\n",
            "iter:  584 train_cost:  0.8950137 train_acc:  0.7421875 test_cost:  0.5520044 test_acc:  0.8203125\n",
            "iter:  585 train_cost:  0.65821683 train_acc:  0.828125 test_cost:  0.5412305 test_acc:  0.8203125\n",
            "iter:  586 train_cost:  0.7652371 train_acc:  0.8125 test_cost:  0.6502329 test_acc:  0.796875\n",
            "iter:  587 train_cost:  0.72703016 train_acc:  0.7421875 test_cost:  0.64317644 test_acc:  0.8125\n",
            "iter:  588 train_cost:  0.6540035 train_acc:  0.8203125 test_cost:  0.6234607 test_acc:  0.8203125\n",
            "iter:  589 train_cost:  0.72473466 train_acc:  0.8046875 test_cost:  0.8351539 test_acc:  0.7265625\n",
            "iter:  590 train_cost:  0.5290301 train_acc:  0.8125 test_cost:  0.60523593 test_acc:  0.8203125\n",
            "iter:  591 train_cost:  0.54014313 train_acc:  0.8203125 test_cost:  0.5702189 test_acc:  0.8515625\n",
            "iter:  592 train_cost:  0.5149014 train_acc:  0.8203125 test_cost:  0.51270366 test_acc:  0.8125\n",
            "iter:  593 train_cost:  0.7903833 train_acc:  0.7578125 test_cost:  0.5965861 test_acc:  0.8046875\n",
            "iter:  594 train_cost:  0.5961549 train_acc:  0.7890625 test_cost:  0.60240334 test_acc:  0.8125\n",
            "iter:  595 train_cost:  0.641044 train_acc:  0.8125 test_cost:  0.5742597 test_acc:  0.828125\n",
            "iter:  596 train_cost:  0.68235695 train_acc:  0.8046875 test_cost:  0.77154624 test_acc:  0.7890625\n",
            "iter:  597 train_cost:  0.63622546 train_acc:  0.8046875 test_cost:  0.7242969 test_acc:  0.7734375\n",
            "iter:  598 train_cost:  0.67374295 train_acc:  0.765625 test_cost:  0.40205905 test_acc:  0.8984375\n",
            "iter:  599 train_cost:  0.5567567 train_acc:  0.8203125 test_cost:  0.5747861 test_acc:  0.796875\n",
            "iter:  600 train_cost:  0.66400766 train_acc:  0.796875 test_cost:  0.6855247 test_acc:  0.796875\n",
            "iter:  601 train_cost:  0.7948689 train_acc:  0.75 test_cost:  0.68455744 test_acc:  0.828125\n",
            "iter:  602 train_cost:  0.679992 train_acc:  0.765625 test_cost:  0.791519 test_acc:  0.796875\n",
            "iter:  603 train_cost:  0.5551237 train_acc:  0.8359375 test_cost:  0.6522594 test_acc:  0.8203125\n",
            "iter:  604 train_cost:  0.5157579 train_acc:  0.84375 test_cost:  0.66999674 test_acc:  0.78125\n",
            "iter:  605 train_cost:  0.67946225 train_acc:  0.796875 test_cost:  0.51383567 test_acc:  0.8359375\n",
            "iter:  606 train_cost:  0.63760424 train_acc:  0.765625 test_cost:  0.5986951 test_acc:  0.796875\n",
            "iter:  607 train_cost:  0.5169755 train_acc:  0.7890625 test_cost:  0.616612 test_acc:  0.828125\n",
            "iter:  608 train_cost:  0.7194838 train_acc:  0.796875 test_cost:  0.44165188 test_acc:  0.8515625\n",
            "iter:  609 train_cost:  0.7100853 train_acc:  0.7890625 test_cost:  0.5461029 test_acc:  0.828125\n",
            "iter:  610 train_cost:  0.4916585 train_acc:  0.875 test_cost:  0.89245534 test_acc:  0.765625\n",
            "iter:  611 train_cost:  0.42940784 train_acc:  0.84375 test_cost:  0.6114919 test_acc:  0.796875\n",
            "iter:  612 train_cost:  0.5664058 train_acc:  0.8203125 test_cost:  0.714103 test_acc:  0.8046875\n",
            "iter:  613 train_cost:  0.54332745 train_acc:  0.8125 test_cost:  0.55642223 test_acc:  0.84375\n",
            "iter:  614 train_cost:  0.5673895 train_acc:  0.8125 test_cost:  0.5946972 test_acc:  0.7890625\n",
            "iter:  615 train_cost:  0.8166498 train_acc:  0.7890625 test_cost:  0.50600123 test_acc:  0.859375\n",
            "iter:  616 train_cost:  0.6596571 train_acc:  0.78125 test_cost:  0.57514495 test_acc:  0.8046875\n",
            "iter:  617 train_cost:  0.5260713 train_acc:  0.8125 test_cost:  0.65753865 test_acc:  0.8046875\n",
            "iter:  618 train_cost:  0.6452008 train_acc:  0.78125 test_cost:  0.6574272 test_acc:  0.796875\n",
            "iter:  619 train_cost:  0.5234466 train_acc:  0.8359375 test_cost:  0.6789294 test_acc:  0.7890625\n",
            "iter:  620 train_cost:  0.7072005 train_acc:  0.8125 test_cost:  0.87121147 test_acc:  0.75\n",
            "iter:  621 train_cost:  0.5012134 train_acc:  0.8515625 test_cost:  0.7113317 test_acc:  0.8125\n",
            "iter:  622 train_cost:  0.63478076 train_acc:  0.8046875 test_cost:  0.7913343 test_acc:  0.75\n",
            "iter:  623 train_cost:  0.75941575 train_acc:  0.7578125 test_cost:  0.6089676 test_acc:  0.7734375\n",
            "iter:  624 train_cost:  0.647304 train_acc:  0.796875 test_cost:  0.6690492 test_acc:  0.7734375\n",
            "iter:  625 train_cost:  0.5785504 train_acc:  0.828125 test_cost:  0.6886048 test_acc:  0.7734375\n",
            "iter:  626 train_cost:  0.60435665 train_acc:  0.8046875 test_cost:  0.7667138 test_acc:  0.8125\n",
            "iter:  627 train_cost:  0.87019396 train_acc:  0.7734375 test_cost:  0.4333312 test_acc:  0.8515625\n",
            "iter:  628 train_cost:  0.5648613 train_acc:  0.8125 test_cost:  0.7801024 test_acc:  0.7890625\n",
            "iter:  629 train_cost:  0.53512084 train_acc:  0.8359375 test_cost:  0.5301075 test_acc:  0.796875\n",
            "iter:  630 train_cost:  0.45866597 train_acc:  0.828125 test_cost:  0.63768065 test_acc:  0.796875\n",
            "iter:  631 train_cost:  0.7217479 train_acc:  0.7734375 test_cost:  0.7872654 test_acc:  0.7890625\n",
            "iter:  632 train_cost:  0.6494782 train_acc:  0.7421875 test_cost:  0.58048594 test_acc:  0.8671875\n",
            "iter:  633 train_cost:  0.46540076 train_acc:  0.859375 test_cost:  0.6857133 test_acc:  0.8125\n",
            "iter:  634 train_cost:  0.7986391 train_acc:  0.78125 test_cost:  0.6008893 test_acc:  0.8125\n",
            "iter:  635 train_cost:  0.73459464 train_acc:  0.796875 test_cost:  0.7021805 test_acc:  0.7421875\n",
            "iter:  636 train_cost:  0.55440456 train_acc:  0.78125 test_cost:  0.6570388 test_acc:  0.7734375\n",
            "iter:  637 train_cost:  0.71497905 train_acc:  0.8125 test_cost:  0.62884855 test_acc:  0.828125\n",
            "iter:  638 train_cost:  0.93833804 train_acc:  0.7578125 test_cost:  0.59058493 test_acc:  0.8359375\n",
            "iter:  639 train_cost:  0.53888094 train_acc:  0.84375 test_cost:  0.6744867 test_acc:  0.78125\n",
            "iter:  640 train_cost:  0.50210065 train_acc:  0.828125 test_cost:  0.7523803 test_acc:  0.765625\n",
            "iter:  641 train_cost:  0.5886153 train_acc:  0.765625 test_cost:  0.63364595 test_acc:  0.8203125\n",
            "iter:  642 train_cost:  0.66414803 train_acc:  0.7578125 test_cost:  0.6024531 test_acc:  0.828125\n",
            "iter:  643 train_cost:  0.45808133 train_acc:  0.8203125 test_cost:  0.64594436 test_acc:  0.7890625\n",
            "iter:  644 train_cost:  0.58949244 train_acc:  0.8125 test_cost:  0.5552 test_acc:  0.8046875\n",
            "iter:  645 train_cost:  0.53902334 train_acc:  0.8203125 test_cost:  0.5958802 test_acc:  0.8203125\n",
            "iter:  646 train_cost:  0.663955 train_acc:  0.765625 test_cost:  0.74129677 test_acc:  0.78125\n",
            "iter:  647 train_cost:  0.4098781 train_acc:  0.8359375 test_cost:  0.6939737 test_acc:  0.8046875\n",
            "iter:  648 train_cost:  0.6936244 train_acc:  0.7890625 test_cost:  0.6244935 test_acc:  0.8359375\n",
            "iter:  649 train_cost:  0.7134582 train_acc:  0.7734375 test_cost:  0.5427853 test_acc:  0.8359375\n",
            "iter:  650 train_cost:  0.5895689 train_acc:  0.796875 test_cost:  0.4066772 test_acc:  0.8515625\n",
            "iter:  651 train_cost:  0.7629559 train_acc:  0.7421875 test_cost:  0.45372003 test_acc:  0.8828125\n",
            "iter:  652 train_cost:  0.40170768 train_acc:  0.8359375 test_cost:  0.69065577 test_acc:  0.796875\n",
            "iter:  653 train_cost:  0.7444941 train_acc:  0.8125 test_cost:  0.49857807 test_acc:  0.8203125\n",
            "iter:  654 train_cost:  0.7369343 train_acc:  0.75 test_cost:  0.66473925 test_acc:  0.8125\n",
            "iter:  655 train_cost:  0.5514228 train_acc:  0.859375 test_cost:  0.50183517 test_acc:  0.84375\n",
            "iter:  656 train_cost:  0.5340588 train_acc:  0.8515625 test_cost:  0.52959657 test_acc:  0.84375\n",
            "iter:  657 train_cost:  0.7222394 train_acc:  0.765625 test_cost:  0.6645007 test_acc:  0.796875\n",
            "iter:  658 train_cost:  0.59575975 train_acc:  0.8125 test_cost:  0.60492444 test_acc:  0.8203125\n",
            "iter:  659 train_cost:  0.5001987 train_acc:  0.8125 test_cost:  0.5675918 test_acc:  0.8203125\n",
            "iter:  660 train_cost:  0.65132666 train_acc:  0.7734375 test_cost:  0.55970794 test_acc:  0.8671875\n",
            "iter:  661 train_cost:  0.59577924 train_acc:  0.8515625 test_cost:  0.58776146 test_acc:  0.828125\n",
            "iter:  662 train_cost:  0.6271735 train_acc:  0.84375 test_cost:  0.70885134 test_acc:  0.84375\n",
            "iter:  663 train_cost:  0.6215488 train_acc:  0.8125 test_cost:  0.5129285 test_acc:  0.84375\n",
            "iter:  664 train_cost:  0.7559398 train_acc:  0.7578125 test_cost:  0.4458856 test_acc:  0.8671875\n",
            "iter:  665 train_cost:  0.78451157 train_acc:  0.734375 test_cost:  0.4985299 test_acc:  0.828125\n",
            "iter:  666 train_cost:  0.601755 train_acc:  0.796875 test_cost:  0.6113063 test_acc:  0.765625\n",
            "iter:  667 train_cost:  0.40712035 train_acc:  0.890625 test_cost:  0.8747885 test_acc:  0.7734375\n",
            "iter:  668 train_cost:  0.58254635 train_acc:  0.828125 test_cost:  0.4599598 test_acc:  0.875\n",
            "iter:  669 train_cost:  0.43933994 train_acc:  0.8671875 test_cost:  0.7842182 test_acc:  0.7734375\n",
            "iter:  670 train_cost:  0.5875907 train_acc:  0.8203125 test_cost:  0.56126696 test_acc:  0.8046875\n",
            "iter:  671 train_cost:  0.6811409 train_acc:  0.796875 test_cost:  0.48521158 test_acc:  0.8359375\n",
            "iter:  672 train_cost:  0.5228731 train_acc:  0.84375 test_cost:  0.6323927 test_acc:  0.796875\n",
            "iter:  673 train_cost:  0.77770305 train_acc:  0.734375 test_cost:  0.67777514 test_acc:  0.8359375\n",
            "iter:  674 train_cost:  0.9216747 train_acc:  0.734375 test_cost:  0.61616904 test_acc:  0.8046875\n",
            "iter:  675 train_cost:  0.5914657 train_acc:  0.8359375 test_cost:  0.52261174 test_acc:  0.8046875\n",
            "iter:  676 train_cost:  0.47739735 train_acc:  0.859375 test_cost:  0.46610156 test_acc:  0.8359375\n",
            "iter:  677 train_cost:  0.3966639 train_acc:  0.8828125 test_cost:  0.60915697 test_acc:  0.8203125\n",
            "iter:  678 train_cost:  0.59581745 train_acc:  0.796875 test_cost:  0.6065469 test_acc:  0.828125\n",
            "iter:  679 train_cost:  0.6199833 train_acc:  0.8203125 test_cost:  0.39513654 test_acc:  0.8515625\n",
            "iter:  680 train_cost:  0.6194325 train_acc:  0.828125 test_cost:  0.70398116 test_acc:  0.765625\n",
            "iter:  681 train_cost:  0.48005262 train_acc:  0.84375 test_cost:  0.7203332 test_acc:  0.8203125\n",
            "iter:  682 train_cost:  0.4971076 train_acc:  0.828125 test_cost:  0.5429655 test_acc:  0.84375\n",
            "iter:  683 train_cost:  0.64069754 train_acc:  0.828125 test_cost:  0.7719922 test_acc:  0.78125\n",
            "iter:  684 train_cost:  0.6945297 train_acc:  0.78125 test_cost:  0.64134777 test_acc:  0.78125\n",
            "iter:  685 train_cost:  0.5447409 train_acc:  0.8046875 test_cost:  0.5421484 test_acc:  0.8515625\n",
            "iter:  686 train_cost:  0.45412725 train_acc:  0.8671875 test_cost:  0.6474407 test_acc:  0.7421875\n",
            "iter:  687 train_cost:  0.6079025 train_acc:  0.8203125 test_cost:  0.44017494 test_acc:  0.859375\n",
            "iter:  688 train_cost:  0.6224402 train_acc:  0.84375 test_cost:  0.5736907 test_acc:  0.8203125\n",
            "iter:  689 train_cost:  0.61084116 train_acc:  0.8046875 test_cost:  0.44099587 test_acc:  0.8515625\n",
            "iter:  690 train_cost:  0.56695884 train_acc:  0.8046875 test_cost:  0.5857271 test_acc:  0.8125\n",
            "iter:  691 train_cost:  0.32982978 train_acc:  0.9140625 test_cost:  0.5383971 test_acc:  0.828125\n",
            "iter:  692 train_cost:  0.6384058 train_acc:  0.8359375 test_cost:  0.6825192 test_acc:  0.765625\n",
            "iter:  693 train_cost:  0.7547726 train_acc:  0.7734375 test_cost:  0.6146774 test_acc:  0.8125\n",
            "iter:  694 train_cost:  0.641467 train_acc:  0.78125 test_cost:  0.71881086 test_acc:  0.796875\n",
            "iter:  695 train_cost:  0.60855174 train_acc:  0.7890625 test_cost:  0.5318388 test_acc:  0.8203125\n",
            "iter:  696 train_cost:  0.5866386 train_acc:  0.8125 test_cost:  0.74754643 test_acc:  0.75\n",
            "iter:  697 train_cost:  0.43395883 train_acc:  0.875 test_cost:  0.71880376 test_acc:  0.7890625\n",
            "iter:  698 train_cost:  0.51732004 train_acc:  0.84375 test_cost:  0.76378375 test_acc:  0.765625\n",
            "iter:  699 train_cost:  0.6507863 train_acc:  0.8046875 test_cost:  0.6723105 test_acc:  0.8359375\n",
            "iter:  700 train_cost:  0.57432806 train_acc:  0.8046875 test_cost:  0.615256 test_acc:  0.7890625\n",
            "iter:  701 train_cost:  0.48390767 train_acc:  0.859375 test_cost:  0.52088284 test_acc:  0.859375\n",
            "iter:  702 train_cost:  0.5537224 train_acc:  0.8046875 test_cost:  0.5582025 test_acc:  0.828125\n",
            "iter:  703 train_cost:  0.5902893 train_acc:  0.8203125 test_cost:  0.7187474 test_acc:  0.8359375\n",
            "iter:  704 train_cost:  0.44951075 train_acc:  0.8515625 test_cost:  0.785717 test_acc:  0.7109375\n",
            "iter:  705 train_cost:  0.5552468 train_acc:  0.859375 test_cost:  0.68553686 test_acc:  0.7734375\n",
            "iter:  706 train_cost:  0.68875813 train_acc:  0.7734375 test_cost:  0.5568782 test_acc:  0.828125\n",
            "iter:  707 train_cost:  0.5107533 train_acc:  0.8125 test_cost:  0.38114414 test_acc:  0.890625\n",
            "iter:  708 train_cost:  0.67155385 train_acc:  0.7734375 test_cost:  0.5963911 test_acc:  0.796875\n",
            "iter:  709 train_cost:  0.57724106 train_acc:  0.84375 test_cost:  0.533216 test_acc:  0.828125\n",
            "iter:  710 train_cost:  0.57199746 train_acc:  0.8203125 test_cost:  0.67091006 test_acc:  0.7890625\n",
            "iter:  711 train_cost:  0.5500438 train_acc:  0.8046875 test_cost:  0.60606897 test_acc:  0.8203125\n",
            "iter:  712 train_cost:  0.63959515 train_acc:  0.796875 test_cost:  0.6396418 test_acc:  0.7890625\n",
            "iter:  713 train_cost:  0.5048084 train_acc:  0.8359375 test_cost:  0.56928605 test_acc:  0.8515625\n",
            "iter:  714 train_cost:  0.5514672 train_acc:  0.8359375 test_cost:  0.5089278 test_acc:  0.8359375\n",
            "iter:  715 train_cost:  0.475168 train_acc:  0.859375 test_cost:  0.64616203 test_acc:  0.796875\n",
            "iter:  716 train_cost:  0.43585986 train_acc:  0.8828125 test_cost:  0.47829473 test_acc:  0.8203125\n",
            "iter:  717 train_cost:  0.4359806 train_acc:  0.8515625 test_cost:  0.5767324 test_acc:  0.8125\n",
            "iter:  718 train_cost:  0.6274648 train_acc:  0.8125 test_cost:  0.6216547 test_acc:  0.8046875\n",
            "iter:  719 train_cost:  0.67605925 train_acc:  0.8125 test_cost:  0.3715472 test_acc:  0.8671875\n",
            "iter:  720 train_cost:  0.5906089 train_acc:  0.8046875 test_cost:  0.48414734 test_acc:  0.875\n",
            "iter:  721 train_cost:  0.36187553 train_acc:  0.9140625 test_cost:  0.82681036 test_acc:  0.7734375\n",
            "iter:  722 train_cost:  0.73284835 train_acc:  0.7734375 test_cost:  0.43436807 test_acc:  0.890625\n",
            "iter:  723 train_cost:  0.5678992 train_acc:  0.859375 test_cost:  0.76590145 test_acc:  0.78125\n",
            "iter:  724 train_cost:  0.5530193 train_acc:  0.8125 test_cost:  0.43141267 test_acc:  0.859375\n",
            "iter:  725 train_cost:  0.80200535 train_acc:  0.765625 test_cost:  0.8199427 test_acc:  0.7578125\n",
            "iter:  726 train_cost:  0.3682856 train_acc:  0.84375 test_cost:  0.62656486 test_acc:  0.78125\n",
            "iter:  727 train_cost:  0.69044226 train_acc:  0.7890625 test_cost:  0.51931846 test_acc:  0.8359375\n",
            "iter:  728 train_cost:  0.62363714 train_acc:  0.84375 test_cost:  0.4835512 test_acc:  0.8828125\n",
            "iter:  729 train_cost:  0.55761284 train_acc:  0.828125 test_cost:  0.6212197 test_acc:  0.828125\n",
            "iter:  730 train_cost:  0.45791012 train_acc:  0.828125 test_cost:  0.55031335 test_acc:  0.8359375\n",
            "iter:  731 train_cost:  0.6203081 train_acc:  0.8125 test_cost:  0.6504717 test_acc:  0.828125\n",
            "iter:  732 train_cost:  0.4754479 train_acc:  0.8359375 test_cost:  0.64922833 test_acc:  0.8203125\n",
            "iter:  733 train_cost:  0.5100055 train_acc:  0.8125 test_cost:  0.73685086 test_acc:  0.8046875\n",
            "iter:  734 train_cost:  0.48778898 train_acc:  0.84375 test_cost:  0.65950507 test_acc:  0.828125\n",
            "iter:  735 train_cost:  0.549609 train_acc:  0.828125 test_cost:  0.6680335 test_acc:  0.828125\n",
            "iter:  736 train_cost:  0.69673043 train_acc:  0.78125 test_cost:  0.60832876 test_acc:  0.796875\n",
            "iter:  737 train_cost:  0.61893374 train_acc:  0.8203125 test_cost:  0.4355326 test_acc:  0.859375\n",
            "iter:  738 train_cost:  0.42653632 train_acc:  0.828125 test_cost:  0.37070376 test_acc:  0.8984375\n",
            "iter:  739 train_cost:  0.5905471 train_acc:  0.796875 test_cost:  0.770176 test_acc:  0.765625\n",
            "iter:  740 train_cost:  0.6690384 train_acc:  0.796875 test_cost:  0.4600184 test_acc:  0.8828125\n",
            "iter:  741 train_cost:  0.5011811 train_acc:  0.8515625 test_cost:  0.4985335 test_acc:  0.8125\n",
            "iter:  742 train_cost:  0.6200302 train_acc:  0.8203125 test_cost:  0.42826122 test_acc:  0.8359375\n",
            "iter:  743 train_cost:  0.66214216 train_acc:  0.796875 test_cost:  0.47467804 test_acc:  0.828125\n",
            "iter:  744 train_cost:  0.61136675 train_acc:  0.8203125 test_cost:  0.52287716 test_acc:  0.8125\n",
            "iter:  745 train_cost:  0.6234926 train_acc:  0.8359375 test_cost:  0.56540966 test_acc:  0.84375\n",
            "iter:  746 train_cost:  0.47425073 train_acc:  0.8515625 test_cost:  0.6913643 test_acc:  0.8046875\n",
            "iter:  747 train_cost:  0.7210384 train_acc:  0.7421875 test_cost:  0.48399615 test_acc:  0.8203125\n",
            "iter:  748 train_cost:  0.5179044 train_acc:  0.8515625 test_cost:  0.7232098 test_acc:  0.796875\n",
            "iter:  749 train_cost:  0.408867 train_acc:  0.859375 test_cost:  0.69569165 test_acc:  0.8125\n",
            "iter:  750 train_cost:  0.46500534 train_acc:  0.8515625 test_cost:  0.42339414 test_acc:  0.84375\n",
            "iter:  751 train_cost:  0.5371445 train_acc:  0.8671875 test_cost:  0.60492444 test_acc:  0.8359375\n",
            "iter:  752 train_cost:  0.60992086 train_acc:  0.796875 test_cost:  0.4633233 test_acc:  0.859375\n",
            "iter:  753 train_cost:  0.46959043 train_acc:  0.8359375 test_cost:  0.73207426 test_acc:  0.7734375\n",
            "iter:  754 train_cost:  0.5603624 train_acc:  0.8359375 test_cost:  0.4455606 test_acc:  0.8671875\n",
            "iter:  755 train_cost:  0.6466719 train_acc:  0.8203125 test_cost:  0.46832472 test_acc:  0.828125\n",
            "iter:  756 train_cost:  0.5436698 train_acc:  0.8046875 test_cost:  0.7562853 test_acc:  0.78125\n",
            "iter:  757 train_cost:  0.5429454 train_acc:  0.828125 test_cost:  0.5714858 test_acc:  0.8671875\n",
            "iter:  758 train_cost:  0.5638844 train_acc:  0.7734375 test_cost:  0.6045623 test_acc:  0.8046875\n",
            "iter:  759 train_cost:  0.6372983 train_acc:  0.8046875 test_cost:  0.5105013 test_acc:  0.8671875\n",
            "iter:  760 train_cost:  0.52441865 train_acc:  0.8515625 test_cost:  0.59572136 test_acc:  0.8203125\n",
            "iter:  761 train_cost:  0.5010634 train_acc:  0.8671875 test_cost:  0.4884453 test_acc:  0.8515625\n",
            "iter:  762 train_cost:  0.52877754 train_acc:  0.796875 test_cost:  0.53178084 test_acc:  0.8359375\n",
            "iter:  763 train_cost:  0.4917481 train_acc:  0.8671875 test_cost:  0.44904333 test_acc:  0.8359375\n",
            "iter:  764 train_cost:  0.53062975 train_acc:  0.8359375 test_cost:  0.55549234 test_acc:  0.84375\n",
            "iter:  765 train_cost:  0.41899872 train_acc:  0.859375 test_cost:  0.49952355 test_acc:  0.875\n",
            "iter:  766 train_cost:  0.5096814 train_acc:  0.8203125 test_cost:  0.51556754 test_acc:  0.828125\n",
            "iter:  767 train_cost:  0.7428795 train_acc:  0.8046875 test_cost:  0.6675148 test_acc:  0.796875\n",
            "iter:  768 train_cost:  0.5517767 train_acc:  0.8125 test_cost:  0.5002094 test_acc:  0.8359375\n",
            "iter:  769 train_cost:  0.663783 train_acc:  0.8046875 test_cost:  0.4373744 test_acc:  0.890625\n",
            "iter:  770 train_cost:  0.47372484 train_acc:  0.8125 test_cost:  0.5089688 test_acc:  0.84375\n",
            "iter:  771 train_cost:  0.4995916 train_acc:  0.8515625 test_cost:  0.5913189 test_acc:  0.84375\n",
            "iter:  772 train_cost:  0.60789204 train_acc:  0.8359375 test_cost:  0.52455854 test_acc:  0.8359375\n",
            "iter:  773 train_cost:  0.628235 train_acc:  0.796875 test_cost:  0.58129025 test_acc:  0.859375\n",
            "iter:  774 train_cost:  0.4964088 train_acc:  0.8515625 test_cost:  0.55143356 test_acc:  0.796875\n",
            "iter:  775 train_cost:  0.45701736 train_acc:  0.875 test_cost:  0.6466704 test_acc:  0.8046875\n",
            "iter:  776 train_cost:  0.91854405 train_acc:  0.7265625 test_cost:  0.5014701 test_acc:  0.859375\n",
            "iter:  777 train_cost:  0.60734415 train_acc:  0.8046875 test_cost:  0.774555 test_acc:  0.7734375\n",
            "iter:  778 train_cost:  0.48656654 train_acc:  0.875 test_cost:  0.72211385 test_acc:  0.7890625\n",
            "iter:  779 train_cost:  0.54178786 train_acc:  0.8125 test_cost:  0.5452634 test_acc:  0.8359375\n",
            "iter:  780 train_cost:  0.4223721 train_acc:  0.8828125 test_cost:  0.3978491 test_acc:  0.8828125\n",
            "iter:  781 train_cost:  0.74975675 train_acc:  0.78125 test_cost:  0.5044027 test_acc:  0.8125\n",
            "iter:  782 train_cost:  0.5726539 train_acc:  0.8515625 test_cost:  0.35621876 test_acc:  0.859375\n",
            "iter:  783 train_cost:  0.5516746 train_acc:  0.8203125 test_cost:  0.605255 test_acc:  0.8203125\n",
            "iter:  784 train_cost:  0.42342412 train_acc:  0.84375 test_cost:  0.81329924 test_acc:  0.796875\n",
            "iter:  785 train_cost:  0.5246241 train_acc:  0.8125 test_cost:  0.3216895 test_acc:  0.9140625\n",
            "iter:  786 train_cost:  0.6544877 train_acc:  0.8125 test_cost:  0.6571745 test_acc:  0.796875\n",
            "iter:  787 train_cost:  0.61484456 train_acc:  0.7890625 test_cost:  0.5922437 test_acc:  0.8203125\n",
            "iter:  788 train_cost:  0.6085273 train_acc:  0.828125 test_cost:  0.6384692 test_acc:  0.7890625\n",
            "iter:  789 train_cost:  0.5703763 train_acc:  0.8125 test_cost:  0.48436734 test_acc:  0.8828125\n",
            "iter:  790 train_cost:  0.41824028 train_acc:  0.875 test_cost:  0.45009583 test_acc:  0.875\n",
            "iter:  791 train_cost:  0.4814762 train_acc:  0.84375 test_cost:  0.5326032 test_acc:  0.859375\n",
            "iter:  792 train_cost:  0.48359728 train_acc:  0.8203125 test_cost:  0.39852923 test_acc:  0.84375\n",
            "iter:  793 train_cost:  0.46937776 train_acc:  0.875 test_cost:  0.5683569 test_acc:  0.8046875\n",
            "iter:  794 train_cost:  0.49101707 train_acc:  0.859375 test_cost:  0.47507703 test_acc:  0.8671875\n",
            "iter:  795 train_cost:  0.3805811 train_acc:  0.890625 test_cost:  0.50358254 test_acc:  0.8515625\n",
            "iter:  796 train_cost:  0.4533232 train_acc:  0.84375 test_cost:  0.6582475 test_acc:  0.796875\n",
            "iter:  797 train_cost:  0.34865117 train_acc:  0.8671875 test_cost:  0.48511717 test_acc:  0.8515625\n",
            "iter:  798 train_cost:  0.42712012 train_acc:  0.859375 test_cost:  0.51805186 test_acc:  0.8515625\n",
            "iter:  799 train_cost:  0.4181431 train_acc:  0.8515625 test_cost:  0.4813986 test_acc:  0.84375\n",
            "iter:  800 train_cost:  0.42143464 train_acc:  0.8515625 test_cost:  0.7029756 test_acc:  0.796875\n",
            "iter:  801 train_cost:  0.45370302 train_acc:  0.875 test_cost:  0.6038035 test_acc:  0.8046875\n",
            "iter:  802 train_cost:  0.7068635 train_acc:  0.8046875 test_cost:  0.41349506 test_acc:  0.8515625\n",
            "iter:  803 train_cost:  0.68440306 train_acc:  0.7734375 test_cost:  0.7476547 test_acc:  0.8125\n",
            "iter:  804 train_cost:  0.38127363 train_acc:  0.890625 test_cost:  0.5396402 test_acc:  0.8203125\n",
            "iter:  805 train_cost:  0.5455599 train_acc:  0.84375 test_cost:  0.47379878 test_acc:  0.859375\n",
            "iter:  806 train_cost:  0.44232345 train_acc:  0.8671875 test_cost:  0.62000847 test_acc:  0.7890625\n",
            "iter:  807 train_cost:  0.46932805 train_acc:  0.8671875 test_cost:  0.5230283 test_acc:  0.8515625\n",
            "iter:  808 train_cost:  0.74535334 train_acc:  0.7734375 test_cost:  0.35128254 test_acc:  0.921875\n",
            "iter:  809 train_cost:  0.38218573 train_acc:  0.875 test_cost:  0.6033358 test_acc:  0.796875\n",
            "iter:  810 train_cost:  0.6417905 train_acc:  0.7890625 test_cost:  0.6452216 test_acc:  0.84375\n",
            "iter:  811 train_cost:  0.51216716 train_acc:  0.8203125 test_cost:  0.60926247 test_acc:  0.84375\n",
            "iter:  812 train_cost:  0.48734477 train_acc:  0.84375 test_cost:  0.5120822 test_acc:  0.859375\n",
            "iter:  813 train_cost:  0.7761935 train_acc:  0.75 test_cost:  0.5544727 test_acc:  0.828125\n",
            "iter:  814 train_cost:  0.6521849 train_acc:  0.765625 test_cost:  0.70826215 test_acc:  0.7890625\n",
            "iter:  815 train_cost:  0.67532647 train_acc:  0.8125 test_cost:  0.578939 test_acc:  0.84375\n",
            "iter:  816 train_cost:  0.36923784 train_acc:  0.8828125 test_cost:  0.54042685 test_acc:  0.7890625\n",
            "iter:  817 train_cost:  0.5256076 train_acc:  0.8359375 test_cost:  0.3345979 test_acc:  0.90625\n",
            "iter:  818 train_cost:  0.5129417 train_acc:  0.8515625 test_cost:  0.519589 test_acc:  0.8046875\n",
            "iter:  819 train_cost:  0.60893226 train_acc:  0.8515625 test_cost:  0.6102982 test_acc:  0.8125\n",
            "iter:  820 train_cost:  0.36107996 train_acc:  0.8828125 test_cost:  0.70413715 test_acc:  0.8046875\n",
            "iter:  821 train_cost:  0.5179622 train_acc:  0.8125 test_cost:  0.72280633 test_acc:  0.7734375\n",
            "iter:  822 train_cost:  0.7267771 train_acc:  0.8046875 test_cost:  0.62915504 test_acc:  0.8125\n",
            "iter:  823 train_cost:  0.46307182 train_acc:  0.8515625 test_cost:  0.6585058 test_acc:  0.8125\n",
            "iter:  824 train_cost:  0.48709774 train_acc:  0.8359375 test_cost:  0.69327474 test_acc:  0.8046875\n",
            "iter:  825 train_cost:  0.5587177 train_acc:  0.8203125 test_cost:  0.51281136 test_acc:  0.84375\n",
            "iter:  826 train_cost:  0.5991606 train_acc:  0.8203125 test_cost:  0.46368787 test_acc:  0.8515625\n",
            "iter:  827 train_cost:  0.48368543 train_acc:  0.859375 test_cost:  0.46218753 test_acc:  0.875\n",
            "iter:  828 train_cost:  0.4910715 train_acc:  0.8359375 test_cost:  0.47377637 test_acc:  0.8359375\n",
            "iter:  829 train_cost:  0.2634488 train_acc:  0.9140625 test_cost:  0.34379652 test_acc:  0.921875\n",
            "iter:  830 train_cost:  0.46810663 train_acc:  0.8515625 test_cost:  0.52435124 test_acc:  0.859375\n",
            "iter:  831 train_cost:  0.6087309 train_acc:  0.7890625 test_cost:  0.39929983 test_acc:  0.875\n",
            "iter:  832 train_cost:  0.4932807 train_acc:  0.875 test_cost:  0.5192594 test_acc:  0.8515625\n",
            "iter:  833 train_cost:  0.5097942 train_acc:  0.8203125 test_cost:  0.56633794 test_acc:  0.8203125\n",
            "iter:  834 train_cost:  0.49805027 train_acc:  0.828125 test_cost:  0.41722107 test_acc:  0.84375\n",
            "iter:  835 train_cost:  0.34041464 train_acc:  0.875 test_cost:  0.38188863 test_acc:  0.8828125\n",
            "iter:  836 train_cost:  0.4602078 train_acc:  0.8515625 test_cost:  0.41653213 test_acc:  0.84375\n",
            "iter:  837 train_cost:  0.33675063 train_acc:  0.8828125 test_cost:  0.6442917 test_acc:  0.8203125\n",
            "iter:  838 train_cost:  0.63361293 train_acc:  0.828125 test_cost:  0.6094791 test_acc:  0.8203125\n",
            "iter:  839 train_cost:  0.65736794 train_acc:  0.8203125 test_cost:  0.58994675 test_acc:  0.8515625\n",
            "iter:  840 train_cost:  0.45608777 train_acc:  0.859375 test_cost:  0.67274404 test_acc:  0.796875\n",
            "iter:  841 train_cost:  0.35193396 train_acc:  0.875 test_cost:  0.6013428 test_acc:  0.8671875\n",
            "iter:  842 train_cost:  0.5087657 train_acc:  0.84375 test_cost:  0.5614174 test_acc:  0.8515625\n",
            "iter:  843 train_cost:  0.47211564 train_acc:  0.875 test_cost:  0.64065075 test_acc:  0.8203125\n",
            "iter:  844 train_cost:  0.5125639 train_acc:  0.859375 test_cost:  0.408718 test_acc:  0.90625\n",
            "iter:  845 train_cost:  0.4198134 train_acc:  0.859375 test_cost:  0.436136 test_acc:  0.859375\n",
            "iter:  846 train_cost:  0.49471748 train_acc:  0.828125 test_cost:  0.6119156 test_acc:  0.8046875\n",
            "iter:  847 train_cost:  0.39719218 train_acc:  0.84375 test_cost:  0.5570327 test_acc:  0.8203125\n",
            "iter:  848 train_cost:  0.9257824 train_acc:  0.7578125 test_cost:  0.5272179 test_acc:  0.8828125\n",
            "iter:  849 train_cost:  0.67068225 train_acc:  0.8125 test_cost:  0.5471733 test_acc:  0.8203125\n",
            "iter:  850 train_cost:  0.53797615 train_acc:  0.8203125 test_cost:  0.65961945 test_acc:  0.796875\n",
            "iter:  851 train_cost:  0.49163815 train_acc:  0.8359375 test_cost:  0.4100182 test_acc:  0.8515625\n",
            "iter:  852 train_cost:  0.6694102 train_acc:  0.7734375 test_cost:  0.5118556 test_acc:  0.8515625\n",
            "iter:  853 train_cost:  0.6229274 train_acc:  0.828125 test_cost:  0.49372584 test_acc:  0.8125\n",
            "iter:  854 train_cost:  0.3891737 train_acc:  0.8515625 test_cost:  0.6375971 test_acc:  0.8203125\n",
            "iter:  855 train_cost:  0.7203056 train_acc:  0.8046875 test_cost:  0.43184736 test_acc:  0.875\n",
            "iter:  856 train_cost:  0.6891857 train_acc:  0.7890625 test_cost:  0.78073066 test_acc:  0.78125\n",
            "iter:  857 train_cost:  0.49163723 train_acc:  0.8359375 test_cost:  0.5782022 test_acc:  0.84375\n",
            "iter:  858 train_cost:  0.3670246 train_acc:  0.875 test_cost:  0.47163245 test_acc:  0.8515625\n",
            "iter:  859 train_cost:  0.4221971 train_acc:  0.8515625 test_cost:  0.41193002 test_acc:  0.9140625\n",
            "iter:  860 train_cost:  0.33502802 train_acc:  0.8828125 test_cost:  0.44717124 test_acc:  0.8203125\n",
            "iter:  861 train_cost:  0.409061 train_acc:  0.84375 test_cost:  0.6096097 test_acc:  0.8046875\n",
            "iter:  862 train_cost:  0.2903439 train_acc:  0.921875 test_cost:  0.5996818 test_acc:  0.8359375\n",
            "iter:  863 train_cost:  0.45491993 train_acc:  0.859375 test_cost:  0.5310933 test_acc:  0.8515625\n",
            "iter:  864 train_cost:  0.631631 train_acc:  0.7890625 test_cost:  0.5417674 test_acc:  0.8359375\n",
            "iter:  865 train_cost:  0.5444109 train_acc:  0.859375 test_cost:  0.42554218 test_acc:  0.8359375\n",
            "iter:  866 train_cost:  0.44682762 train_acc:  0.84375 test_cost:  0.39794782 test_acc:  0.8515625\n",
            "iter:  867 train_cost:  0.59838223 train_acc:  0.828125 test_cost:  0.5383353 test_acc:  0.8515625\n",
            "iter:  868 train_cost:  0.328063 train_acc:  0.8671875 test_cost:  0.41092634 test_acc:  0.8359375\n",
            "iter:  869 train_cost:  0.55820787 train_acc:  0.8203125 test_cost:  0.59741163 test_acc:  0.8203125\n",
            "iter:  870 train_cost:  0.40536058 train_acc:  0.828125 test_cost:  0.5307528 test_acc:  0.8515625\n",
            "iter:  871 train_cost:  0.57362413 train_acc:  0.8671875 test_cost:  0.59308976 test_acc:  0.828125\n",
            "iter:  872 train_cost:  0.4574675 train_acc:  0.875 test_cost:  0.36344993 test_acc:  0.875\n",
            "iter:  873 train_cost:  0.4771715 train_acc:  0.828125 test_cost:  0.73302114 test_acc:  0.796875\n",
            "iter:  874 train_cost:  0.51626724 train_acc:  0.828125 test_cost:  0.33654588 test_acc:  0.90625\n",
            "iter:  875 train_cost:  0.6552646 train_acc:  0.8359375 test_cost:  0.560455 test_acc:  0.8046875\n",
            "iter:  876 train_cost:  0.381827 train_acc:  0.8828125 test_cost:  0.45159918 test_acc:  0.859375\n",
            "iter:  877 train_cost:  0.44057545 train_acc:  0.8515625 test_cost:  0.58281446 test_acc:  0.8515625\n",
            "iter:  878 train_cost:  0.691582 train_acc:  0.78125 test_cost:  0.8303036 test_acc:  0.8046875\n",
            "iter:  879 train_cost:  0.47496837 train_acc:  0.8671875 test_cost:  0.48099843 test_acc:  0.8359375\n",
            "iter:  880 train_cost:  0.4850088 train_acc:  0.828125 test_cost:  0.51999575 test_acc:  0.84375\n",
            "iter:  881 train_cost:  0.3889993 train_acc:  0.8515625 test_cost:  0.5980704 test_acc:  0.8125\n",
            "iter:  882 train_cost:  0.44971257 train_acc:  0.828125 test_cost:  0.59102345 test_acc:  0.7890625\n",
            "iter:  883 train_cost:  0.49867177 train_acc:  0.84375 test_cost:  0.43664268 test_acc:  0.84375\n",
            "iter:  884 train_cost:  0.3198233 train_acc:  0.890625 test_cost:  0.41165644 test_acc:  0.859375\n",
            "iter:  885 train_cost:  0.5246156 train_acc:  0.84375 test_cost:  0.47366503 test_acc:  0.828125\n",
            "iter:  886 train_cost:  0.6090762 train_acc:  0.8046875 test_cost:  0.53003466 test_acc:  0.8671875\n",
            "iter:  887 train_cost:  0.4836617 train_acc:  0.8515625 test_cost:  0.4483177 test_acc:  0.859375\n",
            "iter:  888 train_cost:  0.3961451 train_acc:  0.8828125 test_cost:  0.64570296 test_acc:  0.8125\n",
            "iter:  889 train_cost:  0.5707101 train_acc:  0.875 test_cost:  0.6925198 test_acc:  0.84375\n",
            "iter:  890 train_cost:  0.7170652 train_acc:  0.8203125 test_cost:  0.54014736 test_acc:  0.8515625\n",
            "iter:  891 train_cost:  0.3500719 train_acc:  0.890625 test_cost:  0.5262846 test_acc:  0.8515625\n",
            "iter:  892 train_cost:  0.34527564 train_acc:  0.8984375 test_cost:  0.65183866 test_acc:  0.8203125\n",
            "iter:  893 train_cost:  0.4356169 train_acc:  0.859375 test_cost:  0.45596743 test_acc:  0.859375\n",
            "iter:  894 train_cost:  0.41085345 train_acc:  0.8515625 test_cost:  0.48039186 test_acc:  0.8515625\n",
            "iter:  895 train_cost:  0.6762104 train_acc:  0.765625 test_cost:  0.30880255 test_acc:  0.890625\n",
            "iter:  896 train_cost:  0.5244716 train_acc:  0.828125 test_cost:  0.45746177 test_acc:  0.84375\n",
            "iter:  897 train_cost:  0.42422262 train_acc:  0.84375 test_cost:  0.43657994 test_acc:  0.8671875\n",
            "iter:  898 train_cost:  0.353916 train_acc:  0.890625 test_cost:  0.44837922 test_acc:  0.828125\n",
            "iter:  899 train_cost:  0.5429214 train_acc:  0.7890625 test_cost:  0.51960987 test_acc:  0.859375\n",
            "iter:  900 train_cost:  0.4960792 train_acc:  0.8515625 test_cost:  0.51108253 test_acc:  0.84375\n",
            "iter:  901 train_cost:  0.60264564 train_acc:  0.796875 test_cost:  0.5004831 test_acc:  0.859375\n",
            "iter:  902 train_cost:  0.4209718 train_acc:  0.8671875 test_cost:  0.53289485 test_acc:  0.859375\n",
            "iter:  903 train_cost:  0.6787397 train_acc:  0.828125 test_cost:  0.5616391 test_acc:  0.8359375\n",
            "iter:  904 train_cost:  0.27119893 train_acc:  0.90625 test_cost:  0.59841764 test_acc:  0.828125\n",
            "iter:  905 train_cost:  0.4321 train_acc:  0.8671875 test_cost:  0.3555851 test_acc:  0.8828125\n",
            "iter:  906 train_cost:  0.41670117 train_acc:  0.875 test_cost:  0.41425106 test_acc:  0.8671875\n",
            "iter:  907 train_cost:  0.30947214 train_acc:  0.8828125 test_cost:  0.46554947 test_acc:  0.8515625\n",
            "iter:  908 train_cost:  0.33226123 train_acc:  0.875 test_cost:  0.45834136 test_acc:  0.8671875\n",
            "iter:  909 train_cost:  0.76403207 train_acc:  0.7578125 test_cost:  0.37932292 test_acc:  0.875\n",
            "iter:  910 train_cost:  0.48575228 train_acc:  0.828125 test_cost:  0.60325205 test_acc:  0.8203125\n",
            "iter:  911 train_cost:  0.36731106 train_acc:  0.875 test_cost:  0.60288006 test_acc:  0.8359375\n",
            "iter:  912 train_cost:  0.52573854 train_acc:  0.84375 test_cost:  0.40634894 test_acc:  0.859375\n",
            "iter:  913 train_cost:  0.47875133 train_acc:  0.859375 test_cost:  0.5219785 test_acc:  0.8359375\n",
            "iter:  914 train_cost:  0.4474954 train_acc:  0.8515625 test_cost:  0.4670766 test_acc:  0.8203125\n",
            "iter:  915 train_cost:  0.33294165 train_acc:  0.875 test_cost:  0.5269172 test_acc:  0.8515625\n",
            "iter:  916 train_cost:  0.328946 train_acc:  0.8984375 test_cost:  0.5283892 test_acc:  0.859375\n",
            "iter:  917 train_cost:  0.3493317 train_acc:  0.8984375 test_cost:  0.68373495 test_acc:  0.78125\n",
            "iter:  918 train_cost:  0.47667736 train_acc:  0.890625 test_cost:  0.64779556 test_acc:  0.78125\n",
            "iter:  919 train_cost:  0.3317895 train_acc:  0.8671875 test_cost:  0.36408532 test_acc:  0.90625\n",
            "iter:  920 train_cost:  0.4362602 train_acc:  0.859375 test_cost:  0.43824458 test_acc:  0.875\n",
            "iter:  921 train_cost:  0.5841848 train_acc:  0.828125 test_cost:  0.69447416 test_acc:  0.8046875\n",
            "iter:  922 train_cost:  0.4247169 train_acc:  0.859375 test_cost:  0.6916009 test_acc:  0.7890625\n",
            "iter:  923 train_cost:  0.5405279 train_acc:  0.8203125 test_cost:  0.60450476 test_acc:  0.875\n",
            "iter:  924 train_cost:  0.50330025 train_acc:  0.8671875 test_cost:  0.28582945 test_acc:  0.9296875\n",
            "iter:  925 train_cost:  0.50506926 train_acc:  0.84375 test_cost:  0.4816864 test_acc:  0.8984375\n",
            "iter:  926 train_cost:  0.70540917 train_acc:  0.859375 test_cost:  0.64713067 test_acc:  0.8046875\n",
            "iter:  927 train_cost:  0.4169513 train_acc:  0.890625 test_cost:  0.51539904 test_acc:  0.8359375\n",
            "iter:  928 train_cost:  0.3254849 train_acc:  0.90625 test_cost:  0.42003575 test_acc:  0.8984375\n",
            "iter:  929 train_cost:  0.4502765 train_acc:  0.828125 test_cost:  0.48362413 test_acc:  0.8671875\n",
            "iter:  930 train_cost:  0.6549636 train_acc:  0.8359375 test_cost:  0.63728017 test_acc:  0.8125\n",
            "iter:  931 train_cost:  0.5670258 train_acc:  0.8046875 test_cost:  0.54843205 test_acc:  0.8359375\n",
            "iter:  932 train_cost:  0.48270035 train_acc:  0.8984375 test_cost:  0.49990296 test_acc:  0.8671875\n",
            "iter:  933 train_cost:  0.5419892 train_acc:  0.8203125 test_cost:  0.47030723 test_acc:  0.859375\n",
            "iter:  934 train_cost:  0.39336598 train_acc:  0.890625 test_cost:  0.51643395 test_acc:  0.8515625\n",
            "iter:  935 train_cost:  0.35054153 train_acc:  0.875 test_cost:  0.40858954 test_acc:  0.8671875\n",
            "iter:  936 train_cost:  0.36125344 train_acc:  0.859375 test_cost:  0.6148582 test_acc:  0.796875\n",
            "iter:  937 train_cost:  0.5836824 train_acc:  0.8203125 test_cost:  0.47048792 test_acc:  0.859375\n",
            "iter:  938 train_cost:  0.48619425 train_acc:  0.8984375 test_cost:  0.6126023 test_acc:  0.8359375\n",
            "iter:  939 train_cost:  0.7117764 train_acc:  0.7890625 test_cost:  0.5596787 test_acc:  0.828125\n",
            "iter:  940 train_cost:  0.55368996 train_acc:  0.8515625 test_cost:  0.6234045 test_acc:  0.8125\n",
            "iter:  941 train_cost:  0.572755 train_acc:  0.8046875 test_cost:  0.7121854 test_acc:  0.7890625\n",
            "iter:  942 train_cost:  0.5118393 train_acc:  0.890625 test_cost:  0.65104717 test_acc:  0.75\n",
            "iter:  943 train_cost:  0.484783 train_acc:  0.8671875 test_cost:  0.49141645 test_acc:  0.875\n",
            "iter:  944 train_cost:  0.4025305 train_acc:  0.8515625 test_cost:  0.44948065 test_acc:  0.8671875\n",
            "iter:  945 train_cost:  0.48403546 train_acc:  0.8203125 test_cost:  0.32957393 test_acc:  0.90625\n",
            "iter:  946 train_cost:  0.45059934 train_acc:  0.875 test_cost:  0.4735832 test_acc:  0.8359375\n",
            "iter:  947 train_cost:  0.640926 train_acc:  0.7890625 test_cost:  0.5546576 test_acc:  0.8203125\n",
            "iter:  948 train_cost:  0.34950706 train_acc:  0.8828125 test_cost:  0.45407566 test_acc:  0.8515625\n",
            "iter:  949 train_cost:  0.41630912 train_acc:  0.890625 test_cost:  1.0663801 test_acc:  0.7421875\n",
            "iter:  950 train_cost:  0.6010992 train_acc:  0.8515625 test_cost:  0.504458 test_acc:  0.8125\n",
            "iter:  951 train_cost:  0.51944846 train_acc:  0.8125 test_cost:  0.6228657 test_acc:  0.8515625\n",
            "iter:  952 train_cost:  0.4673953 train_acc:  0.84375 test_cost:  0.49628398 test_acc:  0.8671875\n",
            "iter:  953 train_cost:  0.537839 train_acc:  0.84375 test_cost:  0.57086337 test_acc:  0.8359375\n",
            "iter:  954 train_cost:  0.5123314 train_acc:  0.828125 test_cost:  0.45010564 test_acc:  0.8515625\n",
            "iter:  955 train_cost:  0.3274486 train_acc:  0.8828125 test_cost:  0.3831703 test_acc:  0.859375\n",
            "iter:  956 train_cost:  0.42239892 train_acc:  0.890625 test_cost:  0.43959224 test_acc:  0.890625\n",
            "iter:  957 train_cost:  0.5143475 train_acc:  0.8125 test_cost:  0.46528816 test_acc:  0.859375\n",
            "iter:  958 train_cost:  0.34738022 train_acc:  0.8984375 test_cost:  0.54713905 test_acc:  0.859375\n",
            "iter:  959 train_cost:  0.5188596 train_acc:  0.78125 test_cost:  0.6560424 test_acc:  0.8203125\n",
            "iter:  960 train_cost:  0.47487187 train_acc:  0.8359375 test_cost:  0.41120484 test_acc:  0.890625\n",
            "iter:  961 train_cost:  0.5458833 train_acc:  0.8515625 test_cost:  0.45601386 test_acc:  0.875\n",
            "iter:  962 train_cost:  0.39881852 train_acc:  0.875 test_cost:  0.61807525 test_acc:  0.7890625\n",
            "iter:  963 train_cost:  0.33617717 train_acc:  0.890625 test_cost:  0.31982067 test_acc:  0.890625\n",
            "iter:  964 train_cost:  0.6263063 train_acc:  0.765625 test_cost:  0.58523506 test_acc:  0.8359375\n",
            "iter:  965 train_cost:  0.59130764 train_acc:  0.8125 test_cost:  0.44046298 test_acc:  0.8671875\n",
            "iter:  966 train_cost:  0.4242075 train_acc:  0.859375 test_cost:  0.5771243 test_acc:  0.84375\n",
            "iter:  967 train_cost:  0.46902436 train_acc:  0.8515625 test_cost:  0.4608487 test_acc:  0.875\n",
            "iter:  968 train_cost:  0.41667843 train_acc:  0.8515625 test_cost:  0.40252522 test_acc:  0.8515625\n",
            "iter:  969 train_cost:  0.62764657 train_acc:  0.765625 test_cost:  0.5227413 test_acc:  0.8515625\n",
            "iter:  970 train_cost:  0.6959441 train_acc:  0.8203125 test_cost:  0.4566189 test_acc:  0.8828125\n",
            "iter:  971 train_cost:  0.5410651 train_acc:  0.8125 test_cost:  0.46769464 test_acc:  0.875\n",
            "iter:  972 train_cost:  0.56021243 train_acc:  0.8515625 test_cost:  0.4546221 test_acc:  0.859375\n",
            "iter:  973 train_cost:  0.46305138 train_acc:  0.8828125 test_cost:  0.36839652 test_acc:  0.8828125\n",
            "iter:  974 train_cost:  0.6405748 train_acc:  0.796875 test_cost:  0.5024124 test_acc:  0.875\n",
            "iter:  975 train_cost:  0.39191568 train_acc:  0.859375 test_cost:  0.46683854 test_acc:  0.859375\n",
            "iter:  976 train_cost:  0.38247573 train_acc:  0.8671875 test_cost:  0.33859324 test_acc:  0.8828125\n",
            "iter:  977 train_cost:  0.5273139 train_acc:  0.859375 test_cost:  0.42316538 test_acc:  0.875\n",
            "iter:  978 train_cost:  0.679656 train_acc:  0.796875 test_cost:  0.59391487 test_acc:  0.859375\n",
            "iter:  979 train_cost:  0.49461526 train_acc:  0.8203125 test_cost:  0.48937804 test_acc:  0.84375\n",
            "iter:  980 train_cost:  0.44001985 train_acc:  0.8359375 test_cost:  0.33816504 test_acc:  0.8984375\n",
            "iter:  981 train_cost:  0.68862605 train_acc:  0.8203125 test_cost:  0.436332 test_acc:  0.875\n",
            "iter:  982 train_cost:  0.5909736 train_acc:  0.84375 test_cost:  0.595108 test_acc:  0.84375\n",
            "iter:  983 train_cost:  0.45406997 train_acc:  0.84375 test_cost:  0.42201084 test_acc:  0.8671875\n",
            "iter:  984 train_cost:  0.5554631 train_acc:  0.859375 test_cost:  0.5748304 test_acc:  0.8359375\n",
            "iter:  985 train_cost:  0.57604563 train_acc:  0.8359375 test_cost:  0.4379096 test_acc:  0.859375\n",
            "iter:  986 train_cost:  0.4017322 train_acc:  0.90625 test_cost:  0.43338925 test_acc:  0.8515625\n",
            "iter:  987 train_cost:  0.47944507 train_acc:  0.84375 test_cost:  0.45176733 test_acc:  0.828125\n",
            "iter:  988 train_cost:  0.44538462 train_acc:  0.8359375 test_cost:  0.50093275 test_acc:  0.875\n",
            "iter:  989 train_cost:  0.3723706 train_acc:  0.8671875 test_cost:  0.4655105 test_acc:  0.859375\n",
            "iter:  990 train_cost:  0.39858603 train_acc:  0.875 test_cost:  0.54954743 test_acc:  0.84375\n",
            "iter:  991 train_cost:  0.504836 train_acc:  0.859375 test_cost:  0.45513493 test_acc:  0.828125\n",
            "iter:  992 train_cost:  0.37653255 train_acc:  0.8828125 test_cost:  0.4890504 test_acc:  0.8515625\n",
            "iter:  993 train_cost:  0.3916993 train_acc:  0.8671875 test_cost:  0.36722916 test_acc:  0.875\n",
            "iter:  994 train_cost:  0.23440811 train_acc:  0.921875 test_cost:  0.41650546 test_acc:  0.8671875\n",
            "iter:  995 train_cost:  0.40496403 train_acc:  0.84375 test_cost:  0.34225586 test_acc:  0.8984375\n",
            "iter:  996 train_cost:  0.5138674 train_acc:  0.8515625 test_cost:  0.55477273 test_acc:  0.8046875\n",
            "iter:  997 train_cost:  0.4718479 train_acc:  0.8515625 test_cost:  0.5071697 test_acc:  0.828125\n",
            "iter:  998 train_cost:  0.7132556 train_acc:  0.7734375 test_cost:  0.40614748 test_acc:  0.8203125\n",
            "iter:  999 train_cost:  0.3944431 train_acc:  0.8828125 test_cost:  0.49809828 test_acc:  0.859375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.378722"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDkPoIsB90MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 10.579477999999998 TPU\n",
        "# 10.682025999999999 CPU\n",
        "# 8.640348 GPU \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXZtWbWQ92P_",
        "colab_type": "text"
      },
      "source": [
        "# CNN using Tensorflow\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6k9YPZQ95z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e8334d0-1771-462a-e1d6-abd24035bf42"
      },
      "source": [
        "n_classes=10\n",
        "# Create some wrappers for simplicity\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # x input image, W..weights is a tamplate(3d matrix for images),\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # x 28x28x32\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases):\n",
        "    # Reshape input picture\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1]) # if coloured it will be 3 not 1\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    print('con1_before max',conv1.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=1)\n",
        "    print('con1_after max',conv1.get_shape().as_list())\n",
        "\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    print('con2_before max',conv2.get_shape().as_list())\n",
        "\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=1)\n",
        "    print('con2_after max', conv2.get_shape().as_list())\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    #wd1 numx3x3  wd1.get_shape() -> numx9 \n",
        "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) #[-1,7*7*64]\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    #fc1 = tf.nn.dropout(fc1, dropout) #droput  between 0 and 1, represents %\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out\n",
        "\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"wc1\"),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), #2^n makes it more efficent on gpus\n",
        "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "y_p = conv_net(x, weights, biases)\n",
        "\n",
        "#crossentropy cost\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# optimisation \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initalizing the graph and the weights\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in range(1000):\n",
        "        \n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "    \n",
        "\n",
        "        train_cost, train_acc  = sess.run([cost,accuracy], feed_dict={x: batch_x,y: batch_y})\n",
        "    \n",
        "        \n",
        "        test_batch_x, test_batch_y = mnist.test.next_batch(batch_size)\n",
        "\n",
        "        test_cost, test_acc  = sess.run([cost,accuracy], feed_dict={x: test_batch_x,y: test_batch_y})\n",
        "        print('iter: ',i, 'train_cost: ', train_cost, 'train_acc: ', train_acc,'test_cost: ', test_cost, 'test_acc: ', test_acc )\n",
        "\n",
        "    \n",
        "    #y_p_p = sess.run(y_p, feed_dict={x: x_gr, y: y_gr})\n",
        "    \n",
        "    print('predicted ', y_p_p)\n",
        "    print('real ', y_gr)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "con1_before max [None, 28, 28, 32]\n",
            "con1_after max [None, 28, 28, 32]\n",
            "con2_before max [None, 28, 28, 64]\n",
            "con2_after max [None, 28, 28, 64]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[{{node softmax_cross_entropy_with_logits_sg_2}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d0b6acb3d028>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[2048,10] labels_size=[128,10]\n\t [[node softmax_cross_entropy_with_logits_sg_2 (defined at /tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'softmax_cross_entropy_with_logits_sg_2':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 456, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 486, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 438, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-d0b6acb3d028>\", line 76, in <module>\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_p, labels=y)) # cross entropy cost\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_ops.py\", line 3300, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, axis=dim, name=name)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_ops.py\", line 3105, in softmax_cross_entropy_with_logits_v2\n    labels=labels, logits=logits, axis=axis, name=name)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_ops.py\", line 3206, in softmax_cross_entropy_with_logits_v2_helper\n    precise_logits, labels, name=name)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/gen_nn_ops.py\", line 11458, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    }
  ]
}